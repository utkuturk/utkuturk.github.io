{
  "hash": "b226308f973ea4ab07e65d35ba227c20",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"montreal forced aligner workflow for pcibex production experiments\"\nauthor: \"Utku Turk\"\ndate: \"2024-07-12\"\ndescription: step-by-step instructions for filtering, renaming, and aligning pcibex recordings with mfa\ncategories: [linguistics, papers, sound, experiment]\nembed-resources: true\nself-contained-math: true\nimage: ./mfa.jpeg\ntoc: true\nparams:\n  run: false\n---\n\n\n\n\n# Introduction \n\nWe have our data from PCIbex and our zip files from the server. Let's assume we have unzipped them and converted them from `.webm` to `.wav` files. Now, it's time to align them using MFA. But before that, we need to prepare our files accordingly. Here are the steps we will follow in this document:\n\n- Load the PCIbex results\n- Filter out irrelevant sound files\n- Move all of our `.wav` and `.TextGrid` files to the same directory\n- Rename our files according to MFA guidelines\n- Run MFA\n- Create a dataframe\n\nBefore we start, let me load my favorite packages. The `library()` function loads the packages we need, assuming they are already installed. If not, use the `install.packages()` function to install them. While `library()` does not require quotes, you should use quotes with `install.packages()`, e.g., `install.packages(\"tidyverse\")`. If it asks you to select a mirror from a list, choose a location geographically close to you, such as the UK.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # I have to have tidyverse\nlibrary(stringr) # to manipulate string\nlibrary(readtextgrid) # to read TextGrid files\nlibrary(dplyr)\n```\n:::\n\n\n\n# PCIbex Results\n\n## Read the results\n\nThe main reason we are loading PCIbex results is because sometimes we use the `async()` function in our PCIbex code. The `async()` function allows us to send recordings to our server whenever we want without waiting for the end of the experiment. Even though it is extremely helpful in reducing some of the server-PCIbex connection load at the end of the experiment, it also creates some pesky situations. For example, if a participant decides not to complete their experiment, we will still end up with some of their recordings. We do not want participants who were window-shopping, mainly because we are not sure about the quality of their data. Luckily for us, PCIbex only saves the results of participants who complete the entire experiment.\n\nTo read the PCIbex results, we are going to use the function provided in the [PCIbex documentation](https://doc.pcibex.net/advanced-tutorial/12_examining-data.html). Scroll down on that page, and you will see the words \"Click for Base R Version.\" The function is provided there as well. Moreover, please be careful whenever you are copying and pasting functions from this file, or any file, as sometimes PDF or HTML files can include unwanted elements, like a page number.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# User-defined function to read in PCIbex Farm results files\nread.pcibex <- function(\n    filepath, \n    auto.colnames=TRUE, \n    fun.col=\\(col,cols){cols[cols==col]<-paste(col,\"Ibex\",sep=\".\");return(cols)}\n    ) {\n  n.cols <- max(count.fields(filepath,sep=\",\",quote=NULL),na.rm=TRUE)\n  if (auto.colnames){\n    cols <- c()\n    con <- file(filepath, \"r\")\n    while ( TRUE ) {\n      line <- readLines(con, n = 1, warn=FALSE)\n      if ( length(line) == 0) {\n        break\n      }\n      m <- regmatches(line,regexec(\"^# (\\\\d+)\\\\. (.+)\\\\.$\",line))[[1]]\n      if (length(m) == 3) {\n        index <- as.numeric(m[2])\n        value <- m[3]\n        if (is.function(fun.col)){\n         cols <- fun.col(value,cols)\n        }\n        cols[index] <- value\n        if (index == n.cols){\n          break\n        }\n      }\n    }\n    close(con)\n    return(read.csv(filepath, comment.char=\"#\", header=FALSE, col.names=cols))\n  }\n  else{\n    return(read.csv(filepath, comment.char=\"#\", header=FALSE, col.names=seq(1:n.cols)))\n  }\n}\n```\n:::\n\n\nSo, now what we have to do is load our file. I also want to check my file using the `str()` function. Please run `?str` to see what this function does. For any function that you do not understand, you can run the `?` operator to see the help pages and some examples.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nibex <- read.pcibex(\"~/octo-recall-ibex.csv\")\nstr(ibex)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\nNow, what I want to do is to get to filenames that are recorded in the PCIbex results. Before doing that, I advise you to go to [this documentation](https://github.com/addrummond/ibex/blob/master/docs/manual.md) and read more about PCIbex under the Basic Concepts header.\n\n| **Column** | **Information**                                                  |\n|:-----------|:-----------------------------------------------------------------|\n| 1          | Time results were received (seconds since Jan 1 1970)            |\n| 2          | MD5 hash identifying subject. This is based on the subject's IP address and various properties of their browser. Together with the value of the first column, this value should uniquely identify each subject. |\n| 3          | Name of the controller for the entity (e.g. \"DashedSentence\")    |\n| 4          | Item number                                                      |\n| 5          | Element number                                                   |\n| 6          | Label. Label of the newTrial()                                   |\n| 7          | Latin.Square.Group. The group they are assigned to.              |\n| 8          | PennElementType. Name of the specific element, like \"Html\", \"MediaRecorder\" |\n| 9          | PennElementName. Name we have given to the specific Penn Elements.          |\n| 10          | Parameter. This is about what type of element the script is running and saving as a parameter.          |\n| 11          | Value. Value saved for the parameters in column 10.          |\n| 12          | EventTime. Time that specific Element is screened or any action taken with that Element (seconds since Jan 1 1970)       |\n\n\n## Filter the results \n\nSince we are dealing with media recordings, I will first filter the file using the `PennElementType` column and will only select rows with \"MediaRecorder\" values in that column.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nibex <- ibex |> filter(PennElementType == \"MediaRecorder\")\nunique(ibex$Value)[1:3]\n```\n:::\n\n\nAfter checking my `Value` column where the file names for `MediaRecorder` are stored, I realize that this will not be enough given that we still have other unwanted elements like `test-recorder.webm` file or some practice files. There are multiple ways to get rid of these files, and you have to think about how to get rid of them for your own specific dataframe. For my own data, I will filter my data utilizing the labels I provided in my PCIbex code. They are stored in the `Labels` column. What I want is to only get the `MediaRecorders` that are within a trial whose label starts with the word `trial`. \n\nYou may have coded your data differently; you may have used a different word; you may not even have any practice or test-recorders, so maybe you do not even need this second filtering. Check your dataframe using the `View()` function. I am also using a function called `str_detect()`, which detects a regular expression pattern, in this case `^trial`, meaning starting with the word trial. Now, when I check my dataframe, I will only see experimental trials and recordings related to those trials. Just to make sure, I am also using the `unique()` function so that I do not have repetitions. And, I am assigning my filenames to a list called `ibex_files`. You can see that any random sample with the `sample()` function will give filenames related to experimental trials.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nibex <- ibex |> filter(str_detect(Label, \"^trial\"))\nibex_files <- ibex$Value |> unique()\nsample(ibex_files, 3)\n```\n:::\n\n\n\n\n## Operators used in this section \n\n<div class=\"function\">\n`?`\n\nOpens the help page for any function.  \n  \n \n  \nexample use: `?library()`\n</div>\n\n<br>\n<div class=\"function\">\n`==`\n\nTest for equality. **Don't confuse with a single =, which is an assignment operator (and also always returns TRUE).** \n  \n \n  \nexample use: ``\n</div>\n\n<br>\n<div class=\"function\">\n`|>`\n\n*(Forward) pipe:* Use the expression on the left as a part of the expression on the right.\n\n- Read `x |> fn()` as *'use `x` as the **only** argument of function `fn`'.*\n- Read `x |> fn(1, 2)` as *'use `x` as the **first** argument of function `fn`'.*\n- Read `x |> fn(1, ., 2)` as *'use `x` as the **second** argument of function `fn`'.* \n  \n \n  \nexample use: ``\n</div>\n\n## Our Task List \n\n- ~~Load the PCIbex results~~\n- Filter irrelevant sound files \n- Move all of our `.wav` and `.TextGrid` files to the same directory\n- Rename our files according to MFA guidelines\n- Run MFA\n- Create a dataframe\n\n\n\n# Filtering Files from the Server\n\nNow that we have a _gold list_, we can go ahead and filter our files from the server according to our gold list, based on the results from PCIbex. To do this, first we will create a temporary folder called `gold`. We will strip every file name of its extension `.wav` or `.TextGrid` and check if that name exists in our _gold list_. If that is the case, we will move the file. To this end, we are going to use something called [_for loops_](https://www.youtube.com/watch?v=5zOTJ0fOllI) and [_if statements_](https://www.youtube.com/watch?v=N6E_qqhwr7M). You can click on the hyperlinks to watch more on them. ^[On principle, I am against for loops in R, but it is better to use here instead of confusing you more.]\n\n- First, we need to list all of our files. I have all my `.wav` and `.TextGrid` files in the same place, so I just need to list a single directory. You might have them in different places. Check the commented-out part. To only get relevant files, I am using a regular expression saying only choose the ones that end (`$`) with `.wav` or (`|`) `.TextGrid` using the pattern argument.\n- Second, we create our _gold directory_ with the `dir.create()` function.\n- Third, we iterate over every file in the files list, get its name without its extension using the `tools::file_path_sans_ext()` function and check whether it exists in our _gold list_ using the `%in%` operator.\n- If it is also present in the gold list, we specify where the file exists and assign it to a variable called `old_file_path`. We also specify where it should be moved and assign it to a variable called `new_file_path`. We use the `file.path()` function and `dir`/`gold_dir` variables, along with the file `f` we are iterating over.\n- Lastly, we move the file using the `file.rename()` function. Even though the name of the function is _rename_, it can be used to move files. Every file exists with its pointer, such as `~/data/important.R`. By changing this pointer, we can change its name, as well as its location to something like `~/gold_data/really_important.R`. We did not only change the file name from important to really_important. We also changed other parts of this pointer, that is the folder part. If the folder `gold_data` exists, it will be moved there.\n- To make sure this for loop works, I also print a message after every file movement using the `cat()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndir <- \"~/data\"\ngold_dir <- \"~/data/gold\"\n# Use these lines if you have sound and transcriptions in different places.\n# wav_dir <- \"~/wav_data\"\n# tg_dir <- \"~/tg_data\"\n\nfiles <- list.files(data, pattern = \"\\\\.wav$|\\\\.TextGrid\")\n# Use these lines if you have sound and transcriptions in different places.\n# tg_files <- list.files(wav_dir, pattern = \"\\\\.wav$|\\\\.TextGrid\")\n# wav_files <- list.files(tg_dir, pattern = \"\\\\.wav$|\\\\.TextGrid\")\n\ndir.create(gold_dir)\n\nfor (f in files) {\n  if (tools::file_path_sans_ext(f) %in% tools::file_path_sans_ext(ibex_files)) {\n    old_file_path <- file.path(dir, f)\n    new_file_path <- file.path(gold_dir, f)\n    file.rename(old_file_path, new_file_path)\n    cat(\"Moved \", f, \" to \", new_file_path, \"\\n\")\n  }\n}\n```\n:::\n\n\n\nOne important thing to note here is that if you have your `.wav` and `.TextGrid` files in different directories, you can either manually move them to a single folder using your file management application. Alternatively, you can run the commands above using `wav_dir` and `tg_dir` variables, along with `tg_files` and `wav_files` variables. It should look like the following. There are, of course, better ways to solve this problem, and I leave that to your creativity.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwav_dir <- \"~/wav_data\"\ntg_dir <- \"~/tg_data\"\ngold_dir <- \"~/data/gold\"\n\n\ntg_files <- list.files(wav_dir, pattern = \"\\\\.wav$|\\\\.TextGrid\")\nwav_files <- list.files(tg_dir, pattern = \"\\\\.wav$|\\\\.TextGrid\")\n\ndir.create(gold_dir)\n\nfor (f in tg_files) {\n  if (tools::file_path_sans_ext(f) %in% tools::file_path_sans_ext(ibex_files)) {\n    old_file_path <- file.path(tg_dir, f)\n    new_file_path <- file.path(gold_dir, f)\n    file.rename(old_file_path, new_file_path)\n    cat(\"Moved\", f, \"to\", new_file_path, \"\\n\")\n  }\n}\n\nfor (f in wav_files) {\n  if (tools::file_path_sans_ext(f) %in% tools::file_path_sans_ext(ibex_files)) {\n    old_file_path <- file.path(wav_dir, f)\n    new_file_path <- file.path(gold_dir, f)\n    file.rename(old_file_path, new_file_path)\n    cat(\"Moved\", f, \"to\", new_file_path, \"\\n\")\n  }\n}\n```\n:::\n\n\n\n## Operators used in this section \n<div class=\"function\">\n`%in%`\n\nTest for membership \n  \n \n  \nexample use: ``\n</div>\n\n\n\n## Our Task List \n\n- ~~Load the PCIbex results~~\n- ~~Filter irrelevant sound files~~\n- ~~Move all of our .wav and .TextGrid files to the same directory~~\n- Rename our files according to MFA guidelines\n- Run MFA\n- Create a dataframe\n\n\n# Renaming Files\n\nThis section is going to be the section you have to be most careful about. If you mess anything up in this section, you will have to delete everything, go back, unzip your files, convert them to `.wav` from `.webm`, and do everything in this file again. So, before giving you the for-loop to rename all files, I want to make sure that we go over one of the files and make sure we do it correctly.\n\nThe main reason we rename our files is because we want Montreal Forced Aligner to understand that we have multiple speakers in our dataset. If we do not do that, it will treat all files as if they are from a single speaker and probably will be very confused due to inter-speaker variance in speech. Since we have erroneously specified in our PCIbex files to use the `subject_id` as a suffix rather than a prefix, we have to fix that. If in the future we fix that in our PCIbex script, we do not have to go over this part.\n\n\n## One-file example\n\nMy files, after unzipping, converting to `.wav`, and filtering according to the gold list, look like the following. There are some important things to keep in mind here. First of all, now that everything is in our _gold directory_, we have to use the `gold_dir` variable to list our files. Secondly, we again need to use the pattern argument to make sure we only select relevant files. The last thing to be aware of in the next code is that I am using indexing with square brackets to refer to the first elements in the list. I will use this element to first ensure that what I am doing is correct.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngold_dir <- \"~/data/gold\"\nfiles <- list.files(gold_dir, pattern = \"\\\\.wav$|\\\\.TextGrid$\")\nexample_file <- files[1]\nexample_file\n```\n:::\n\n\n### Get the extension and the name \n\nNow that we have the name of an example file, we can start by extracting its extension. We will use the function `file_ext` from the package called `tools`. Sometimes, we do not want to load an entire package, but we want to access a single function. In those cases, we use the operator `::`. Additionally, we will use `paste0` to prefix the extension with a dot, so that we can use it later when we rename our files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextension <- tools::file_ext(example_file)\nextension <- paste0(\".\", extension)\nextension\n```\n:::\n\n\nAs for the rest of the name, we will use the `file_path_sans_ext()` function that we used earlier.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrest <- tools::file_path_sans_ext(example_file)\nrest\n```\n:::\n\n\n### Get the subject id \n\nNow, the most important part is getting the subject name. If you look at what my `rest` variable returned, you can see that it consists of the last 4 characters, which are also the last set of characters after the last underscore. There are multiple ways to extract the subject id. I will show you both methods so that you can choose and adapt them for your own data. For the underscore version, we will use the function `str_split()`, and for the character counting, we will use `str_sub()`.\n\n\n#### Underscore approach \n\n`str_split()` takes a string and splits it according to the separator you provide. In our case, the separator is the underscore. We are also using an additional argument called `simplify` to make the resulting elements more user-friendly. Our function now returns a small table with 1 row and 5 columns. To select the values in the 5th column, we use square brackets again, this time with a comma. When you apply this approach to your own data, remember that you may end up with fewer or more than 5 columns depending on your naming convention. Be sure to adjust the column number accordingly. It might also be the case that your subject id is not stored last or that your separators are not underscores but simple \"-\". Modify the code according to your specific needs.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using the underscore information\nsubj <- str_split(rest, \"_\", simplify = TRUE)\nsubj\nsubj <- subj[,5]\nsubj\n```\n:::\n\n\nLastly, we have to modify the `rest` variable so that we do not include the subject id twice. I will use the same approach again. After obtaining the table, I will use the `paste()` function to concatenate the columns back together with the underscore separator. Adjust the number of columns used in this function and the separator according to your own data needs.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnosubj <- str_split(rest, \"_\", simplify = TRUE)\nnosubj <- paste(nosubj[,1], nosubj[,2], nosubj[,3], nosubj[,4], sep = \"_\")\nnosubj\n```\n:::\n\n\n#### Character approach \n\n`str_sub()` allows you to extract a substring using indices. In my case, the subject id is the last four characters. To refer to characters from the end, you can use the minus symbol `-`. I specify `-4` in the `start` argument, which means I want to extract the string starting from the fourth character counting back from the end.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubj <- str_sub(rest, start = -4)\nsubj\n```\n:::\n\n\nTo get the rest of the filename, I specify the starting point as `1` and the endpoint as `-6`. Using `-5` would include the underscore as well.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnosubj <- str_sub(rest, start = 1, end = -6)\nnosubj\n```\n:::\n\n\n### Put the new name and the path together \n\nAt this point, we have everything we need: (i) the subject id prefix, (ii) the rest of our file name, and (iii) the extension. Now, we need to combine all of this together. We are going to use the `paste0()` function. Remember, this function is different from `paste()`. The main difference is that with `paste0()`, we cannot specify separators; we have to provide everything. This might seem like a disadvantage at first, but it is beneficial for non-pattern cases like this.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_name <- paste0(subj, \"_\", nosubj, extension)\nnew_name\n```\n:::\n\n\nWe also need to create a new path to rename our file.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_path <- file.path(gold_dir, new_name)\nnew_path\n```\n:::\n\n\n### Rename the file \n\nWe will once again use the `file.rename()` function. This time, we are only changing the file name and not the path, so the file will remain in its current location. We also need to obtain the full path of our `example_file`. We can achieve this easily using the `file.path` function again.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexample_file_path <- file.path(gold_dir, example_file)\nexample_file_path\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile.rename(example_file_path, new_path)\n```\n:::\n\n\n\nAfter running this, make sure the naming convention is as we want. Check your folder by searching for the trial. It should look something like `subj_rest.wav` or `subj_rest.TextGrid`. In my case, it is ``, where `dltc` is my subject id or subj.\n\n\n## Little treat for you\n\nI know that some of your files look like the following: `squid_S_jtfr.wav`. Here, I will provide you with the code to rename this. Please check this code before using it. First, let's arbitrarily assign this name to a variable. Remember, in your case, you will obtain this from your `files` list.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexample_file <- \"squid_S_jtfr.wav\"\n```\n:::\n\n\nNow, I am going to put all the code together in one chunk, except for moving. Also, be aware that I am using my own `gold_dir`; please specify yours according to your needs. Additionally, be mindful of your operating system (Windows or Mac). If you are using Windows, your `gold_dir` variable should look like the second line. I have commented out that part with a hashtag/pound symbol. Uncomment it by deleting the first pound symbol.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngold_dir <- \"~/data/gold\"\n# gold_dir <- \"C:/Users/utkuturk/data/gold\" # for windows\nextension <- tools::file_ext(example_file)\nextension <- paste0(\".\", extension)\nrest <- tools::file_path_sans_ext(example_file)\nsubj <- str_sub(rest, start = -4)\nnosubj <- str_sub(rest, start = 1, end = -6)\nnew_name <- paste0(subj, \"_\", nosubj, extension)\nnew_path <- file.path(gold_dir, new_name)\nnew_path\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"~/data/gold/jtfr_squid_S.wav\"\n```\n\n\n:::\n:::\n\n\nThis would be your original example file path.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexample_file_path <- file.path(gold_dir, example_file)\nexample_file_path\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"~/data/gold/squid_S_jtfr.wav\"\n```\n\n\n:::\n:::\n\n\nAnd this line would handle the renaming from the old `example_file_path` to the `new_path`, thereby assigning the new name.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile.rename(example_file_path, new_path)\n```\n:::\n\n\n## For loop\n\nIf you have ensured that the code above works correctly for you, you are now ready to implement the for loop. Within the loop, define a variable like `f` and use it instead of `example_file`. This way, you will iterate over every file in your list. To verify that it is functioning correctly, I also added a line to print a message each time a file is renamed.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngold_dir <- \"~/data/gold\"\n# gold_dir <- \"C:/Users/utkuturk/data/gold\" # for windows\nfiles <- list.files(gold_dir, pattern = \"\\\\.wav$|\\\\.TextGrid$\")\n\nfor (f in files) {\n  extension <- tools::file_ext(f)\n  extension <- paste0(\".\", extension)\n  rest <- tools::file_path_sans_ext(f)\n  subj <- str_sub(rest, start = -4)\n  nosubj <- str_sub(rest, start = 1, end = -6)\n  new_name <- paste0(subj, \"_\", nosubj, extension)\n  new_path <- file.path(gold_dir, new_name)\n  file_path <- file.path(gold_dir, f)\n  file.rename(file_path, new_path)\n  cat(\"Renamed\", f, \"to\", new_name, \"\\n\")\n}\n```\n:::\n\n\n## Operators used in this section \n\n\n<div class=\"function\">\n`df[selected_rows, indices_columns] or list[selected_element]`\n\n*[], Indexing operator:* Accesses specific rows and/or columns of a data frame. If it is a list, it only takes a single argument to select an element. Remember in R indices start with 1, unlike python. \n  \n\n- `selected_rows` A vector of indices or names.\n- `selected_columns` A vector of indices or names.\n- `selected_element` A vector of indices or names. \n  \nexample use: `files[1]`\n</div>\n\n<br>\n\n<div class=\"function\">\n`::`\n\n*Double colon operator:* Accesses functions and other objects from packages.  Read `x::y` as *'function y from package x.'* \n  \n \n  \nexample use: `tools::file_ext()`\n</div>\n\n\n## Our Task List \n\n- ~~Load the PCIbex results~~\n- ~~Filter irrelevant sound files~~\n- ~~Move all of our .wav and .TextGrid files to the same directory~~\n- ~~Rename our files according to MFA guidelines~~\n- Run MFA\n- Create a dataset\n\n\n# Running the Montreal Forced Aligner\n\nNow that we have our files in the format we want, we can place all of our files in the MFA folder and start running the aligner. We can either move our files using the Explorer app and usual copy-paste or we can use the `file.rename()` function again. Due to the aligner's limitations, the second option is far better. The main constraint is that MFA starts encountering problems when we feed it more than 2000 files at once. Since I have a lot of data, I will use the following function to divide my files and move them into smaller subfolders. But before that, I will show you how to move files without dividing them into subfolders.\n\n\n## Moving the Files to MFA Directory\n\n### Without Dividing \n\nAgain, we are going to use the `file.rename()` and `dir.create()` functions to create the directory we are moving files to, and of course, to move files.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# gold directory, where all of our files are\ngold_dir <- \"~/data/gold\"\n# MFA directory\nmfa_dir <- \"~/Documents/MFA/mycorpus\"\ndir.create(mfa_dir)\n\n# Files \nfiles <- list.files(gold_dir, pattern = \"\\\\.wav$|\\\\.TextGrid$\")\nfor (f in files) {\n  old_file_path <- file.path(gold_dir, f)\n  mfa_path <- file.path(mfa_dir, f)\n  file.rename(old_file_path, mfa_path)\n  cat(\"Moved\", f, \"to\", mfa_dir, \"\\n\")\n}\n```\n:::\n\n\n\n### With Dividing them into subfolders \n\nI will introduce the following function that I use. Here, I will not go into details, but it basically performs the following steps:\n\n- Creates a subfolder called `s1` and moves files into it.\n- Counts up to 2000.\n- When it surpasses 2000, it creates another subfolder by incrementing the number from `s1` to `s2`.\n- Continues this process until there are no more files.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndivide_and_move <- function(source, target, limit=2000) {\n  files <- list.files(source, pattern = \"\\\\.wav$|\\\\.TextGrid$\", full.names = TRUE)\n  base_names <- unique(tools::file_path_sans_ext(basename(files)))\n  s_index <- 1\n  f_index <- 0\n  s_path <- file.path(target, paste0(\"s\", s_index))\n  dir.create(s_path)\n  \n  for (b in base_names) {\n    rel_files <- files[grepl(paste0(\"^\", b, \"\\\\.\"), basename(files))]\n    \n    if (f_index + length(rel_files) > limit) {\n      s_index <- s_index + 1\n      s_path <- file.path(target, paste0(\"s\", s_index))\n      dir.create(s_path)\n      f_index <- 0\n    }\n    \n    for (f in rel_files) {\n      file.rename(f, file.path(s_path, basename(f)))\n    }\n    f_index <- f_index + length(rel_files)\n  }\n}\n```\n:::\n\n\nYou can use this function by simply providing the source and target folders.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# gold directory, where all of our files are\ngold_dir <- \"~/data/gold\"\n# MFA directory\nmfa_main_dir <- \"~/Documents/MFA\"\ndir.create(mfa_dir)\ndivide_and_move(gold_dir, mfa_main_dir)\n```\n:::\n\n\n\n## Terminal Codes \n\nAfter moving the files either with code or by hand to a specific MFA folder, `~/Documents/MFA`, we can start running the terminal commands. At this point, I assume you have gone through the [MFA documentation](https://montreal-forced-aligner.readthedocs.io/en/latest/installation.html) for installation instructions. I am also assuming that you have used a `conda` environment. If you haven't, here are the 3 lines to install MFA.\n\n\n\n::: {.cell filename='Conda Installation in Terminal'}\n\n```{.bash .cell-code}\nconda activate base\nconda install -c conda-forge mamba\nmamba create -n aligner -c conda-forge montreal-forced-aligner\n```\n:::\n\n\nThere are again two ways to do this. One way is to open your Terminal app or use the Terminal tab in the R console below. The other way, which I prefer more, is to execute commands using the R function `system()`. I will first go over the easier one, which is using the Terminal app or the Terminal tab in R. But the reason I prefer the `system()` function is that I can loop over multiple folders more easily that way, and I do not have to run my commands again and again.\n\n### Using Terminal\n\nThe first command we want to run is the conda environment code. Following the MFA documentation, I renamed my environment to `aligner`. So, I start by activating that environment.\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nconda activate aligner\n```\n:::\n\n\nAfter activating the environment, I need to download three models: (i) an acoustic model to recognize phonemes given previous and following acoustic features, (ii) a dictionary to access pretrained phone-word mappings, and (iii) a g2p model to generate sequences of phones based on orthography. For all of these models, we are going to use the `english_us_arpa` model. You can visit [this website](https://mfa-models.readthedocs.io/en/latest/acoustic/index.html) to explore various languages and models.\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmfa model download acoustic english_us_arpa\nmfa model download dictionary english_us_arpa\nmfa model download g2p english_us_arpa\n```\n:::\n\n\nAfter downloading these models, we are going to validate our corpus. There are many customizable parameters for this step. You can check them [here](https://montreal-forced-aligner.readthedocs.io/en/latest/user_guide/data_validation.html). I am going to use my favorite settings here. You can interpret the following command like this: _Dear Montreal Forced Aligner (`mfa`), can you please analyze my files located in `~/Documents/MFA/mycorpus` and `validate` them using the `english_us_arpa` acoustic model and `english_us_arpa` dictionary? Please also consider that I have multiple speakers, indicated by the first 4 characters (`-s 4`). It would be great to use multiprocessing (`--use_mp`) for faster execution. Lastly, please clean up previous and new temporary files (`--clean --final_clean`)._\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmfa validate -s 4 --use_mp --clean --final_clean ~/Documents/MFA/mycorpus english_us_arpa english_us_arpa\n```\n:::\n\n\nThis process will take some time. Afterward, you will have some _out of vocabulary_ words found in your TextGrids. You can easily create new pronunciations for them and add them to your model.\n\nThe `mfa g2p` command can take many arguments; here I am using only three. First, the path to the text file that has _out of vocabulary_ words. This file is automatically created in your folder where your files are located. The path may vary depending on your system and folder naming, but the name of the `.txt` file will be the same. In my case, it is `~/Documents/MFA/mycorpus/oovs_found_english_us_arpa.txt`. The second argument is the name of the g2p model. As you may recall, we downloaded it earlier, and its name is `english_us_arpa`. Finally, the third argument is the path to a target `.txt` file to store new pronunciations. I would like to store them in the same place, so I am using the following path: `~/Documents/MFA/mycorpus/g2pped_oovs.txt`.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmfa g2p ~/Documents/MFA/mycorpus/oovs_found_english_us_arpa.txt english_us_arpa ~/Documents/MFA/mycorpus/g2pped_oovs.txt\n```\n:::\n\n\nAfter creating the pronunciations, you can add them to your model with `mfa model add_words`. This command takes the name of the dictionary as an argument (`english_us_arpa`) and the output of the `mfa g2p` command, which was a `.txt` file storing pronunciations: `~/Documents/MFA/mycorpus/g2pped_oovs.txt`.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmfa model add_words english_us_arpa ~/Documents/MFA/mycorpus/g2pped_oovs.txt\n```\n:::\n\n\nThe last step is the alignment process. It will align (`mfa align`) the words and the phones inside our TextGrids stored in `~/Documents/MFA/mycorpus` using our previously downloaded dictionary (`english_us_arpa`) and model (`english_us_arpa`), and store the newly aligned TextGrids in a new folder called `~/Documents/MFA/output`.\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmfa align ~/Documents/MFA/mycorpus english_us_arpa english_us_arpa ~/Documents/MFA/output\n```\n:::\n\n\n### Using R\n\nWe can also accomplish all of this in R. One advantage of this approach is that it allows us to iterate over multiple subfolders more easily, which can be useful if we have more than 2000 files. We will use four components:\n\n(i) `system()` function to execute terminal commands,\n(ii) `paste()` function to create multiline templates,\n(iii) `%s` string placeholder to create template codes,\n(iv) `sprintf()` function to format our templates.\n\n#### Introduction to `sprintf()` and `%s`\n\nBefore going further with MFA codes, let me illustrate with an example. Suppose we have a list of folder names, and we want to create a `.txt` file in each of these folders. We can use the `system()` function to perform this action. Below, I define my `folder_list`, then create paths for my `.txt` files in each folder, such as `~/data1/mydocument.txt`. Afterwards, I generate a list of commands to create these files using `touch`, which is a command-line tool for creating files. Finally, I execute these commands using the `system()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfolder_list <- c(\"~/data1\", \"~/data2\", \"~/data3\")\n\n\ntxt_list <- paste(folder_list, \"mydocument.txt\", sep=\"/\")\ntxt_list\ncommand_list <- paste(\"touch\", txt_list, sep=\" \")\ncommand_list\n\nfor (command in command_list) {\n  system(command)\n}\n```\n:::\n\n\nTechnically, we didn't need to use a for loop; instead, we could have concatenated all these commands with `;` and run a single system command. Bash can execute multiple commands in a single line when separated by `;`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconcatenated_commands <- paste(command_list[1], \n                               command_list[2], \n                               command_list[3], \n                               sep=\";\") \n\nsystem(concatenated_commands)\n```\n:::\n\n\nWe could achieve the same without needing a folder list by utilizing the `%s` placeholder and the `sprintf()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncommand_template <- \"touch %s/mydocument.txt\"\nconcatenated_commands <- paste(sprintf(command_template, \"~/data1\"), \n                               sprintf(command_template, \"~/data2\"),\n                               sprintf(command_template, \"~/data3\"),\n                               sep=\";\")\n\nsystem(concatenated_commands)\n```\n:::\n\n\nThis approach becomes particularly useful when dealing with multiple placeholders within the same command. For instance, the command template will replace the first `%s` with the first argument, such as `~/data1`, and the second `%s` with the second argument, like `mydoc1`, when formatted using `sprintf()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncommand_template <- \"touch %s/%s.txt\"\nconcatenated_commands <- paste(sprintf(command_template, \"~/data1\", \"mydoc1\"), \n                               sprintf(command_template, \"~/data2\", \"mydoc2\"),\n                               sprintf(command_template, \"~/data3\", \"mydoc3\"),\n                               sep=\";\")\n\nsystem(concatenated_commands)\n```\n:::\n\n\n\n#### Running MFA in R\n\nNext, we'll consolidate the previous code by concatenating it using `paste()` and separating commands with `;`. If needed, we'll incorporate placeholders. Each line will be assigned to a new variable, and then they'll be combined into a single command string using `paste()`. Finally, we'll execute the command string using `system()` with the argument `intern = TRUE` to capture the output into an R variable, which allows for later inspection.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconda_start <- \"conda activate aligner\"\nget_ac <- \"mfa model download acoustic english_us_arpa\"\nget_dic <- \"mfa model download dictionary english_us_arpa\"\nget_g2p <- \"mfa model download g2p english_us_arpa\"\n\nmfa_init <- paste(conda_start, get_ac, get_dic, get_g2p, sep = \";\")\n\nmfa_init_output <- system(mfa_init, intern = TRUE)\n```\n:::\n\n\nAfter initializing the model, the next step involves validation. Again, I'll use the same approach and concatenate the commands together. However, sometimes we may have too many files and need to use subfolders. To accommodate this, I'll use `%s` placeholders. The validation command has one placeholder for different subfolders. Similarly, our pronunciation creation for g2p has two placeholders, though they'll be filled with the same value. Lastly, the `add_words` command will use a single placeholder. Fortunately, all these folders are the same, so we can reuse the same variable repeatedly.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconda_start <- \"conda activate aligner\"\n\nvalidate <- \"mfa validate -s 4 --use_mp --clean --final_clean ~/Documents/MFA/%s english_us_arpa english_us_arpa\"\ng2p_words <- \"mfa g2p ~/Documents/MFA/%s/oovs_found_english_us_arpa.txt english_us_arpa ~/Documents/MFA/%s/g2pped_oovs.txt\"\nadd_words <- \"mfa model add_words english_us_arpa ~/Documents/MFA/%s/g2pped_oovs.txt\"\n\nmfa_val <- paste(conda_start, validate, g2p_words, add_words, sep = \";\")\n```\n:::\n\n\nSince this step takes longer and there's more room for errors, I want to save all my outputs in a list. First, I need to identify which folders exist in my MFA directory. Because my `divide_and_move` function prefixes every subfolder with `s`, I'll use `^s` to filter for relevant folders.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noutput_val <- list()\n\n# Define the base path where folders are located\nbase_path <- \"~/Documents/MFA\"\nfolders <- list.dirs(base_path, recursive = FALSE, full.names = FALSE)\nfolders <- folders[str_detect(folders, \"^s\")]\n\nfolders\n```\n:::\n\n\nNow, we can iterate over this list of folders using a for loop. First, we create a temporary script using `sprintf()` with four placeholders. Next, we execute the current script and save the output in a `temp_output` variable. Later, we assign this output to specific `output_name` variables for each folder using `paste0()` and `assign()` functions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (f in folders) {\n  cur_mfa_val <- sprintf(mfa_val, f, f, f, f)\n  \n  temp_output <- system(cur_mfa_val, intern = TRUE)\n  \n  output_name <- paste0(\"output_val_\", f)\n  \n  assign(output_name, temp_output, envir = .GlobalEnv)\n}\n```\n:::\n\n\nNow you can check the outputs by calling specific variables like `output_val_s1` or `output_val_s2`. After this step, the only task remaining is to run the aligner. We will create a template again, iterate over folders, and assign outputs to their respective names for verification. Meanwhile, the bash code will execute in the background. This time, our placeholders will refer to different inputs and an output folder. Fortunately, we can use the same output folder for every subfolder, so instead of using two placeholders, we'll use a single `%s` placeholder.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconda_start <- \"conda activate aligner\"\n\nalign <- \"mfa align ~/Documents/MFA/%s english_us_arpa english_us_arpa ~/Documents/MFA/output\"\n\nmfa_align <- paste(conda_start, align, sep = \";\")\n\nfor (f in folders) {\n  cur_mfa_align <- sprintf(mfa_align, f)\n  temp_output <- system(cur_mfa_align, intern=TRUE)\n  output_name <- paste0(\"output_align_\", f)\n  assign(output_name, temp_output, envir = .GlobalEnv)\n}\n```\n:::\n\n\nThis for loop completes the MFA alignment. There is one final task remaining: creating a dataframe for further data analysis.\n\n\n## Our Task List \n\n- ~~Load the PCIbex results~~\n- ~~Filter irrelevant sound files~~\n- ~~Move all of our .wav and .TextGrid files to the same directory~~\n- ~~Rename our files according to MFA guidelines~~\n- ~~Run MFA~~\n- Create a dataframe\n\n# Dataframe Creation\n\nNow that we have all our `.TextGrid` files aligned, we can create a dataframe for subsequent statistical analyses using the `readtextgrid` package. First, I'll demonstrate the process for a single file. Later, I'll explain how to extend this to an entire directory. Let's begin by specifying our directory and listing the files. You can view the first few elements of a list or dataframe using the `head()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the directory and list files\ntg_dir <- \"~/Documents/MFA/output\"\nfile_list <- list.files(path = tg_dir, pattern = \"\\\\.TextGrid$\")\nhead(file_list, n = 5)\n```\n:::\n\n\n\n## One-file example\n\nAgain, let's work with an example file from our list, starting with the first file `[1]`. First, we'll retrieve its full file path. Then, we'll use the `read_textgrid()` function to create a dataframe for this single file. I'll print the structure of the dataframe to give you a clearer view of its contents.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexample_file <- file_list[1]\nfile_path <- file.path(tg_dir, example_file)\nexample_df <- readtextgrid::read_textgrid(file_path)\nstr(example_df)\n```\n:::\n\n\nIn this project, which involves aligning words, we are interested in only a couple of these columns. Specifically, we focus on the file identifier (`file`) to determine the trial from which the data originates, the tier name (`tier_name`) to differentiate between word and phone tiers, the start (`xmin`) and end (`xmax`) of each interval, and finally, the `text`. Additionally, I am not interested in retaining the file extension in the `file` identifier. Therefore, we will first filter to include only annotated words, then select the important columns using `select()`, remove the `.TextGrid` extension, and concatenate the words so that we can see the full response for each trial.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexample_df <- example_df |> \n  # Filter annotated \"words\" tier\n  filter(tier_name == \"words\" & text != \"\") |> \n  # Select relevant columns\n  select(file, xmin, xmax, text, annotation_num) |> \n  # Remove .TextGrid and put the response together\n  mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n         response = paste(text, collapse = \" \"))\n\nexample_df\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\nWe also need some information about the trial. Luckily, all of our information is provided in our file name. So, I am going to parse that name to create a dataframe with more information. I am using a set of function that all start with `separate_wider_`. \n\n-   The `delim` version uses a deliminator to split a row of a dataframe. \n-   The `regex` version uses regular expressions to split the data. \n-   Finally, the `position` version uses the number of characters to split the data. \n\nI am doing all of this because of how I initially coded my experiment output in my PCIbex script. You may need to change this code to process your own data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexample_df <- example_df |>\n  # split the `file` column into 5 different columns.\n  separate_wider_delim(file, \"_\", names = c(\"subj\", \"exp\", \"headVerb\", \"NumNum\", \"sem_type\"), cols_remove = F) |>\n  # split the headVerb column from the \"U\" character\n  separate_wider_regex(headVerb, c(head = \".*\", \"U\", verb_type = \".*\")) |>\n  # Add the \"U\" character back to \"Unacc\" and \"Unerg\"s\n  mutate(verb_type = paste0(\"U\", verb_type)) |>\n  # split the head and distractor numbers.\n  separate_wider_position(NumNum, c(head_num = 2, dist_num = 2))\n\nexample_df\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n## Another Treat for you\n\nLet's see what you will need to do. You will find your file in the `MFA/output` folder as well, and your file will look like `jtfr_squid_S.TextGrid`. Let's arbitrarily put them here. Remember, you will have to use the `file.list()` function as well. You will not need to change anything in the first part where we work on the TextGrid. The necessary changes will need to be done in the parsing procedure. Instead of using the entire `regex` or `position` methods, you will just need to use the `delim` version of the function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the directory and list files\ntg_dir <- \"~/Documents/MFA/output\"\nyour_file <- \"jtfr_squid_S.TextGrid\"\n\nfile_path <- file.path(tg_dir, your_file)\n\n\n### LETS SAY YOU RAN read_textgrid function.\n### I commented out this part, because I do not have your data.\n### Final dataframe will be slightly different here, because I do not have the textgrid data here.\n# your_df <- readtextgrid::read_textgrid(path = file_path) |>\n#     filter(tier_name == \"words\" & text != \"\") |>\n#     select(file, xmin, xmax, text, annotation_num) |>\n#     mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n#            response = paste(text, collapse = \" \"))\n#   \n\nyour_df <- your_df |>\n  separate_wider_delim(file, \"_\", names = c(\"subj\", \"head\", \"condition\"), cols_remove = F) \n\nyour_df\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|subj |head  |condition |file         |xmin |xmax |text   | annotation_num|response                    |\n|:----|:-----|:---------|:------------|:----|:----|:------|--------------:|:---------------------------|\n|jtfr |squid |S         |jtfr_squid_S |     |     |squid  |              1|squid jumped over the fence |\n|jtfr |squid |S         |jtfr_squid_S |     |     |jumped |              2|squid jumped over the fence |\n|jtfr |squid |S         |jtfr_squid_S |     |     |over   |              3|squid jumped over the fence |\n|jtfr |squid |S         |jtfr_squid_S |     |     |the    |              4|squid jumped over the fence |\n|jtfr |squid |S         |jtfr_squid_S |     |     |fence  |              5|squid jumped over the fence |\n\n\n:::\n:::\n\n\n## For-loop\n\nNow, we need to apply this process to all files in our output directory. To simplify this, I'll start by creating a function for processing each file individually and then apply it to all files. The function takes a file name and its directory as inputs and returns a dataframe. Before creating each dataframe, it prints \"Reading the file.\" to indicate progress.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprocess_textgrid <- function(file, directory) {\n  cat(\"Reading\", file, \"\\n\")\n  file_path <- file.path(directory, file)\n  df <- readtextgrid::read_textgrid(path = file_path) |>\n    filter(tier_name == \"words\" & text != \"\") |>\n    select(file, xmin, xmax, text, annotation_num) |>\n    mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n           response = paste(text, collapse = \" \"))\n  \n  df <- df |>\n    separate_wider_delim(file, \"_\", names = c(\"subj\", \"exp\", \"headVerb\", \"NumNum\", \"sem_type\"), cols_remove = F) |>\n    separate_wider_regex(headVerb, c(head = \".*\", \"U\", verb_type = \".*\")) |>\n    mutate(verb_type = paste0(\"U\", verb_type)) |>\n    separate_wider_position(NumNum, c(head_num = 2, dist_num = 2))\n  \n  return(df)\n}\n```\n:::\n\n\nThis is essentially the same process as before, but encapsulated within a function for easier application. Here's an example of how I'm redefining my directory and file list:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the directory and list files\ntg_dir <- \"~/Documents/MFA/output\"\nfile_list <- list.files(path = tg_dir, pattern = \"\\\\.TextGrid$\")\nexample_file <- file_list[1]\nprocess_textgrid(example_file, tg_dir)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\nYour version will look like this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprocess_textgrid <- function(file, directory) {\n  cat(\"Reading\", file, \"\\n\")\n  file_path <- file.path(directory, file)\n  df <- readtextgrid::read_textgrid(path = file_path) |>\n    filter(tier_name == \"words\" & text != \"\") |>\n    select(file, xmin, xmax, text, annotation_num) |>\n    mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n           response = paste(text, collapse = \" \"))\n  \n  df <- df |>\n    separate_wider_delim(file, \"_\", names = c(\"subj\", \"head\", \"condition\"), cols_remove = F) \n  \n  return(df)\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the directory and list files\ntg_dir <- \"~/Documents/MFA/output\"\nfile_list <- list.files(path = tg_dir, pattern = \"\\\\.TextGrid$\")\nexample_file <- file_list[1]\nprocess_textgrid(example_file, tg_dir)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\nNow, we need to integrate this function into our for-loop. Instead of using a single file like `file_list[1]`, we will apply it to an entire directory. Unlike previous for loops, we will use the `map` function from the `purrr` package. It is faster and easier to use in cases like this. `map()` will return all of our dataframes embedded in a list. After using `map()`, we need to combine all these smaller dataframes into a larger one using `bind_rows`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the directory and list files\ntg_dir <- \"~/Documents/MFA/output\"\nfile_list <- list.files(path = tg_dir, pattern = \"\\\\.TextGrid$\")\n# Run our function, I am using mine, you should use your own.\nprocess_textgrid <- function(file, directory) {\n  cat(\"Reading\", file, \"\\n\")\n  file_path <- file.path(directory, file)\n  df <- readtextgrid::read_textgrid(path = file_path) |>\n    filter(tier_name == \"words\" & text != \"\") |>\n    select(file, xmin, xmax, text, annotation_num) |>\n    mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n           response = paste(text, collapse = \" \"))\n  \n  df <- df |>\n    separate_wider_delim(file, \"_\", names = c(\"subj\", \"exp\", \"headVerb\", \"NumNum\", \"sem_type\"), cols_remove = F) |>\n    separate_wider_regex(headVerb, c(head = \".*\", \"U\", verb_type = \".*\")) |>\n    mutate(verb_type = paste0(\"U\", verb_type)) |>\n    separate_wider_position(NumNum, c(head_num = 2, dist_num = 2))\n  \n  return(df)\n}\n\ndfs <- map(file_list, process_textgrid, directory = tg_dir)\n\nfinal_df <- bind_rows(dfs)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\nThis completes our MFA Aligning work. We have successfully completed every task on our list, aligned our data, and created a dataframe for analysis. You can check the structure of our final dataframe, the number of rows, and the count of unique trials.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(final_df)\n\nnrow(final_df)\n\nlength(unique(final_df$file))\n```\n:::\n\n\n\n# An Example Descriptive Summary\n\n\nLet me also show a small example of descriptive statistics using basic functions. One thing we might want to do is check whether there is a difference between conditions in people's time to start uttering the sentence. Let's assume people are still thinking about the sentence when they utter the first determiner \"_the_,\" since it is kind of automatic in English and all of our sentences start with this determiner anyway. I am going to filter my dataframe using the `text` column. Additionally, I want to ensure it is the first occurrence of \"_the_\" and not any other \"_the_\" in the sentences, so I will use the `annotation_num` column in my filtering.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst_thes <- final_df |> filter(text == \"the\" & annotation_num < 3)\n```\n:::\n\n\nThen, what I am going to do is summarize my dataframe. I will group my dataframe by columns `verb_type` and `sem_type` since my experiment had 2 different verb types (unergative, unaccusative) and 2 different types of semantic distractors (related, unrelated). Then, I am going to use the `summarize()` function to get the mean, standard error, and credible interval for each condition. Since we are assuming people are still planning sentences while they utter the first determiner, I am going to summarize my data using the offset of the _the_ (`xmax`). Before doing this, I am going to convert `xmax` from seconds to milliseconds. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst_thes |>\n  mutate(xmax = xmax*1000) |>\n  group_by(verb_type, sem_type) |>\n  summarise_each(funs(mean, se=sd(.)/sqrt(n()), ci=se*1.96), xmax)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\nFrom this point on, I leave the modeling and plotting of the data for another guide.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}