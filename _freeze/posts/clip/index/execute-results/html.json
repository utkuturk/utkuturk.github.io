{
  "hash": "ca8bff21844db8717e3aeec5ff15e9e0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"unaccusativity syntax or picture difficulty?\"\nauthor: \"Utku Turk\"\ndate: \"2026-01-16\"\ncategories: [psycholinguistics, ai, experiments, CLIP, unaccusativity]\nimage: \"model_comparison_plot.png\"\ndescription: \"i am obssessed with early advance planning, but let me make sure about some picture saliency\"\nembed-resources: true\nself-contained-math: true\ntoc: true\nformat:\n  html:\n    code-overflow: wrap\n---\n\n\n## Prelude\n\n### What?\n\nDuring my PhD, I've become fairly obsessed with production studies. I find them extremely interesting, especially the way they combine what we know from theoretical linguistics with creative experimental methods. Not to mention that the theoretical framing of production work is quite poor and the main theories used need a major overhaul. One of the most interesting papers in this area is Shota Momma's work on advanced verb planning. Similar work was also done in German and Basque by Sebastian Sauppe's group.\n\nLet me set the scene. He and his colleagues ran multiple picture description experiments where participants saw images like:\n- \"The octopus below the spoon is swimming\" (unergative)\n- \"The octopus below the spoon is boiling\" (unaccusative)\n\nIf you're not a syntax nerd, here's the ultra-compressed version: verbs like \"swim\" and \"bark\" (unergatives) are different from verbs like \"sink\" and \"melt\" (unaccusatives), even though they both describe single-argument events. The difference has to do with argument structure—where the subject comes from in the underlying syntax. It has been argued that the subjects of unaccusatives are actually 'deep objects' for lack of a better term, and they structurally start in the same position as any other object.\n\nThey showed that these two verb types behave differently in production experiments. Speakers plan them differently. They tested this by showing related or unrelated words superimposed on the pictures. They found that when the verbs were related, participants slowed down before they started speaking—but only with unaccusatives. His theoretical claim was that unaccusative verbs are planned *earlier* in the sentence production process—possibly right at the beginning, along with the subject. \n\n\n### Why though?\n\nHere's the thing. Another thing that made me very excited about the production endeavor is that there are probably so many possible confounds that require checking. And I love this song and dance in psycholinguistics, where I can stress-test findings and see how stable they are. It's especially important when you find an unexpected result—like participants taking **longer to start speaking** when they won't say the verb for at least 3 more seconds—my first instinct, and I hope yours, is to wonder: \"Is this real, or is something else going on?\"\n\nThis post is built on a very specific worry: **What if unaccusative scenes themselves, and not the syntax of them, created the results?** One interesting finding in Shota Momma's papers was that unergative planning was seemingly invisible. He has shown that there are reasons to believe that it happens while saying the second NP. But quantitatively, the signature of unergative planning seems to be more dissolved throughout the sentence, while the unaccusative planning is strikingly clear.\n\nThis creates the following question: is it possible that participants, simply because the picture was more difficult to understand or the subject was more involved in the action, spent more time initially to either understand the event or to extract the subject from the event, and during this time a deterministic analysis of the written word kicked in and slowed them down when it was related? Since the unergative subjects are more easily dissociable from the event, since nothing is happening to them in those pictures, it takes less time, and since it's less of a resource-heavy process, no additional process interferes with it. This has several predictions. First, in follow-up experiments where the unergative pictures are hard to 'retrieve' from the scene, one should see similar onset effects. Second, if there is some sort of picture-difficulty metric, the advance planning should align with that metric item-wise.\n\nThe second prediction is going to be the basis of this blog post, where we will find a way to quantify the picture difficulty. \n\n### I make assumptions\n\nI assume the following 'two-way' distinction with respect to lexical verbs. However, one needs to admit that unaccusativity is not stable all the time. Many such unaccusative verbs can be used as unergatives given some adverbial modification or different contexts. This would create some minor infelicity in English, but that is not the case for many languages. For example, Laz can make any verb 'agentive' with a small prefix. Imagine a Laz-type English where you have \"I cried\" vs. \"I do-cried,\" where the second one means that you made yourself cry or you deliberately cried. Or a better example might be: imagine if English \"jump\" were decomposable into a prefix \"do-\" and \"fall.\" So, for now I only assume that these properties are lexical properties of the verb, but one needs to admit that these are event-related ones.\n\n- **Unergative actions** (swimming, barking, running): The action is performed by the agent. You can see the octopus swimming—the action is somewhat separable from what happens to the entity.\n- **Unaccusative actions** (boiling, melting, sinking): Something is *happening to* the entity. The octopus isn't \"doing\" boiling—it's *undergoing* a change of state. The action and the entity are less separable.\n\nAnother assumption I make is about CLIP/VLM. The input that CLIP takes is a written sentence and a picture. I am fully aware that the way CLIP assesses pictures is nowhere near how humans do.^[However, an interesting sidenote is that we do not really know if human cognition is also propositional.] I am also aware that in human speech, the scenes are what is encoded and the speech is the decoding. CLIP works differently. CLIP is a two-encoder model. Given two inputs of a picture and a text, it creates two separate vectors and checks how similar those vectors are. Thus, it does not give us anything about human cognition. But it gives us a way to quantify relevant metrics.  Below what I assume to be the models of human speech production based on Levelt's work and CLIP's architecture.\n\n**Levelt's Speech Production Model:**\n\n```{mermaid}\ngraph LR\n    %% Conceptual Stage\n    C2(((\"sleep(x)\")))\n    \n    %% Lemma Stage (Centralized)\n    subgraph Lemma_Stage [\"Lemma Stage\"]\n        direction LR\n        F1[\"V⁰\"] --- F2[\"pres\"] --- L1([\"sleep\"]) --- F3[\"3\"] --- F4[\"pl\"]\n    end\n\n    %% Lexeme Stage\n    subgraph Lexeme_Stage [\"Lexeme Stage\"]\n        direction TB\n        LX1[\"/sl:ip/\"]\n        LX2[\"/s/\"]\n        LX3[\"/slEpt/\"]\n        LX4[\"/IN/\"]\n    end\n\n    %% Realization Stage\n    subgraph Realization_Stage [\"Realization Stage\"]\n        direction TB\n        R1[\"[sli:p]\"]\n        R2[\"[sli:ps]\"]\n        R3[\"[slEpt]\"]\n        R4[\"[IN]\"]\n    end\n\n    %% Strict Linear Connections\n    C2 --> L1\n    L1 --> LX1 & LX2 & LX3 & LX4\n    LX1 --> R1\n    LX1 --> R2\n    LX1 --> R3\n    LX2 --> R2\n    LX3 --> R3\n    LX4 --> R4\n```\n\n**CLIP Architecture:**\n\n```{mermaid}\nflowchart TD\n    A[Picture] --> B[Image Encoder]\n    C[Text] --> D[Text Encoder]\n    B --> E[Image Embedding]\n    D --> F[Text Embedding]\n    E --> G[Similarity Score]\n    F --> G\n\n    style A fill:#e1f5dd\n    style C fill:#e1f5dd\n    style B fill:#d4e9f7\n    style D fill:#d4e9f7\n    style E fill:#fff3cd\n    style F fill:#fff3cd\n    style G fill:#f8d7da\n```\n\n**Multimodal LLMs:**\n\nMore recently, multimodal large language models have emerged that work quite differently from CLIP. Instead of creating separate embeddings and comparing them, these models integrate visual and textual information into a unified representation and can generate natural language descriptions or answers about images. \n\nI have to say, writing their code is also a bit funny. You basically have to build a pipeline where you create a 'chat template' and ask them to give you an output. I am not sure that is how you are supposed to use them, but it works.^[It works very slowly because they are extremely resource hungry. The reason this post waited this much was because I was waiting for results to come in.]\n\nModels like [Qwen3-Omni](https://github.com/QwenLM/Qwen3-Omni) take both images and text as input, process them through vision encoders and language models together, and generate coherent text outputs. Unlike CLIP's similarity metric, multimodal LLMs can provide richer, more nuanced interpretations of visual scenes and answer complex questions about them. We will use both of them and compare here.\n\n```{mermaid}\nflowchart TD\n    A[Picture] --> B[Vision Encoder]\n    C[Text Prompt] --> D[Tokenizer]\n    B --> E[Visual Tokens]\n    D --> F[Text Tokens]\n    E --> G[Unified LLM]\n    F --> G\n    G --> H[Generated Text Output]\n\n    style A fill:#e1f5dd\n    style C fill:#e1f5dd\n    style B fill:#d4e9f7\n    style D fill:#d4e9f7\n    style E fill:#fff3cd\n    style F fill:#fff3cd\n    style G fill:#ffd4e5\n    style H fill:#f8d7da\n```\n\nLastly, these experiments were conducted as a extended-PWI experiment, where participants were provided with a picture with a superimposed text on it. Neither the pictures, nor the tasks I improvise here does not have any relation to picture word interference task. It would be indeed interesting if we have an understanding how PWI would look like interms of LLM tasks. However it is far from what I would like to achieve here. If I have that idea I will probably submit a paper or an abstract somewhere :). \n\n\n### Predictions\n\nIf unaccusative actions (like \"boiling\" or \"melting\") are genuinely harder to see in pictures, or if the subjects are harder to visually identify in the scenes, we'd expect:\n- **Lower similarity scores** between the images and their target sentences\n- Evidence that models struggle to \"ground\" the sentence/entity in the visual input, in the form of subject saliency.\n\nIf that's the case, we have a problem—the onset latency effect might just be about picture difficulty.^[There are of course other ways to test this. For example Griffin & Bock (2000) used a free-production task where participants were not given an initial word to use with the pictures. They quantified how many different words they used for each picture and named that variable 'codability' of the picture and tested if codability was related to onset latency. Egurtzegi et al. (2022) used a similar approach.]\n\nBut if the similarity scores are **comparable or higher** for unaccusatives, then we can rule out the perceptual confound for now and be more confident that the effects reflect genuine linguistic processing.\n\n\n## Model Base\n\n### CLIP\n\n[CLIP (Contrastive Language-Image Pre-training)](https://openai.com/research/clip) is a neural network trained on 400 million image-text pairs from the internet. It learns to match images with their corresponding text descriptions by projecting both into a shared embedding space.\n\n#### Setting Up\n\nLet's start by loading the packages we'll need. I'm going to build this up step by step, just like I did when I first ran this analysis.\n\n::: {#db79cae4 .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport torch\nimport clip\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Set up plotting style\nsns.set_style(\"whitegrid\")\n# plt.rcParams['figure.figsize'] = (10, 6)\n```\n:::\n\n\nFirst, we need to load the CLIP model. I'm using the ViT-B/32 variant, which is a good balance between performance and computational efficiency:\n\n::: {#affad342 .cell execution_count=2}\n``` {.python .cell-code}\n# Load two decoder CLIP model\n# Note: We use CPU for everything if MPS is detected to avoid moondream2 issues\nif torch.cuda.is_available():\n    device = \"cuda\"\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    device = \"cpu\" \nelse:\n    device = \"cpu\"\n\nmodel_clip, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n\nprint(f\"Using device: {device}\")\nprint(f\"CLIP model loaded successfully!\")\n```\n:::\n\n\nNow let's also load a multimodal LLM for comparison. We'll use Qwen-VL-Chat, a powerful vision-language model:\n\n::: {#9f678a51 .cell execution_count=3}\n``` {.python .cell-code}\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\nimport transformers\nimport torch\nfrom transformers.generation.beam_search import BeamSearchScorer\ntransformers.BeamSearchScorer = BeamSearchScorer\n\n# Load Qwen-VL-Chat model\nmodel_id = \"Qwen/Qwen-VL-Chat\"\n\nmodel_vlm = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    dtype=torch.float32\n).to('cpu')\ntokenizer_vlm = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n\n# Create the streamer\nstreamer = TextStreamer(tokenizer_vlm, skip_prompt=True)\n```\n:::\n\n\n## The Data Structure\n\nMy experimental materials consist of 24 scenes:\n- 12 **unergative** scenes (swimming, running, barking, etc.)\n- 12 **unaccusative** scenes (boiling, shrinking, sinking, etc.)\n\nEach scene pairs a character (octopus, ballerina, chef, etc.) with an action. Let's create a dataframe with our materials:\n\n::: {#fada1d7a .cell execution_count=4}\n``` {.python .cell-code}\n# Unergative scenes\ndf_unerg = pd.DataFrame({\n    \"Filename\": [\n        \"./pictures/octopus_swim.jpg\",\n        \"./pictures/ballerina_run.jpg\",\n        \"./pictures/boy_float.jpg\",\n        \"./pictures/chef_yell.jpg\",\n        \"./pictures/clown_walk.jpg\",\n        \"./pictures/cowboy_wink.jpg\",\n        \"./pictures/dog_bark.jpg\",\n        \"./pictures/monkey_sleep.jpg\",\n        \"./pictures/penguin_sneeze.jpg\",\n        \"./pictures/pirate_cough.jpg\",\n        \"./pictures/rabbit_smile.jpg\",\n        \"./pictures/snail_crawl.jpg\",\n    ],\n    \"Sentence\": [\n        \"The octopus is swimming.\",\n        \"The ballerina is running.\",\n        \"The boy is floating.\",\n        \"The chef is yelling.\",\n        \"The clown is walking.\",\n        \"The cowboy is winking.\",\n        \"The dog is barking.\",\n        \"The monkey is sleeping.\",\n        \"The penguin is sneezing.\",\n        \"The pirate is coughing.\",\n        \"The rabbit is smiling.\",\n        \"The snail is crawling.\",\n    ]\n})\n\n# Unaccusative scenes\ndf_unacc = pd.DataFrame({\n    \"Filename\": [\n        \"./pictures/octopus_boil.jpg\",\n        \"./pictures/ballerina_shrink.jpg\",\n        \"./pictures/boy_yawn.jpg\",\n        \"./pictures/chef_drown.jpg\",\n        \"./pictures/clown_grow.jpg\",\n        \"./pictures/cowboy_fall.jpg\",\n        \"./pictures/dog_spin.jpg\",\n        \"./pictures/monkey_trip.jpg\",\n        \"./pictures/penguin_bounce.jpg\",\n        \"./pictures/pirate_sink.jpg\",\n        \"./pictures/rabbit_shake.jpg\",\n        \"./pictures/snail_melt.jpg\",\n    ],\n    \"Sentence\": [\n        \"The octopus is boiling.\",\n        \"The ballerina is shrinking.\",\n        \"The boy is yawning.\",\n        \"The chef is drowning.\",\n        \"The clown is growing.\",\n        \"The cowboy is falling.\",\n        \"The dog is spinning.\",\n        \"The monkey is tripping.\",\n        \"The penguin is bouncing.\",\n        \"The pirate is sinking.\",\n        \"The rabbit is shaking.\",\n        \"The snail is melting.\",\n    ]\n})\n```\n:::\n\n\n## Computing Similarity Scores\n\nNow for the main event. For each image-sentence pair, we'll compute CLIP's similarity score. This tells us how well the model thinks the image matches the text.\n\n::: {#f0b48857 .cell execution_count=5}\n``` {.python .cell-code}\ndef compute_clip_similarity(df, model, preprocess, device):\n    \"\"\"\n    Compute CLIP similarity scores for image-text pairs.\n\n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        DataFrame with 'Filename' and 'Sentence' columns\n    model : CLIP model\n        Loaded CLIP model\n    preprocess : function\n        CLIP preprocessing function\n    device : str\n        'cuda' or 'cpu'\n\n    Returns:\n    --------\n    pandas.DataFrame\n        Original dataframe with added 'CLIP_Similarity' column\n    \"\"\"\n    similarity_scores = []\n\n    for _, row in df.iterrows():\n        img_path = row['Filename']\n        text = row['Sentence']\n\n        # Preprocess image and tokenize text\n        img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n        text_tokenized = clip.tokenize([text]).to(device)\n\n        # Compute similarity\n        with torch.no_grad():\n            logits_per_image, _ = model(img, text_tokenized)\n            similarity_score = logits_per_image.item()\n\n        similarity_scores.append(similarity_score)\n\n    # Add scores to dataframe\n    df_copy = df.copy()\n    df_copy['CLIP_Similarity'] = similarity_scores\n\n    return df_copy\n\ndef compute_subject_salience(df, model, preprocess, device):\n    \"\"\"\n    Compute CLIP similarity scores for subject noun alone.\n    This measures how visually salient/easy to identify the subject is.\n    \n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        DataFrame with 'Filename' and 'Sentence' columns\n    model : CLIP model\n        Loaded CLIP model\n    preprocess : function\n        CLIP preprocessing function\n    device : str\n        'cuda' or 'cpu'\n    \n    Returns:\n    --------\n    pandas.DataFrame\n        Original dataframe with added 'Subject_Salience' column\n    \"\"\"\n    subject_scores = []\n    \n    for _, row in df.iterrows():\n        img_path = row['Filename']\n        sentence = row['Sentence']\n        \n        # Extract subject noun (assumes format \"The X is ...\")\n        # Extract word after \"The \" and before \" is\"\n        subject = sentence.split(\"The \")[1].split(\" is\")[0]\n        \n        # Preprocess image and tokenize subject\n        img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n        text_tokenized = clip.tokenize([subject]).to(device)\n        \n        # Compute similarity\n        with torch.no_grad():\n            logits_per_image, _ = model(img, text_tokenized)\n            similarity_score = logits_per_image.item()\n        \n        subject_scores.append(similarity_score)\n    \n    df_copy = df.copy()\n    df_copy['Subject_Salience'] = subject_scores\n    \n    return df_copy\n```\n:::\n\n\nWe can also use a multimodal LLM to verify the image-sentence match in a different way. Instead of computing similarity scores, we'll ask the model to rate how well the sentence describes the image:\n\n::: {#8ea9514b .cell execution_count=6}\n``` {.python .cell-code}\ndef compute_qwen_scores(df, model, tokenizer, streamer=None):\n    \"\"\"\n    Compute verification scores using Qwen-VL-Chat multimodal LLM.\n\n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        DataFrame with 'Filename' and 'Sentence' columns\n    model : Qwen-VL-Chat model\n        Loaded Qwen model\n    tokenizer : AutoTokenizer\n        Qwen tokenizer\n    streamer : TextStreamer, optional\n        Streamer for real-time output\n\n    Returns:\n    --------\n    pandas.DataFrame\n        Original dataframe with added 'VLM_Score' and 'VLM_Response' columns\n    \"\"\"\n    import re\n    scores = []\n    responses = []\n\n    for idx, row in df.iterrows():\n        img_path = row['Filename']\n        sentence = row['Sentence']\n\n        # Create query for Qwen-VL-Chat\n        query = tokenizer.from_list_format([\n            {'image': img_path},\n            {'text': f'Rate how well this sentence describes the image: \"{sentence}\"\\nScore from 1-10 (1=mismatch, 10=perfect match). Reply with just the number.'},\n        ])\n\n        # Generate response\n        with torch.no_grad():\n            response, _ = model.chat(tokenizer, query=query, history=None, streamer=streamer)\n\n        # Extract numeric score\n        try:\n            match = re.search(r'(\\d+(?:\\.\\d+)?)', response)\n            score = float(match.group(1)) if match else 5.0\n            score = min(10.0, max(1.0, score))  # Clamp to 1-10\n        except:\n            score = 5.0\n\n        scores.append(score)\n        responses.append(response)\n\n    df_copy = df.copy()\n    df_copy['VLM_Score'] = scores\n    df_copy['VLM_Response'] = responses\n\n    return df_copy\n```\n:::\n\n\nLet's run this on both datasets. To avoid re-computing the slow VLM scores on every render, we cache results to a CSV file:\n\n::: {#95f175dc .cell execution_count=7}\n``` {.python .cell-code}\nimport os\n\nCACHE_FILE = \"./cached_scores.csv\"\n\n\nif os.path.exists(CACHE_FILE):\n    df_all = pd.read_csv(CACHE_FILE)\nelse:\n    # Compute CLIP similarities\n    df_unerg_clip = compute_clip_similarity(df_unerg, model_clip, preprocess, device)\n    df_unacc_clip = compute_clip_similarity(df_unacc, model_clip, preprocess, device)\n    \n    # Compute subject salience scores\n    df_unerg_subj = compute_subject_salience(df_unerg, model_clip, preprocess, device)\n    df_unacc_subj = compute_subject_salience(df_unacc, model_clip, preprocess, device)\n\n    # Compute Qwen-VL scores\n    df_unerg_vlm = compute_qwen_scores(df_unerg, model_vlm, tokenizer_vlm, streamer=streamer)\n    df_unacc_vlm = compute_qwen_scores(df_unacc, model_vlm, tokenizer_vlm, streamer=streamer)\n\n    # Combine CLIP scores with VLM scores and subject salience\n    df_unerg_scored = df_unerg_clip.copy()\n    df_unerg_scored['Subject_Salience'] = df_unerg_subj['Subject_Salience']\n    df_unerg_scored['VLM_Score'] = df_unerg_vlm['VLM_Score']\n    df_unerg_scored['VLM_Response'] = df_unerg_vlm['VLM_Response']\n    df_unerg_scored['VerbType'] = 'Unergative'\n\n    df_unacc_scored = df_unacc_clip.copy()\n    df_unacc_scored['Subject_Salience'] = df_unacc_subj['Subject_Salience']\n    df_unacc_scored['VLM_Score'] = df_unacc_vlm['VLM_Score']\n    df_unacc_scored['VLM_Response'] = df_unacc_vlm['VLM_Response']\n    df_unacc_scored['VerbType'] = 'Unaccusative'\n\n    # Combine for analysis\n    df_all = pd.concat([df_unerg_scored, df_unacc_scored], ignore_index=True)\n\n    # Save to cache\n    df_all.to_csv(CACHE_FILE, index=False)\n\nprint(df_all.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              Filename                   Sentence  CLIP_Similarity  \\\n0   ./octopus_swim.jpg   The octopus is swimming.        29.137495   \n1  ./ballerina_run.jpg  The ballerina is running.        27.731918   \n2      ./boy_float.jpg       The boy is floating.        20.843243   \n3      ./chef_yell.jpg       The chef is yelling.        27.878561   \n4     ./clown_walk.jpg      The clown is walking.        27.077477   \n\n   Subject_Salience  VLM_Score  VLM_Response    VerbType  \n0         28.454519        8.0             8  Unergative  \n1         25.250607        7.0             7  Unergative  \n2         21.628622        1.0             1  Unergative  \n3         28.490120        8.0             8  Unergative  \n4         26.241133        8.0             8  Unergative  \n```\n:::\n:::\n\n\n## Descriptive Results\n\nLet's start by looking at the descriptive statistics across all three metrics:\n\n::: {#056c67bb .cell execution_count=8}\n``` {.python .cell-code}\n# Create comparison plot with all three metrics\nfig, axes = plt.subplots(1, 3, figsize=(8, 5))\n\n# CLIP full sentence results\nsns.pointplot(data=df_all, x='VerbType', y='CLIP_Similarity',\n              hue='VerbType', palette=['#3498db', '#e74c3c'], \n              ax=axes[0], errorbar='ci', capsize=0.1, \n              linestyle='none', markers='o', legend=False)\nsns.stripplot(data=df_all, x='VerbType', y='CLIP_Similarity',\n              color='black', alpha=0.5, size=8, ax=axes[0], jitter=0.2)\n\naxes[0].set_xlabel('Verb Type', fontsize=14, fontweight='bold')\naxes[0].set_ylabel('CLIP Similarity Score', fontsize=14, fontweight='bold')\naxes[0].set_title('Full Sentence Similarity',\n                  fontsize=16, fontweight='bold', pad=20)\n\nfor verb_type in ['Unergative', 'Unaccusative']:\n    mean_val = df_all[df_all['VerbType'] == verb_type]['CLIP_Similarity'].mean()\n    axes[0].text(0 if verb_type == 'Unergative' else 1, mean_val + 1,\n                 f'M = {mean_val:.2f}', ha='center', fontsize=12, fontweight='bold')\n\n# Subject salience results\nsns.pointplot(data=df_all, x='VerbType', y='Subject_Salience',\n              hue='VerbType', palette=['#3498db', '#e74c3c'], \n              ax=axes[1], errorbar='ci', capsize=0.1, \n              linestyle='none', markers='o', legend=False)\nsns.stripplot(data=df_all, x='VerbType', y='Subject_Salience',\n              color='black', alpha=0.5, size=8, ax=axes[1], jitter=0.2)\n\naxes[1].set_xlabel('Verb Type', fontsize=14, fontweight='bold')\naxes[1].set_ylabel('Subject Salience Score', fontsize=14, fontweight='bold')\naxes[1].set_title('Subject Noun Identifiability',\n                  fontsize=16, fontweight='bold', pad=20)\n\nfor verb_type in ['Unergative', 'Unaccusative']:\n    mean_val = df_all[df_all['VerbType'] == verb_type]['Subject_Salience'].mean()\n    axes[1].text(0 if verb_type == 'Unergative' else 1, mean_val + 0.5,\n                 f'M = {mean_val:.2f}', ha='center', fontsize=12, fontweight='bold')\n\n# VLM results\nsns.pointplot(data=df_all, x='VerbType', y='VLM_Score',\n              hue='VerbType', palette=['#3498db', '#e74c3c'], \n              ax=axes[2], errorbar='ci', capsize=0.1, \n              linestyle='none', markers='o', legend=False)\nsns.stripplot(data=df_all, x='VerbType', y='VLM_Score',\n              color='black', alpha=0.5, size=8, ax=axes[2], jitter=0.2)\n\naxes[2].set_xlabel('Verb Type', fontsize=14, fontweight='bold')\naxes[2].set_ylabel('Qwen-VL Match Score (1-10)', fontsize=14, fontweight='bold')\naxes[2].set_title('Scene Verification (Qwen-VL)',\n                  fontsize=16, fontweight='bold', pad=20)\n\nfor verb_type in ['Unergative', 'Unaccusative']:\n    mean_val = df_all[df_all['VerbType'] == verb_type]['VLM_Score'].mean()\n    axes[2].text(0 if verb_type == 'Unergative' else 1, mean_val + 0.3,\n                 f'M = {mean_val:.2f}', ha='center', fontsize=12, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('./model_comparison_plot.png', dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=811 height=470}\n:::\n:::\n\n\n### A Deeper Dive with Bayesian Analysis\n\nWhile the plots above give us a good first look, they don't tell the whole story. To really understand the strength of the evidence, we need to go beyond just comparing averages. This is where Bayesian analysis comes in.\n\nInstead of just getting a single number for the difference, a Bayesian regression gives us a full range of plausible values for the effect of `VerbType` on our scores, along with a measure of our certainty.\n\nFor the nerds out there, I used Pyro to run three separate models: two simple linear regressions for the CLIP and Subject Salience scores, and an ordered logistic regression for the VLM scores (since they are on a 1-10 scale). In all models, the key parameter is `beta`, which represents the estimated difference between unaccusative and unergative verbs.\n\nHere's the code to set up and run the models:\n\n::: {#35a185f2 .cell execution_count=9}\n``` {.python .cell-code}\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import MCMC, NUTS\n\n# Prepare data for Pyro\n# We'll center the scores and code VerbType numerically\ndf_pyro = df_all.copy()\ndf_pyro['VerbType_num'] = df_pyro['VerbType'].map({'Unergative': -0.5, 'Unaccusative': 0.5})\ndf_pyro['CLIP_centered'] = df_pyro['CLIP_Similarity'] - df_pyro['CLIP_Similarity'].mean()\ndf_pyro['Subject_centered'] = df_pyro['Subject_Salience'] - df_pyro['Subject_Salience'].mean()\nvlm_score_tensor = torch.tensor(df_pyro['VLM_Score'].values, dtype=torch.long)\n\n# Convert to tensors\nverb_type_tensor = torch.tensor(df_pyro['VerbType_num'].values, dtype=torch.float32)\nclip_tensor = torch.tensor(df_pyro['CLIP_centered'].values, dtype=torch.float32)\nsubject_tensor = torch.tensor(df_pyro['Subject_centered'].values, dtype=torch.float32)\n\n# --- Model for CLIP Similarity ---\ndef clip_model(verb_type, obs=None):\n    intercept = pyro.sample('intercept', dist.Normal(0., 10.))\n    beta = pyro.sample('beta', dist.Normal(0., 10.))\n    sigma = pyro.sample('sigma', dist.HalfNormal(10.))\n    mu = intercept + beta * verb_type\n    with pyro.plate('data', len(verb_type)):\n        pyro.sample('obs', dist.Normal(mu, sigma), obs=obs)\n\n# --- Model for Subject Salience ---\ndef subject_model(verb_type, obs=None):\n    intercept = pyro.sample('intercept', dist.Normal(0., 10.))\n    beta = pyro.sample('beta', dist.Normal(0., 10.))\n    sigma = pyro.sample('sigma', dist.HalfNormal(10.))\n    mu = intercept + beta * verb_type\n    with pyro.plate('data', len(verb_type)):\n        pyro.sample('obs', dist.Normal(mu, sigma), obs=obs)\n        \n# --- Model for VLM Score (Ordered Logistic) ---\nk_categories = vlm_score_tensor.max().item() + 1\nk_cutpoints = k_categories - 1\ndef vlm_model(verb_type, obs=None):\n    alpha = pyro.sample('alpha', dist.Normal(0., 10.))\n    beta = pyro.sample('beta', dist.Normal(0., 10.))\n    with pyro.plate(\"cutpoints_plate\", k_cutpoints):\n        raw_cutpoints = pyro.sample('raw_cutpoints', dist.Normal(torch.arange(k_cutpoints).float(), 1.))\n    cutpoints = torch.sort(raw_cutpoints)[0]\n    latent_propensity = alpha + beta * verb_type\n    with pyro.plate('data', len(verb_type)):\n        pyro.sample('obs', dist.OrderedLogistic(latent_propensity, cutpoints), obs=obs)\n\n# Run the MCMC samplers\nmcmc_clip = MCMC(NUTS(clip_model), num_samples=2000, warmup_steps=1000)\nmcmc_clip.run(verb_type_tensor, clip_tensor)\nclip_samples = mcmc_clip.get_samples()\n\nmcmc_subject = MCMC(NUTS(subject_model), num_samples=2000, warmup_steps=1000)\nmcmc_subject.run(verb_type_tensor, subject_tensor)\nsubject_samples = mcmc_subject.get_samples()\n\nmcmc_vlm = MCMC(NUTS(vlm_model), num_samples=2000, warmup_steps=1000, num_chains=1)\nmcmc_vlm.run(verb_type_tensor, vlm_score_tensor)\nvlm_samples = mcmc_vlm.get_samples()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\rWarmup:   0%|          | 0/3000 [00:00, ?it/s]\rWarmup:   0%|          | 1/3000 [00:00,  5.32it/s, step size=8.99e-01, acc. prob=1.000]\rWarmup:   0%|          | 14/3000 [00:00, 54.00it/s, step size=1.02e-01, acc. prob=0.758]\rWarmup:   1%|          | 21/3000 [00:00, 55.95it/s, step size=1.70e-01, acc. prob=0.779]\rWarmup:   1%|          | 28/3000 [00:00, 39.49it/s, step size=2.29e-01, acc. prob=0.787]\rWarmup:   1%|          | 34/3000 [00:00, 36.75it/s, step size=1.32e-01, acc. prob=0.783]\rWarmup:   1%|▏         | 39/3000 [00:01, 36.89it/s, step size=1.72e-01, acc. prob=0.787]\rWarmup:   1%|▏         | 44/3000 [00:01, 37.75it/s, step size=5.76e-02, acc. prob=0.778]\rWarmup:   2%|▏         | 49/3000 [00:01, 38.79it/s, step size=1.51e-01, acc. prob=0.788]\rWarmup:   2%|▏         | 58/3000 [00:01, 50.08it/s, step size=3.10e-01, acc. prob=0.795]\rWarmup:   2%|▏         | 67/3000 [00:01, 56.62it/s, step size=1.32e-01, acc. prob=0.789]\rWarmup:   2%|▏         | 73/3000 [00:01, 53.91it/s, step size=1.57e-01, acc. prob=0.791]\rWarmup:   3%|▎         | 83/3000 [00:01, 64.24it/s, step size=1.13e-01, acc. prob=0.789]\rWarmup:   3%|▎         | 90/3000 [00:01, 57.58it/s, step size=1.54e-01, acc. prob=0.792]\rWarmup:   3%|▎         | 98/3000 [00:02, 60.31it/s, step size=2.45e-01, acc. prob=0.795]\rWarmup:   4%|▍         | 115/3000 [00:02, 86.98it/s, step size=4.17e-01, acc. prob=0.787]\rWarmup:   4%|▍         | 133/3000 [00:02, 107.84it/s, step size=3.80e-01, acc. prob=0.787]\rWarmup:   5%|▍         | 149/3000 [00:02, 121.27it/s, step size=2.47e+00, acc. prob=0.789]\rWarmup:   6%|▌         | 165/3000 [00:02, 127.30it/s, step size=4.11e-01, acc. prob=0.782]\rWarmup:   6%|▌         | 179/3000 [00:02, 116.15it/s, step size=1.46e+00, acc. prob=0.785]\rWarmup:   6%|▋         | 192/3000 [00:02, 117.52it/s, step size=6.64e-01, acc. prob=0.784]\rWarmup:   7%|▋         | 216/3000 [00:02, 149.99it/s, step size=1.59e+00, acc. prob=0.786]\rWarmup:   8%|▊         | 237/3000 [00:02, 166.31it/s, step size=2.55e+00, acc. prob=0.788]\rWarmup:   8%|▊         | 255/3000 [00:02, 164.93it/s, step size=4.66e-01, acc. prob=0.781]\rWarmup:   9%|▉         | 272/3000 [00:03, 149.06it/s, step size=9.45e-01, acc. prob=0.783]\rWarmup:  10%|▉         | 296/3000 [00:03, 171.33it/s, step size=1.07e+00, acc. prob=0.784]\rWarmup:  11%|█         | 319/3000 [00:03, 186.88it/s, step size=9.72e-01, acc. prob=0.785]\rWarmup:  12%|█▏        | 346/3000 [00:03, 209.26it/s, step size=9.89e-01, acc. prob=0.785]\rWarmup:  12%|█▏        | 373/3000 [00:03, 225.61it/s, step size=6.43e-01, acc. prob=0.785]\rWarmup:  14%|█▎        | 410/3000 [00:03, 265.42it/s, step size=1.18e+00, acc. prob=0.787]\rWarmup:  15%|█▍        | 444/3000 [00:03, 286.25it/s, step size=1.21e+00, acc. prob=0.788]\rWarmup:  16%|█▌        | 473/3000 [00:03, 276.33it/s, step size=1.29e+00, acc. prob=0.788]\rWarmup:  17%|█▋        | 501/3000 [00:03, 251.81it/s, step size=9.21e-01, acc. prob=0.788]\rWarmup:  18%|█▊        | 528/3000 [00:04, 256.41it/s, step size=1.04e+00, acc. prob=0.788]\rWarmup:  18%|█▊        | 555/3000 [00:04, 249.17it/s, step size=8.16e-01, acc. prob=0.788]\rWarmup:  19%|█▉        | 582/3000 [00:04, 254.31it/s, step size=7.11e-01, acc. prob=0.789]\rWarmup:  21%|██        | 617/3000 [00:04, 277.60it/s, step size=9.55e-01, acc. prob=0.789]\rWarmup:  22%|██▏       | 646/3000 [00:04, 278.02it/s, step size=6.50e-01, acc. prob=0.789]\rWarmup:  22%|██▎       | 675/3000 [00:04, 280.49it/s, step size=1.34e+00, acc. prob=0.790]\rWarmup:  24%|██▎       | 707/3000 [00:04, 291.77it/s, step size=9.23e-01, acc. prob=0.790]\rWarmup:  25%|██▍       | 737/3000 [00:04, 278.12it/s, step size=8.46e-01, acc. prob=0.790]\rWarmup:  26%|██▌       | 772/3000 [00:04, 293.59it/s, step size=1.02e+00, acc. prob=0.791]\rWarmup:  27%|██▋       | 805/3000 [00:05, 302.15it/s, step size=1.29e+00, acc. prob=0.791]\rWarmup:  28%|██▊       | 836/3000 [00:05, 288.08it/s, step size=1.15e+00, acc. prob=0.791]\rWarmup:  29%|██▉       | 870/3000 [00:05, 301.31it/s, step size=1.36e+00, acc. prob=0.792]\rWarmup:  30%|███       | 902/3000 [00:05, 303.05it/s, step size=9.16e-01, acc. prob=0.792]\rWarmup:  31%|███       | 933/3000 [00:05, 276.56it/s, step size=8.12e-01, acc. prob=0.792]\rWarmup:  32%|███▏      | 962/3000 [00:05, 239.29it/s, step size=4.39e-01, acc. prob=0.790]\rWarmup:  33%|███▎      | 988/3000 [00:05, 220.40it/s, step size=7.48e-01, acc. prob=0.791]\rSample:  34%|███▎      | 1011/3000 [00:05, 215.27it/s, step size=6.90e-01, acc. prob=0.910]\rSample:  34%|███▍      | 1034/3000 [00:06, 211.77it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  35%|███▌      | 1056/3000 [00:06, 204.58it/s, step size=6.90e-01, acc. prob=0.932]\rSample:  36%|███▌      | 1079/3000 [00:06, 210.12it/s, step size=6.90e-01, acc. prob=0.922]\rSample:  37%|███▋      | 1101/3000 [00:06, 211.52it/s, step size=6.90e-01, acc. prob=0.922]\rSample:  38%|███▊      | 1125/3000 [00:06, 216.14it/s, step size=6.90e-01, acc. prob=0.922]\rSample:  38%|███▊      | 1147/3000 [00:06, 216.25it/s, step size=6.90e-01, acc. prob=0.921]\rSample:  39%|███▉      | 1169/3000 [00:06, 215.16it/s, step size=6.90e-01, acc. prob=0.925]\rSample:  40%|███▉      | 1191/3000 [00:06, 197.60it/s, step size=6.90e-01, acc. prob=0.924]\rSample:  40%|████      | 1212/3000 [00:06, 194.57it/s, step size=6.90e-01, acc. prob=0.925]\rSample:  41%|████      | 1232/3000 [00:07, 164.80it/s, step size=6.90e-01, acc. prob=0.927]\rSample:  42%|████▏     | 1250/3000 [00:07, 149.31it/s, step size=6.90e-01, acc. prob=0.928]\rSample:  42%|████▏     | 1266/3000 [00:07, 148.10it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  43%|████▎     | 1284/3000 [00:07, 154.43it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  43%|████▎     | 1300/3000 [00:07, 152.56it/s, step size=6.90e-01, acc. prob=0.928]\rSample:  44%|████▍     | 1319/3000 [00:07, 161.90it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  45%|████▍     | 1337/3000 [00:07, 166.80it/s, step size=6.90e-01, acc. prob=0.928]\rSample:  45%|████▌     | 1358/3000 [00:07, 177.18it/s, step size=6.90e-01, acc. prob=0.928]\rSample:  46%|████▌     | 1383/3000 [00:07, 197.51it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  47%|████▋     | 1408/3000 [00:08, 212.04it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  48%|████▊     | 1432/3000 [00:08, 217.29it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  49%|████▊     | 1456/3000 [00:08, 223.35it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  49%|████▉     | 1480/3000 [00:08, 227.78it/s, step size=6.90e-01, acc. prob=0.931]\rSample:  50%|█████     | 1505/3000 [00:08, 230.80it/s, step size=6.90e-01, acc. prob=0.931]\rSample:  51%|█████     | 1530/3000 [00:08, 233.49it/s, step size=6.90e-01, acc. prob=0.931]\rSample:  52%|█████▏    | 1557/3000 [00:08, 243.66it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  53%|█████▎    | 1582/3000 [00:08, 245.50it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  54%|█████▎    | 1607/3000 [00:08, 239.32it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  54%|█████▍    | 1631/3000 [00:09, 239.22it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  55%|█████▌    | 1656/3000 [00:09, 242.17it/s, step size=6.90e-01, acc. prob=0.931]\rSample:  56%|█████▌    | 1682/3000 [00:09, 244.55it/s, step size=6.90e-01, acc. prob=0.931]\rSample:  57%|█████▋    | 1709/3000 [00:09, 250.17it/s, step size=6.90e-01, acc. prob=0.931]\rSample:  58%|█████▊    | 1735/3000 [00:09, 252.89it/s, step size=6.90e-01, acc. prob=0.931]\rSample:  59%|█████▊    | 1762/3000 [00:09, 257.06it/s, step size=6.90e-01, acc. prob=0.932]\rSample:  60%|█████▉    | 1790/3000 [00:09, 259.23it/s, step size=6.90e-01, acc. prob=0.931]\rSample:  61%|██████    | 1816/3000 [00:09, 253.30it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  61%|██████▏   | 1842/3000 [00:09, 254.84it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  62%|██████▏   | 1868/3000 [00:09, 243.39it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  63%|██████▎   | 1895/3000 [00:10, 250.69it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  64%|██████▍   | 1921/3000 [00:10, 245.08it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  65%|██████▍   | 1946/3000 [00:10, 242.90it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  66%|██████▌   | 1971/3000 [00:10, 224.74it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  66%|██████▋   | 1994/3000 [00:10, 222.83it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  67%|██████▋   | 2018/3000 [00:10, 227.25it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  68%|██████▊   | 2043/3000 [00:10, 231.43it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  69%|██████▉   | 2067/3000 [00:10, 224.78it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  70%|██████▉   | 2090/3000 [00:10, 224.51it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  71%|███████   | 2117/3000 [00:11, 235.51it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  72%|███████▏  | 2147/3000 [00:11, 251.54it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  72%|███████▏  | 2173/3000 [00:11, 251.86it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  73%|███████▎  | 2199/3000 [00:11, 249.47it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  74%|███████▍  | 2228/3000 [00:11, 260.20it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  75%|███████▌  | 2255/3000 [00:11, 262.96it/s, step size=6.90e-01, acc. prob=0.931]\rSample:  76%|███████▌  | 2284/3000 [00:11, 267.93it/s, step size=6.90e-01, acc. prob=0.931]\rSample:  77%|███████▋  | 2313/3000 [00:11, 273.33it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  78%|███████▊  | 2341/3000 [00:11, 273.01it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  79%|███████▉  | 2369/3000 [00:11, 274.66it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  80%|███████▉  | 2398/3000 [00:12, 277.81it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  81%|████████  | 2428/3000 [00:12, 283.95it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  82%|████████▏ | 2457/3000 [00:12, 267.04it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  83%|████████▎ | 2484/3000 [00:12, 267.72it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  84%|████████▎ | 2511/3000 [00:12, 264.42it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  85%|████████▍ | 2538/3000 [00:12, 260.01it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  86%|████████▌ | 2565/3000 [00:12, 257.99it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  86%|████████▋ | 2591/3000 [00:12, 249.38it/s, step size=6.90e-01, acc. prob=0.929]\rSample:  87%|████████▋ | 2617/3000 [00:12, 238.93it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  88%|████████▊ | 2642/3000 [00:13, 237.25it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  89%|████████▉ | 2667/3000 [00:13, 240.08it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  90%|████████▉ | 2692/3000 [00:13, 239.77it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  91%|█████████ | 2717/3000 [00:13, 234.47it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  91%|█████████▏| 2741/3000 [00:13, 225.85it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  92%|█████████▏| 2766/3000 [00:13, 232.05it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  93%|█████████▎| 2796/3000 [00:13, 251.22it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  94%|█████████▍| 2822/3000 [00:13, 248.16it/s, step size=6.90e-01, acc. prob=0.930]\rSample:  95%|█████████▍| 2847/3000 [00:13, 239.03it/s, step size=6.90e-01, acc. prob=0.931]\rSample:  96%|█████████▌| 2874/3000 [00:13, 246.72it/s, step size=6.90e-01, acc. prob=0.931]\rSample:  97%|█████████▋| 2904/3000 [00:14, 261.95it/s, step size=6.90e-01, acc. prob=0.931]\rSample:  98%|█████████▊| 2931/3000 [00:14, 260.85it/s, step size=6.90e-01, acc. prob=0.931]\rSample:  99%|█████████▊| 2958/3000 [00:14, 262.09it/s, step size=6.90e-01, acc. prob=0.931]\rSample: 100%|█████████▉| 2990/3000 [00:14, 277.84it/s, step size=6.90e-01, acc. prob=0.930]\rSample: 100%|██████████| 3000/3000 [00:14, 207.75it/s, step size=6.90e-01, acc. prob=0.930]\n\rWarmup:   0%|          | 0/3000 [00:00, ?it/s]\rWarmup:   0%|          | 8/3000 [00:00, 71.68it/s, step size=8.63e-02, acc. prob=0.776]\rWarmup:   1%|          | 16/3000 [00:00, 65.81it/s, step size=1.04e-01, acc. prob=0.792]\rWarmup:   1%|          | 23/3000 [00:00, 62.66it/s, step size=2.33e-01, acc. prob=0.806]\rWarmup:   1%|          | 30/3000 [00:00, 60.43it/s, step size=5.75e-02, acc. prob=0.788]\rWarmup:   1%|          | 37/3000 [00:00, 60.82it/s, step size=2.98e-01, acc. prob=0.807]\rWarmup:   2%|▏         | 45/3000 [00:00, 66.17it/s, step size=1.14e-01, acc. prob=0.797]\rWarmup:   2%|▏         | 55/3000 [00:00, 71.24it/s, step size=8.20e-02, acc. prob=0.795]\rWarmup:   2%|▏         | 63/3000 [00:00, 71.65it/s, step size=3.11e-01, acc. prob=0.805]\rWarmup:   2%|▏         | 72/3000 [00:01, 74.13it/s, step size=2.39e-01, acc. prob=0.803]\rWarmup:   3%|▎         | 80/3000 [00:01, 74.80it/s, step size=1.11e-01, acc. prob=0.798]\rWarmup:   3%|▎         | 88/3000 [00:01, 63.05it/s, step size=2.27e-01, acc. prob=0.802]\rWarmup:   3%|▎         | 96/3000 [00:01, 65.47it/s, step size=1.85e-01, acc. prob=0.801]\rWarmup:   4%|▍         | 118/3000 [00:01, 104.81it/s, step size=1.55e+00, acc. prob=0.794]\rWarmup:   5%|▍         | 145/3000 [00:01, 147.86it/s, step size=1.12e+00, acc. prob=0.793]\rWarmup:   5%|▌         | 163/3000 [00:01, 148.65it/s, step size=4.37e-01, acc. prob=0.789]\rWarmup:   6%|▌         | 186/3000 [00:01, 169.02it/s, step size=9.61e-01, acc. prob=0.791]\rWarmup:   7%|▋         | 214/3000 [00:01, 198.36it/s, step size=9.85e-01, acc. prob=0.791]\rWarmup:   8%|▊         | 243/3000 [00:02, 223.05it/s, step size=9.15e-01, acc. prob=0.792]\rWarmup:   9%|▉         | 266/3000 [00:02, 219.35it/s, step size=7.35e-01, acc. prob=0.792]\rWarmup:  10%|▉         | 290/3000 [00:02, 223.90it/s, step size=3.82e-01, acc. prob=0.791]\rWarmup:  10%|█         | 315/3000 [00:02, 229.24it/s, step size=7.24e-01, acc. prob=0.792]\rWarmup:  11%|█▏        | 342/3000 [00:02, 238.69it/s, step size=5.58e-01, acc. prob=0.792]\rWarmup:  12%|█▏        | 371/3000 [00:02, 253.51it/s, step size=7.66e-01, acc. prob=0.793]\rWarmup:  13%|█▎        | 403/3000 [00:02, 272.96it/s, step size=4.47e-01, acc. prob=0.792]\rWarmup:  14%|█▍        | 431/3000 [00:02, 269.04it/s, step size=8.82e-01, acc. prob=0.793]\rWarmup:  15%|█▌        | 460/3000 [00:02, 274.38it/s, step size=2.80e+00, acc. prob=0.793]\rWarmup:  16%|█▋        | 488/3000 [00:03, 263.86it/s, step size=2.69e-01, acc. prob=0.791]\rWarmup:  17%|█▋        | 515/3000 [00:03, 255.86it/s, step size=8.96e-01, acc. prob=0.792]\rWarmup:  18%|█▊        | 547/3000 [00:03, 273.74it/s, step size=1.54e+00, acc. prob=0.792]\rWarmup:  19%|█▉        | 577/3000 [00:03, 273.23it/s, step size=5.80e-01, acc. prob=0.791]\rWarmup:  20%|██        | 607/3000 [00:03, 278.14it/s, step size=9.28e-01, acc. prob=0.792]\rWarmup:  21%|██        | 637/3000 [00:03, 280.27it/s, step size=1.18e+00, acc. prob=0.792]\rWarmup:  22%|██▏       | 669/3000 [00:03, 290.91it/s, step size=9.07e-01, acc. prob=0.792]\rWarmup:  23%|██▎       | 703/3000 [00:03, 303.81it/s, step size=1.17e+00, acc. prob=0.793]\rWarmup:  25%|██▍       | 736/3000 [00:03, 310.11it/s, step size=7.65e-01, acc. prob=0.792]\rWarmup:  26%|██▌       | 770/3000 [00:03, 318.13it/s, step size=1.32e+00, acc. prob=0.793]\rWarmup:  27%|██▋       | 802/3000 [00:04, 314.52it/s, step size=1.63e+00, acc. prob=0.794]\rWarmup:  28%|██▊       | 837/3000 [00:04, 322.00it/s, step size=7.65e-01, acc. prob=0.793]\rWarmup:  29%|██▉       | 870/3000 [00:04, 319.17it/s, step size=1.40e+00, acc. prob=0.794]\rWarmup:  30%|███       | 909/3000 [00:04, 338.18it/s, step size=8.79e-01, acc. prob=0.793]\rWarmup:  31%|███▏      | 944/3000 [00:04, 339.72it/s, step size=9.67e-01, acc. prob=0.793]\rWarmup:  33%|███▎      | 979/3000 [00:04, 251.98it/s, step size=7.08e-01, acc. prob=0.793]\rSample:  34%|███▎      | 1010/3000 [00:04, 264.72it/s, step size=6.64e-01, acc. prob=0.906]\rSample:  35%|███▍      | 1040/3000 [00:04, 265.97it/s, step size=6.64e-01, acc. prob=0.912]\rSample:  36%|███▌      | 1069/3000 [00:05, 259.26it/s, step size=6.64e-01, acc. prob=0.913]\rSample:  37%|███▋      | 1097/3000 [00:05, 259.64it/s, step size=6.64e-01, acc. prob=0.911]\rSample:  37%|███▋      | 1124/3000 [00:05, 260.51it/s, step size=6.64e-01, acc. prob=0.917]\rSample:  38%|███▊      | 1151/3000 [00:05, 260.15it/s, step size=6.64e-01, acc. prob=0.923]\rSample:  39%|███▉      | 1178/3000 [00:05, 258.42it/s, step size=6.64e-01, acc. prob=0.921]\rSample:  40%|████      | 1205/3000 [00:05, 258.26it/s, step size=6.64e-01, acc. prob=0.922]\rSample:  41%|████      | 1233/3000 [00:05, 261.88it/s, step size=6.64e-01, acc. prob=0.923]\rSample:  42%|████▏     | 1262/3000 [00:05, 269.77it/s, step size=6.64e-01, acc. prob=0.923]\rSample:  43%|████▎     | 1290/3000 [00:05, 263.54it/s, step size=6.64e-01, acc. prob=0.922]\rSample:  44%|████▍     | 1321/3000 [00:05, 276.28it/s, step size=6.64e-01, acc. prob=0.922]\rSample:  45%|████▍     | 1349/3000 [00:06, 270.18it/s, step size=6.64e-01, acc. prob=0.924]\rSample:  46%|████▌     | 1377/3000 [00:06, 259.56it/s, step size=6.64e-01, acc. prob=0.924]\rSample:  47%|████▋     | 1404/3000 [00:06, 255.77it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  48%|████▊     | 1430/3000 [00:06, 246.98it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  49%|████▊     | 1458/3000 [00:06, 253.18it/s, step size=6.64e-01, acc. prob=0.928]\rSample:  50%|████▉     | 1488/3000 [00:06, 266.11it/s, step size=6.64e-01, acc. prob=0.929]\rSample:  50%|█████     | 1515/3000 [00:06, 263.65it/s, step size=6.64e-01, acc. prob=0.928]\rSample:  51%|█████▏    | 1542/3000 [00:06, 259.39it/s, step size=6.64e-01, acc. prob=0.928]\rSample:  52%|█████▏    | 1570/3000 [00:06, 262.00it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  53%|█████▎    | 1601/3000 [00:07, 274.94it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  54%|█████▍    | 1630/3000 [00:07, 277.46it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  55%|█████▌    | 1658/3000 [00:07, 277.97it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  56%|█████▌    | 1686/3000 [00:07, 275.99it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  57%|█████▋    | 1714/3000 [00:07, 276.40it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  58%|█████▊    | 1743/3000 [00:07, 277.28it/s, step size=6.64e-01, acc. prob=0.928]\rSample:  59%|█████▉    | 1771/3000 [00:07, 272.12it/s, step size=6.64e-01, acc. prob=0.928]\rSample:  60%|█████▉    | 1799/3000 [00:07, 268.09it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  61%|██████    | 1826/3000 [00:07, 264.21it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  62%|██████▏   | 1859/3000 [00:07, 281.96it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  63%|██████▎   | 1888/3000 [00:08, 264.00it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  64%|██████▍   | 1917/3000 [00:08, 270.04it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  65%|██████▍   | 1945/3000 [00:08, 271.24it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  66%|██████▌   | 1976/3000 [00:08, 282.28it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  67%|██████▋   | 2010/3000 [00:08, 295.49it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  68%|██████▊   | 2040/3000 [00:08, 286.57it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  69%|██████▉   | 2069/3000 [00:08, 278.32it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  70%|██████▉   | 2098/3000 [00:08, 279.46it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  71%|███████   | 2127/3000 [00:08, 274.03it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  72%|███████▏  | 2155/3000 [00:09, 259.21it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  73%|███████▎  | 2182/3000 [00:09, 258.35it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  74%|███████▎  | 2211/3000 [00:09, 263.85it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  75%|███████▍  | 2241/3000 [00:09, 273.40it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  76%|███████▌  | 2269/3000 [00:09, 269.10it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  77%|███████▋  | 2297/3000 [00:09, 266.24it/s, step size=6.64e-01, acc. prob=0.925]\rSample:  78%|███████▊  | 2325/3000 [00:09, 269.36it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  78%|███████▊  | 2352/3000 [00:09, 265.86it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  79%|███████▉  | 2379/3000 [00:09, 259.44it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  80%|████████  | 2406/3000 [00:10, 262.10it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  81%|████████  | 2437/3000 [00:10, 274.29it/s, step size=6.64e-01, acc. prob=0.927]\rSample:  82%|████████▏ | 2465/3000 [00:10, 271.54it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  83%|████████▎ | 2494/3000 [00:10, 274.19it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  84%|████████▍ | 2523/3000 [00:10, 275.40it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  85%|████████▌ | 2551/3000 [00:10, 270.30it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  86%|████████▌ | 2579/3000 [00:10, 271.55it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  87%|████████▋ | 2607/3000 [00:10, 266.45it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  88%|████████▊ | 2637/3000 [00:10, 274.31it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  89%|████████▉ | 2669/3000 [00:10, 283.71it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  90%|████████▉ | 2698/3000 [00:11, 278.18it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  91%|█████████ | 2726/3000 [00:11, 272.50it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  92%|█████████▏| 2757/3000 [00:11, 281.77it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  93%|█████████▎| 2786/3000 [00:11, 283.95it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  94%|█████████▍| 2815/3000 [00:11, 276.43it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  95%|█████████▍| 2843/3000 [00:11, 275.92it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  96%|█████████▌| 2871/3000 [00:11, 266.33it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  97%|█████████▋| 2900/3000 [00:11, 271.66it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  98%|█████████▊| 2928/3000 [00:11, 269.59it/s, step size=6.64e-01, acc. prob=0.926]\rSample:  99%|█████████▊| 2956/3000 [00:12, 265.85it/s, step size=6.64e-01, acc. prob=0.926]\rSample: 100%|█████████▉| 2986/3000 [00:12, 275.33it/s, step size=6.64e-01, acc. prob=0.926]\rSample: 100%|██████████| 3000/3000 [00:12, 246.59it/s, step size=6.64e-01, acc. prob=0.926]\n\rWarmup:   0%|          | 0/3000 [00:00, ?it/s]\rWarmup:   0%|          | 5/3000 [00:00, 45.90it/s, step size=3.73e-02, acc. prob=0.518]\rWarmup:   0%|          | 10/3000 [00:00, 34.67it/s, step size=1.10e-01, acc. prob=0.701]\rWarmup:   1%|          | 16/3000 [00:00, 41.09it/s, step size=8.11e-02, acc. prob=0.730]\rWarmup:   1%|          | 21/3000 [00:00, 38.42it/s, step size=1.78e-01, acc. prob=0.757]\rWarmup:   1%|          | 27/3000 [00:00, 43.73it/s, step size=9.71e-02, acc. prob=0.757]\rWarmup:   1%|          | 35/3000 [00:00, 54.03it/s, step size=7.69e-02, acc. prob=0.762]\rWarmup:   1%|▏         | 41/3000 [00:00, 52.61it/s, step size=7.77e-02, acc. prob=0.766]\rWarmup:   2%|▏         | 47/3000 [00:00, 53.50it/s, step size=1.11e-01, acc. prob=0.772]\rWarmup:   2%|▏         | 55/3000 [00:01, 59.90it/s, step size=4.55e-02, acc. prob=0.768]\rWarmup:   2%|▏         | 62/3000 [00:01, 49.88it/s, step size=1.68e-01, acc. prob=0.780]\rWarmup:   2%|▏         | 68/3000 [00:01, 42.05it/s, step size=1.30e-01, acc. prob=0.779]\rWarmup:   2%|▏         | 73/3000 [00:01, 43.23it/s, step size=9.33e-02, acc. prob=0.778]\rWarmup:   3%|▎         | 78/3000 [00:01, 40.81it/s, step size=6.37e-02, acc. prob=0.777]\rWarmup:   3%|▎         | 84/3000 [00:01, 44.64it/s, step size=9.60e-02, acc. prob=0.780]\rWarmup:   3%|▎         | 89/3000 [00:02, 38.52it/s, step size=1.13e-01, acc. prob=0.782]\rWarmup:   3%|▎         | 94/3000 [00:02, 40.62it/s, step size=1.06e-01, acc. prob=0.782]\rWarmup:   3%|▎         | 99/3000 [00:02, 40.39it/s, step size=7.87e-01, acc. prob=0.778]\rWarmup:   3%|▎         | 104/3000 [00:02, 42.21it/s, step size=9.37e-02, acc. prob=0.765]\rWarmup:   4%|▎         | 109/3000 [00:02, 41.03it/s, step size=3.03e-01, acc. prob=0.771]\rWarmup:   4%|▍         | 115/3000 [00:02, 45.29it/s, step size=4.31e-01, acc. prob=0.773]\rWarmup:   4%|▍         | 120/3000 [00:02, 39.66it/s, step size=1.64e-01, acc. prob=0.771]\rWarmup:   4%|▍         | 127/3000 [00:02, 46.01it/s, step size=1.48e-01, acc. prob=0.772]\rWarmup:   4%|▍         | 135/3000 [00:02, 53.59it/s, step size=2.66e-01, acc. prob=0.774]\rWarmup:   5%|▍         | 141/3000 [00:03, 50.92it/s, step size=1.58e-01, acc. prob=0.774]\rWarmup:   5%|▍         | 147/3000 [00:03, 47.72it/s, step size=3.21e-01, acc. prob=0.776]\rWarmup:   5%|▌         | 153/3000 [00:03, 49.92it/s, step size=6.10e-02, acc. prob=0.770]\rWarmup:   5%|▌         | 159/3000 [00:03, 41.08it/s, step size=2.33e-01, acc. prob=0.774]\rWarmup:   6%|▌         | 168/3000 [00:03, 51.41it/s, step size=2.28e-01, acc. prob=0.775]\rWarmup:   6%|▌         | 175/3000 [00:03, 53.36it/s, step size=1.61e-01, acc. prob=0.775]\rWarmup:   6%|▌         | 181/3000 [00:03, 54.90it/s, step size=1.17e-01, acc. prob=0.775]\rWarmup:   6%|▋         | 188/3000 [00:03, 58.79it/s, step size=2.19e-01, acc. prob=0.777]\rWarmup:   6%|▋         | 195/3000 [00:04, 53.74it/s, step size=1.20e-01, acc. prob=0.777]\rWarmup:   7%|▋         | 201/3000 [00:04, 52.38it/s, step size=2.14e-01, acc. prob=0.778]\rWarmup:   7%|▋         | 207/3000 [00:04, 53.01it/s, step size=2.25e-01, acc. prob=0.779]\rWarmup:   7%|▋         | 213/3000 [00:04, 51.34it/s, step size=1.80e-01, acc. prob=0.779]\rWarmup:   7%|▋         | 220/3000 [00:04, 53.54it/s, step size=7.44e-02, acc. prob=0.777]\rWarmup:   8%|▊         | 226/3000 [00:04, 46.79it/s, step size=1.40e-01, acc. prob=0.779]\rWarmup:   8%|▊         | 234/3000 [00:04, 54.70it/s, step size=3.33e-01, acc. prob=0.781]\rWarmup:   8%|▊         | 241/3000 [00:04, 57.37it/s, step size=1.92e-01, acc. prob=0.781]\rWarmup:   8%|▊         | 252/3000 [00:05, 70.66it/s, step size=1.40e-01, acc. prob=0.776]\rWarmup:   9%|▊         | 260/3000 [00:05, 69.62it/s, step size=4.11e-01, acc. prob=0.779]\rWarmup:   9%|▉         | 268/3000 [00:05, 65.85it/s, step size=8.07e-02, acc. prob=0.778]\rWarmup:   9%|▉         | 275/3000 [00:05, 66.37it/s, step size=6.66e-02, acc. prob=0.778]\rWarmup:   9%|▉         | 282/3000 [00:05, 58.81it/s, step size=1.60e-01, acc. prob=0.779]\rWarmup:  10%|▉         | 290/3000 [00:05, 64.00it/s, step size=4.70e-01, acc. prob=0.781]\rWarmup:  10%|▉         | 297/3000 [00:05, 55.57it/s, step size=8.51e-02, acc. prob=0.779]\rWarmup:  10%|█         | 303/3000 [00:05, 53.94it/s, step size=1.77e-01, acc. prob=0.780]\rWarmup:  10%|█         | 309/3000 [00:06, 48.94it/s, step size=1.41e-01, acc. prob=0.780]\rWarmup:  10%|█         | 315/3000 [00:06, 49.80it/s, step size=2.46e-01, acc. prob=0.781]\rWarmup:  11%|█         | 322/3000 [00:06, 52.16it/s, step size=1.49e-01, acc. prob=0.780]\rWarmup:  11%|█         | 328/3000 [00:06, 49.99it/s, step size=1.49e-01, acc. prob=0.781]\rWarmup:  11%|█         | 334/3000 [00:06, 51.49it/s, step size=1.75e-01, acc. prob=0.781]\rWarmup:  11%|█▏        | 340/3000 [00:06, 51.09it/s, step size=1.51e-01, acc. prob=0.781]\rWarmup:  12%|█▏        | 348/3000 [00:06, 58.58it/s, step size=1.89e-01, acc. prob=0.782]\rWarmup:  12%|█▏        | 356/3000 [00:06, 63.36it/s, step size=9.28e-02, acc. prob=0.781]\rWarmup:  12%|█▏        | 363/3000 [00:07, 59.10it/s, step size=1.93e-01, acc. prob=0.782]\rWarmup:  12%|█▏        | 373/3000 [00:07, 66.71it/s, step size=1.60e-01, acc. prob=0.782]\rWarmup:  13%|█▎        | 380/3000 [00:07, 62.41it/s, step size=1.63e-01, acc. prob=0.782]\rWarmup:  13%|█▎        | 388/3000 [00:07, 66.94it/s, step size=5.37e-01, acc. prob=0.785]\rWarmup:  13%|█▎        | 395/3000 [00:07, 65.43it/s, step size=3.34e-01, acc. prob=0.784]\rWarmup:  13%|█▎        | 402/3000 [00:07, 65.46it/s, step size=1.93e-01, acc. prob=0.783]\rWarmup:  14%|█▎        | 409/3000 [00:07, 63.39it/s, step size=1.79e-01, acc. prob=0.783]\rWarmup:  14%|█▍        | 418/3000 [00:07, 69.33it/s, step size=1.78e-01, acc. prob=0.784]\rWarmup:  14%|█▍        | 428/3000 [00:07, 76.01it/s, step size=1.43e-01, acc. prob=0.784]\rWarmup:  15%|█▍        | 436/3000 [00:08, 75.30it/s, step size=2.64e-01, acc. prob=0.785]\rWarmup:  15%|█▍        | 445/3000 [00:08, 78.95it/s, step size=3.00e-01, acc. prob=0.785]\rWarmup:  15%|█▌        | 453/3000 [00:08, 70.88it/s, step size=2.72e-01, acc. prob=0.783]\rWarmup:  15%|█▌        | 461/3000 [00:08, 62.58it/s, step size=2.05e-01, acc. prob=0.784]\rWarmup:  16%|█▌        | 470/3000 [00:08, 68.48it/s, step size=7.23e-02, acc. prob=0.783]\rWarmup:  16%|█▌        | 478/3000 [00:08, 59.39it/s, step size=1.69e-01, acc. prob=0.784]\rWarmup:  16%|█▌        | 485/3000 [00:08, 61.77it/s, step size=1.35e-01, acc. prob=0.784]\rWarmup:  16%|█▋        | 492/3000 [00:09, 62.27it/s, step size=7.61e-02, acc. prob=0.784]\rWarmup:  17%|█▋        | 499/3000 [00:09, 61.23it/s, step size=4.53e-01, acc. prob=0.785]\rWarmup:  17%|█▋        | 506/3000 [00:09, 61.11it/s, step size=3.11e-01, acc. prob=0.785]\rWarmup:  17%|█▋        | 513/3000 [00:09, 55.18it/s, step size=1.81e-01, acc. prob=0.785]\rWarmup:  17%|█▋        | 519/3000 [00:09, 52.27it/s, step size=2.92e-01, acc. prob=0.785]\rWarmup:  18%|█▊        | 527/3000 [00:09, 58.80it/s, step size=2.04e-01, acc. prob=0.785]\rWarmup:  18%|█▊        | 534/3000 [00:09, 56.82it/s, step size=1.53e-01, acc. prob=0.785]\rWarmup:  18%|█▊        | 543/3000 [00:09, 64.12it/s, step size=1.35e-01, acc. prob=0.785]\rWarmup:  18%|█▊        | 552/3000 [00:09, 66.96it/s, step size=9.26e-02, acc. prob=0.785]\rWarmup:  19%|█▊        | 559/3000 [00:10, 66.03it/s, step size=2.09e-01, acc. prob=0.786]\rWarmup:  19%|█▉        | 566/3000 [00:10, 65.71it/s, step size=3.12e-01, acc. prob=0.786]\rWarmup:  19%|█▉        | 575/3000 [00:10, 68.77it/s, step size=1.21e-01, acc. prob=0.786]\rWarmup:  19%|█▉        | 582/3000 [00:10, 63.78it/s, step size=1.47e-01, acc. prob=0.786]\rWarmup:  20%|█▉        | 589/3000 [00:10, 50.14it/s, step size=2.49e-01, acc. prob=0.787]\rWarmup:  20%|█▉        | 598/3000 [00:10, 56.80it/s, step size=1.84e-01, acc. prob=0.787]\rWarmup:  20%|██        | 605/3000 [00:10, 53.72it/s, step size=1.26e-01, acc. prob=0.786]\rWarmup:  20%|██        | 612/3000 [00:11, 56.60it/s, step size=1.56e-01, acc. prob=0.787]\rWarmup:  21%|██        | 618/3000 [00:11, 56.45it/s, step size=2.16e-01, acc. prob=0.787]\rWarmup:  21%|██        | 624/3000 [00:11, 56.86it/s, step size=1.91e-01, acc. prob=0.787]\rWarmup:  21%|██        | 631/3000 [00:11, 59.09it/s, step size=2.11e-01, acc. prob=0.787]\rWarmup:  21%|██▏       | 638/3000 [00:11, 60.36it/s, step size=2.23e-01, acc. prob=0.787]\rWarmup:  22%|██▏       | 647/3000 [00:11, 66.19it/s, step size=2.31e-01, acc. prob=0.788]\rWarmup:  22%|██▏       | 655/3000 [00:11, 67.88it/s, step size=2.00e-01, acc. prob=0.788]\rWarmup:  22%|██▏       | 662/3000 [00:11, 67.90it/s, step size=3.23e-01, acc. prob=0.788]\rWarmup:  22%|██▏       | 669/3000 [00:11, 65.10it/s, step size=2.28e-01, acc. prob=0.788]\rWarmup:  23%|██▎       | 676/3000 [00:12, 58.13it/s, step size=2.10e-01, acc. prob=0.788]\rWarmup:  23%|██▎       | 682/3000 [00:12, 55.68it/s, step size=1.96e-01, acc. prob=0.788]\rWarmup:  23%|██▎       | 688/3000 [00:12, 48.51it/s, step size=1.64e-01, acc. prob=0.788]\rWarmup:  23%|██▎       | 696/3000 [00:12, 54.84it/s, step size=1.55e-01, acc. prob=0.788]\rWarmup:  23%|██▎       | 703/3000 [00:12, 57.63it/s, step size=2.55e-01, acc. prob=0.789]\rWarmup:  24%|██▎       | 711/3000 [00:12, 63.08it/s, step size=3.32e-01, acc. prob=0.789]\rWarmup:  24%|██▍       | 718/3000 [00:12, 64.35it/s, step size=1.83e-01, acc. prob=0.788]\rWarmup:  24%|██▍       | 725/3000 [00:12, 58.97it/s, step size=1.89e-01, acc. prob=0.789]\rWarmup:  24%|██▍       | 735/3000 [00:13, 68.77it/s, step size=2.91e-01, acc. prob=0.789]\rWarmup:  25%|██▍       | 743/3000 [00:13, 70.71it/s, step size=1.82e-01, acc. prob=0.789]\rWarmup:  25%|██▌       | 751/3000 [00:13, 61.92it/s, step size=1.08e-01, acc. prob=0.788]\rWarmup:  25%|██▌       | 758/3000 [00:13, 53.09it/s, step size=1.91e-01, acc. prob=0.789]\rWarmup:  25%|██▌       | 764/3000 [00:13, 49.73it/s, step size=1.23e-01, acc. prob=0.788]\rWarmup:  26%|██▌       | 770/3000 [00:13, 49.73it/s, step size=1.31e-01, acc. prob=0.789]\rWarmup:  26%|██▌       | 777/3000 [00:13, 53.30it/s, step size=2.56e-01, acc. prob=0.790]\rWarmup:  26%|██▌       | 784/3000 [00:13, 55.89it/s, step size=2.49e-01, acc. prob=0.790]\rWarmup:  26%|██▋       | 792/3000 [00:14, 61.28it/s, step size=1.93e-01, acc. prob=0.789]\rWarmup:  27%|██▋       | 800/3000 [00:14, 62.90it/s, step size=1.27e-01, acc. prob=0.789]\rWarmup:  27%|██▋       | 807/3000 [00:14, 48.16it/s, step size=1.43e-01, acc. prob=0.789]\rWarmup:  27%|██▋       | 813/3000 [00:14, 47.34it/s, step size=2.21e-01, acc. prob=0.790]\rWarmup:  27%|██▋       | 822/3000 [00:14, 54.49it/s, step size=2.51e-01, acc. prob=0.790]\rWarmup:  28%|██▊       | 828/3000 [00:14, 47.33it/s, step size=1.89e-01, acc. prob=0.790]\rWarmup:  28%|██▊       | 835/3000 [00:14, 51.76it/s, step size=2.53e-01, acc. prob=0.790]\rWarmup:  28%|██▊       | 842/3000 [00:15, 55.46it/s, step size=1.86e-01, acc. prob=0.790]\rWarmup:  28%|██▊       | 849/3000 [00:15, 58.58it/s, step size=2.76e-01, acc. prob=0.790]\rWarmup:  29%|██▊       | 856/3000 [00:15, 59.52it/s, step size=1.69e-01, acc. prob=0.790]\rWarmup:  29%|██▉       | 863/3000 [00:15, 55.72it/s, step size=1.36e-01, acc. prob=0.790]\rWarmup:  29%|██▉       | 870/3000 [00:15, 57.72it/s, step size=1.59e-01, acc. prob=0.790]\rWarmup:  29%|██▉       | 876/3000 [00:15, 50.89it/s, step size=1.65e-01, acc. prob=0.790]\rWarmup:  29%|██▉       | 882/3000 [00:15, 51.35it/s, step size=1.77e-01, acc. prob=0.790]\rWarmup:  30%|██▉       | 889/3000 [00:15, 55.01it/s, step size=1.71e-01, acc. prob=0.790]\rWarmup:  30%|██▉       | 895/3000 [00:16, 51.94it/s, step size=2.45e-01, acc. prob=0.791]\rWarmup:  30%|███       | 902/3000 [00:16, 53.99it/s, step size=1.36e-01, acc. prob=0.790]\rWarmup:  30%|███       | 908/3000 [00:16, 51.26it/s, step size=1.73e-01, acc. prob=0.790]\rWarmup:  30%|███       | 914/3000 [00:16, 52.52it/s, step size=1.69e-01, acc. prob=0.790]\rWarmup:  31%|███       | 920/3000 [00:16, 48.67it/s, step size=1.88e-01, acc. prob=0.791]\rWarmup:  31%|███       | 928/3000 [00:16, 54.47it/s, step size=1.80e-01, acc. prob=0.791]\rWarmup:  31%|███       | 934/3000 [00:16, 50.55it/s, step size=1.75e-01, acc. prob=0.791]\rWarmup:  31%|███▏      | 941/3000 [00:16, 54.59it/s, step size=1.15e-01, acc. prob=0.790]\rWarmup:  32%|███▏      | 947/3000 [00:17, 52.46it/s, step size=1.63e-01, acc. prob=0.791]\rWarmup:  32%|███▏      | 956/3000 [00:17, 58.97it/s, step size=2.93e-01, acc. prob=0.790]\rWarmup:  32%|███▏      | 963/3000 [00:17, 59.00it/s, step size=2.02e-01, acc. prob=0.790]\rWarmup:  32%|███▏      | 969/3000 [00:17, 55.44it/s, step size=1.22e-01, acc. prob=0.790]\rWarmup:  32%|███▎      | 975/3000 [00:17, 50.97it/s, step size=2.85e-01, acc. prob=0.790]\rWarmup:  33%|███▎      | 982/3000 [00:17, 53.88it/s, step size=2.34e-01, acc. prob=0.790]\rWarmup:  33%|███▎      | 990/3000 [00:17, 56.46it/s, step size=1.77e-01, acc. prob=0.790]\rWarmup:  33%|███▎      | 996/3000 [00:17, 54.82it/s, step size=2.25e-01, acc. prob=0.790]\rSample:  33%|███▎      | 1002/3000 [00:18, 54.47it/s, step size=2.22e-01, acc. prob=0.788]\rSample:  34%|███▎      | 1008/3000 [00:18, 54.66it/s, step size=2.22e-01, acc. prob=0.835]\rSample:  34%|███▍      | 1014/3000 [00:18, 48.40it/s, step size=2.22e-01, acc. prob=0.838]\rSample:  34%|███▍      | 1021/3000 [00:18, 52.91it/s, step size=2.22e-01, acc. prob=0.844]\rSample:  34%|███▍      | 1028/3000 [00:18, 55.76it/s, step size=2.22e-01, acc. prob=0.840]\rSample:  35%|███▍      | 1036/3000 [00:18, 61.69it/s, step size=2.22e-01, acc. prob=0.815]\rSample:  35%|███▍      | 1044/3000 [00:18, 66.23it/s, step size=2.22e-01, acc. prob=0.802]\rSample:  35%|███▌      | 1051/3000 [00:18, 66.70it/s, step size=2.22e-01, acc. prob=0.798]\rSample:  35%|███▌      | 1060/3000 [00:18, 71.67it/s, step size=2.22e-01, acc. prob=0.783]\rSample:  36%|███▌      | 1068/3000 [00:19, 73.49it/s, step size=2.22e-01, acc. prob=0.778]\rSample:  36%|███▌      | 1076/3000 [00:19, 74.89it/s, step size=2.22e-01, acc. prob=0.774]\rSample:  36%|███▌      | 1084/3000 [00:19, 76.20it/s, step size=2.22e-01, acc. prob=0.775]\rSample:  36%|███▋      | 1092/3000 [00:19, 68.51it/s, step size=2.22e-01, acc. prob=0.762]\rSample:  37%|███▋      | 1100/3000 [00:19, 66.97it/s, step size=2.22e-01, acc. prob=0.771]\rSample:  37%|███▋      | 1107/3000 [00:19, 67.46it/s, step size=2.22e-01, acc. prob=0.774]\rSample:  37%|███▋      | 1115/3000 [00:19, 69.50it/s, step size=2.22e-01, acc. prob=0.775]\rSample:  37%|███▋      | 1123/3000 [00:19, 70.22it/s, step size=2.22e-01, acc. prob=0.777]\rSample:  38%|███▊      | 1132/3000 [00:19, 73.15it/s, step size=2.22e-01, acc. prob=0.773]\rSample:  38%|███▊      | 1140/3000 [00:20, 71.69it/s, step size=2.22e-01, acc. prob=0.775]\rSample:  38%|███▊      | 1148/3000 [00:20, 67.00it/s, step size=2.22e-01, acc. prob=0.768]\rSample:  38%|███▊      | 1155/3000 [00:20, 66.75it/s, step size=2.22e-01, acc. prob=0.770]\rSample:  39%|███▉      | 1165/3000 [00:20, 75.22it/s, step size=2.22e-01, acc. prob=0.771]\rSample:  39%|███▉      | 1174/3000 [00:20, 76.77it/s, step size=2.22e-01, acc. prob=0.779]\rSample:  39%|███▉      | 1182/3000 [00:20, 75.32it/s, step size=2.22e-01, acc. prob=0.777]\rSample:  40%|███▉      | 1190/3000 [00:20, 69.89it/s, step size=2.22e-01, acc. prob=0.776]\rSample:  40%|███▉      | 1198/3000 [00:20, 68.71it/s, step size=2.22e-01, acc. prob=0.777]\rSample:  40%|████      | 1206/3000 [00:20, 70.59it/s, step size=2.22e-01, acc. prob=0.777]\rSample:  40%|████      | 1215/3000 [00:21, 72.04it/s, step size=2.22e-01, acc. prob=0.774]\rSample:  41%|████      | 1223/3000 [00:21, 72.88it/s, step size=2.22e-01, acc. prob=0.773]\rSample:  41%|████      | 1232/3000 [00:21, 76.06it/s, step size=2.22e-01, acc. prob=0.774]\rSample:  41%|████▏     | 1240/3000 [00:21, 76.29it/s, step size=2.22e-01, acc. prob=0.776]\rSample:  42%|████▏     | 1249/3000 [00:21, 78.94it/s, step size=2.22e-01, acc. prob=0.778]\rSample:  42%|████▏     | 1260/3000 [00:21, 85.04it/s, step size=2.22e-01, acc. prob=0.777]\rSample:  42%|████▏     | 1269/3000 [00:21, 79.58it/s, step size=2.22e-01, acc. prob=0.777]\rSample:  43%|████▎     | 1278/3000 [00:21, 79.35it/s, step size=2.22e-01, acc. prob=0.777]\rSample:  43%|████▎     | 1286/3000 [00:21, 77.77it/s, step size=2.22e-01, acc. prob=0.778]\rSample:  43%|████▎     | 1296/3000 [00:22, 81.90it/s, step size=2.22e-01, acc. prob=0.780]\rSample:  44%|████▎     | 1305/3000 [00:22, 78.53it/s, step size=2.22e-01, acc. prob=0.780]\rSample:  44%|████▍     | 1313/3000 [00:22, 77.41it/s, step size=2.22e-01, acc. prob=0.781]\rSample:  44%|████▍     | 1321/3000 [00:22, 73.82it/s, step size=2.22e-01, acc. prob=0.782]\rSample:  44%|████▍     | 1329/3000 [00:22, 71.17it/s, step size=2.22e-01, acc. prob=0.782]\rSample:  45%|████▍     | 1339/3000 [00:22, 76.56it/s, step size=2.22e-01, acc. prob=0.783]\rSample:  45%|████▍     | 1347/3000 [00:22, 72.65it/s, step size=2.22e-01, acc. prob=0.779]\rSample:  45%|████▌     | 1357/3000 [00:22, 77.56it/s, step size=2.22e-01, acc. prob=0.781]\rSample:  46%|████▌     | 1365/3000 [00:23, 76.68it/s, step size=2.22e-01, acc. prob=0.778]\rSample:  46%|████▌     | 1373/3000 [00:23, 76.28it/s, step size=2.22e-01, acc. prob=0.778]\rSample:  46%|████▌     | 1382/3000 [00:23, 78.67it/s, step size=2.22e-01, acc. prob=0.778]\rSample:  46%|████▋     | 1390/3000 [00:23, 74.54it/s, step size=2.22e-01, acc. prob=0.778]\rSample:  47%|████▋     | 1399/3000 [00:23, 77.50it/s, step size=2.22e-01, acc. prob=0.777]\rSample:  47%|████▋     | 1407/3000 [00:23, 77.70it/s, step size=2.22e-01, acc. prob=0.774]\rSample:  47%|████▋     | 1416/3000 [00:23, 77.06it/s, step size=2.22e-01, acc. prob=0.773]\rSample:  47%|████▋     | 1424/3000 [00:23, 74.87it/s, step size=2.22e-01, acc. prob=0.772]\rSample:  48%|████▊     | 1432/3000 [00:23, 70.03it/s, step size=2.22e-01, acc. prob=0.774]\rSample:  48%|████▊     | 1440/3000 [00:24, 67.39it/s, step size=2.22e-01, acc. prob=0.774]\rSample:  48%|████▊     | 1448/3000 [00:24, 69.56it/s, step size=2.22e-01, acc. prob=0.775]\rSample:  49%|████▊     | 1456/3000 [00:24, 71.54it/s, step size=2.22e-01, acc. prob=0.776]\rSample:  49%|████▉     | 1464/3000 [00:24, 70.67it/s, step size=2.22e-01, acc. prob=0.776]\rSample:  49%|████▉     | 1473/3000 [00:24, 75.48it/s, step size=2.22e-01, acc. prob=0.775]\rSample:  49%|████▉     | 1481/3000 [00:24, 73.36it/s, step size=2.22e-01, acc. prob=0.776]\rSample:  50%|████▉     | 1489/3000 [00:24, 71.99it/s, step size=2.22e-01, acc. prob=0.777]\rSample:  50%|████▉     | 1497/3000 [00:24, 68.91it/s, step size=2.22e-01, acc. prob=0.777]\rSample:  50%|█████     | 1504/3000 [00:24, 65.04it/s, step size=2.22e-01, acc. prob=0.775]\rSample:  50%|█████     | 1511/3000 [00:25, 54.97it/s, step size=2.22e-01, acc. prob=0.776]\rSample:  51%|█████     | 1517/3000 [00:25, 55.46it/s, step size=2.22e-01, acc. prob=0.777]\rSample:  51%|█████     | 1523/3000 [00:25, 53.96it/s, step size=2.22e-01, acc. prob=0.778]\rSample:  51%|█████     | 1531/3000 [00:25, 57.56it/s, step size=2.22e-01, acc. prob=0.778]\rSample:  51%|█████▏    | 1539/3000 [00:25, 61.67it/s, step size=2.22e-01, acc. prob=0.778]\rSample:  52%|█████▏    | 1546/3000 [00:25, 63.22it/s, step size=2.22e-01, acc. prob=0.778]\rSample:  52%|█████▏    | 1554/3000 [00:25, 67.28it/s, step size=2.22e-01, acc. prob=0.778]\rSample:  52%|█████▏    | 1562/3000 [00:25, 70.28it/s, step size=2.22e-01, acc. prob=0.780]\rSample:  52%|█████▏    | 1570/3000 [00:26, 72.40it/s, step size=2.22e-01, acc. prob=0.781]\rSample:  53%|█████▎    | 1578/3000 [00:26, 73.16it/s, step size=2.22e-01, acc. prob=0.781]\rSample:  53%|█████▎    | 1586/3000 [00:26, 73.68it/s, step size=2.22e-01, acc. prob=0.782]\rSample:  53%|█████▎    | 1594/3000 [00:26, 71.44it/s, step size=2.22e-01, acc. prob=0.782]\rSample:  53%|█████▎    | 1602/3000 [00:26, 72.58it/s, step size=2.22e-01, acc. prob=0.782]\rSample:  54%|█████▎    | 1610/3000 [00:26, 74.30it/s, step size=2.22e-01, acc. prob=0.782]\rSample:  54%|█████▍    | 1618/3000 [00:26, 71.11it/s, step size=2.22e-01, acc. prob=0.783]\rSample:  54%|█████▍    | 1626/3000 [00:26, 68.44it/s, step size=2.22e-01, acc. prob=0.784]\rSample:  54%|█████▍    | 1634/3000 [00:26, 69.04it/s, step size=2.22e-01, acc. prob=0.784]\rSample:  55%|█████▍    | 1641/3000 [00:27, 65.81it/s, step size=2.22e-01, acc. prob=0.784]\rSample:  55%|█████▍    | 1648/3000 [00:27, 63.28it/s, step size=2.22e-01, acc. prob=0.784]\rSample:  55%|█████▌    | 1655/3000 [00:27, 63.31it/s, step size=2.22e-01, acc. prob=0.784]\rSample:  55%|█████▌    | 1662/3000 [00:27, 58.44it/s, step size=2.22e-01, acc. prob=0.785]\rSample:  56%|█████▌    | 1668/3000 [00:27, 56.58it/s, step size=2.22e-01, acc. prob=0.786]\rSample:  56%|█████▌    | 1674/3000 [00:27, 51.65it/s, step size=2.22e-01, acc. prob=0.786]\rSample:  56%|█████▌    | 1680/3000 [00:27, 46.57it/s, step size=2.22e-01, acc. prob=0.785]\rSample:  56%|█████▌    | 1686/3000 [00:27, 49.64it/s, step size=2.22e-01, acc. prob=0.785]\rSample:  56%|█████▋    | 1693/3000 [00:28, 53.03it/s, step size=2.22e-01, acc. prob=0.785]\rSample:  57%|█████▋    | 1700/3000 [00:28, 56.75it/s, step size=2.22e-01, acc. prob=0.785]\rSample:  57%|█████▋    | 1706/3000 [00:28, 56.85it/s, step size=2.22e-01, acc. prob=0.785]\rSample:  57%|█████▋    | 1713/3000 [00:28, 57.08it/s, step size=2.22e-01, acc. prob=0.785]\rSample:  57%|█████▋    | 1722/3000 [00:28, 64.41it/s, step size=2.22e-01, acc. prob=0.786]\rSample:  58%|█████▊    | 1730/3000 [00:28, 68.19it/s, step size=2.22e-01, acc. prob=0.786]\rSample:  58%|█████▊    | 1738/3000 [00:28, 70.60it/s, step size=2.22e-01, acc. prob=0.787]\rSample:  58%|█████▊    | 1747/3000 [00:28, 73.87it/s, step size=2.22e-01, acc. prob=0.787]\rSample:  58%|█████▊    | 1755/3000 [00:28, 68.75it/s, step size=2.22e-01, acc. prob=0.786]\rSample:  59%|█████▉    | 1763/3000 [00:29, 69.69it/s, step size=2.22e-01, acc. prob=0.786]\rSample:  59%|█████▉    | 1771/3000 [00:29, 70.65it/s, step size=2.22e-01, acc. prob=0.786]\rSample:  59%|█████▉    | 1779/3000 [00:29, 66.12it/s, step size=2.22e-01, acc. prob=0.787]\rSample:  60%|█████▉    | 1787/3000 [00:29, 67.65it/s, step size=2.22e-01, acc. prob=0.788]\rSample:  60%|█████▉    | 1794/3000 [00:29, 65.60it/s, step size=2.22e-01, acc. prob=0.787]\rSample:  60%|██████    | 1801/3000 [00:29, 63.90it/s, step size=2.22e-01, acc. prob=0.787]\rSample:  60%|██████    | 1808/3000 [00:29, 64.17it/s, step size=2.22e-01, acc. prob=0.788]\rSample:  60%|██████    | 1815/3000 [00:29, 62.74it/s, step size=2.22e-01, acc. prob=0.788]\rSample:  61%|██████    | 1822/3000 [00:29, 62.06it/s, step size=2.22e-01, acc. prob=0.788]\rSample:  61%|██████    | 1829/3000 [00:30, 59.08it/s, step size=2.22e-01, acc. prob=0.788]\rSample:  61%|██████    | 1836/3000 [00:30, 59.91it/s, step size=2.22e-01, acc. prob=0.788]\rSample:  61%|██████▏   | 1843/3000 [00:30, 58.48it/s, step size=2.22e-01, acc. prob=0.789]\rSample:  62%|██████▏   | 1849/3000 [00:30, 57.22it/s, step size=2.22e-01, acc. prob=0.789]\rSample:  62%|██████▏   | 1855/3000 [00:30, 43.37it/s, step size=2.22e-01, acc. prob=0.789]\rSample:  62%|██████▏   | 1861/3000 [00:30, 46.59it/s, step size=2.22e-01, acc. prob=0.790]\rSample:  62%|██████▏   | 1868/3000 [00:30, 50.95it/s, step size=2.22e-01, acc. prob=0.790]\rSample:  62%|██████▏   | 1874/3000 [00:31, 48.40it/s, step size=2.22e-01, acc. prob=0.790]\rSample:  63%|██████▎   | 1880/3000 [00:31, 50.78it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  63%|██████▎   | 1886/3000 [00:31, 52.90it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  63%|██████▎   | 1893/3000 [00:31, 54.51it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  63%|██████▎   | 1899/3000 [00:31, 50.94it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  64%|██████▎   | 1905/3000 [00:31, 50.50it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  64%|██████▎   | 1911/3000 [00:31, 46.18it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  64%|██████▍   | 1917/3000 [00:31, 49.02it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  64%|██████▍   | 1923/3000 [00:32, 41.96it/s, step size=2.22e-01, acc. prob=0.794]\rSample:  64%|██████▍   | 1928/3000 [00:32, 40.50it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  64%|██████▍   | 1933/3000 [00:32, 39.39it/s, step size=2.22e-01, acc. prob=0.794]\rSample:  65%|██████▍   | 1939/3000 [00:32, 40.98it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  65%|██████▍   | 1946/3000 [00:32, 46.88it/s, step size=2.22e-01, acc. prob=0.794]\rSample:  65%|██████▌   | 1951/3000 [00:32, 43.44it/s, step size=2.22e-01, acc. prob=0.794]\rSample:  65%|██████▌   | 1957/3000 [00:32, 45.65it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  65%|██████▌   | 1962/3000 [00:32, 46.27it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  66%|██████▌   | 1969/3000 [00:33, 51.31it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  66%|██████▌   | 1975/3000 [00:33, 53.27it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  66%|██████▌   | 1982/3000 [00:33, 57.04it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  66%|██████▋   | 1990/3000 [00:33, 61.82it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  67%|██████▋   | 1998/3000 [00:33, 66.25it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  67%|██████▋   | 2005/3000 [00:33, 66.76it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  67%|██████▋   | 2012/3000 [00:33, 62.49it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  67%|██████▋   | 2020/3000 [00:33, 63.99it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  68%|██████▊   | 2028/3000 [00:33, 67.65it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  68%|██████▊   | 2036/3000 [00:34, 69.78it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  68%|██████▊   | 2044/3000 [00:34, 69.35it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  68%|██████▊   | 2051/3000 [00:34, 69.51it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  69%|██████▊   | 2059/3000 [00:34, 71.48it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  69%|██████▉   | 2067/3000 [00:34, 69.47it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  69%|██████▉   | 2074/3000 [00:34, 67.40it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  69%|██████▉   | 2083/3000 [00:34, 71.80it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  70%|██████▉   | 2091/3000 [00:34, 70.29it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  70%|██████▉   | 2099/3000 [00:34, 71.64it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  70%|███████   | 2107/3000 [00:35, 72.93it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  70%|███████   | 2115/3000 [00:35, 69.13it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  71%|███████   | 2122/3000 [00:35, 68.70it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  71%|███████   | 2131/3000 [00:35, 72.89it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  71%|███████▏  | 2139/3000 [00:35, 74.38it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  72%|███████▏  | 2148/3000 [00:35, 76.49it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  72%|███████▏  | 2156/3000 [00:35, 73.65it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  72%|███████▏  | 2164/3000 [00:35, 70.34it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  72%|███████▏  | 2172/3000 [00:35, 70.28it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  73%|███████▎  | 2180/3000 [00:36, 71.37it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  73%|███████▎  | 2188/3000 [00:36, 70.63it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  73%|███████▎  | 2197/3000 [00:36, 73.71it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  74%|███████▎  | 2205/3000 [00:36, 74.78it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  74%|███████▍  | 2213/3000 [00:36, 69.36it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  74%|███████▍  | 2221/3000 [00:36, 71.35it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  74%|███████▍  | 2229/3000 [00:36, 72.41it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  75%|███████▍  | 2237/3000 [00:36, 68.26it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  75%|███████▍  | 2244/3000 [00:36, 67.89it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  75%|███████▌  | 2251/3000 [00:37, 66.46it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  75%|███████▌  | 2258/3000 [00:37, 66.09it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  76%|███████▌  | 2266/3000 [00:37, 69.00it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  76%|███████▌  | 2273/3000 [00:37, 68.70it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  76%|███████▌  | 2280/3000 [00:37, 67.12it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  76%|███████▋  | 2288/3000 [00:37, 68.76it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  76%|███████▋  | 2295/3000 [00:37, 68.69it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  77%|███████▋  | 2302/3000 [00:37, 68.10it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  77%|███████▋  | 2309/3000 [00:37, 68.59it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  77%|███████▋  | 2316/3000 [00:38, 63.21it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  77%|███████▋  | 2323/3000 [00:38, 63.36it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  78%|███████▊  | 2330/3000 [00:38, 61.25it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  78%|███████▊  | 2337/3000 [00:38, 62.15it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  78%|███████▊  | 2344/3000 [00:38, 59.60it/s, step size=2.22e-01, acc. prob=0.794]\rSample:  78%|███████▊  | 2351/3000 [00:38, 60.60it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  79%|███████▊  | 2358/3000 [00:38, 61.78it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  79%|███████▉  | 2366/3000 [00:38, 63.99it/s, step size=2.22e-01, acc. prob=0.794]\rSample:  79%|███████▉  | 2373/3000 [00:39, 62.83it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  79%|███████▉  | 2380/3000 [00:39, 57.59it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  80%|███████▉  | 2387/3000 [00:39, 58.76it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  80%|███████▉  | 2394/3000 [00:39, 61.01it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  80%|████████  | 2402/3000 [00:39, 64.75it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  80%|████████  | 2409/3000 [00:39, 63.59it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  81%|████████  | 2417/3000 [00:39, 67.19it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  81%|████████  | 2425/3000 [00:39, 69.10it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  81%|████████  | 2434/3000 [00:39, 72.57it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  81%|████████▏ | 2442/3000 [00:40, 69.06it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  82%|████████▏ | 2450/3000 [00:40, 70.28it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  82%|████████▏ | 2458/3000 [00:40, 65.50it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  82%|████████▏ | 2465/3000 [00:40, 65.64it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  82%|████████▏ | 2472/3000 [00:40, 62.79it/s, step size=2.22e-01, acc. prob=0.793]\rSample:  83%|████████▎ | 2479/3000 [00:40, 59.98it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  83%|████████▎ | 2487/3000 [00:40, 64.45it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  83%|████████▎ | 2494/3000 [00:40, 61.79it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  83%|████████▎ | 2501/3000 [00:41, 58.42it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  84%|████████▎ | 2507/3000 [00:41, 53.42it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  84%|████████▍ | 2513/3000 [00:41, 52.60it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  84%|████████▍ | 2520/3000 [00:41, 55.29it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  84%|████████▍ | 2529/3000 [00:41, 63.42it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  85%|████████▍ | 2536/3000 [00:41, 59.27it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  85%|████████▍ | 2544/3000 [00:41, 63.98it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  85%|████████▌ | 2551/3000 [00:41, 63.67it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  85%|████████▌ | 2558/3000 [00:41, 64.89it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  86%|████████▌ | 2565/3000 [00:42, 59.85it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  86%|████████▌ | 2572/3000 [00:42, 60.35it/s, step size=2.22e-01, acc. prob=0.790]\rSample:  86%|████████▌ | 2580/3000 [00:42, 63.90it/s, step size=2.22e-01, acc. prob=0.790]\rSample:  86%|████████▋ | 2588/3000 [00:42, 67.15it/s, step size=2.22e-01, acc. prob=0.790]\rSample:  86%|████████▋ | 2595/3000 [00:42, 66.84it/s, step size=2.22e-01, acc. prob=0.790]\rSample:  87%|████████▋ | 2603/3000 [00:42, 68.73it/s, step size=2.22e-01, acc. prob=0.790]\rSample:  87%|████████▋ | 2612/3000 [00:42, 73.43it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  87%|████████▋ | 2620/3000 [00:42, 74.43it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  88%|████████▊ | 2628/3000 [00:42, 75.30it/s, step size=2.22e-01, acc. prob=0.790]\rSample:  88%|████████▊ | 2636/3000 [00:43, 71.89it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  88%|████████▊ | 2644/3000 [00:43, 72.37it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  88%|████████▊ | 2652/3000 [00:43, 73.98it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  89%|████████▊ | 2660/3000 [00:43, 72.53it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  89%|████████▉ | 2668/3000 [00:43, 74.11it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  89%|████████▉ | 2676/3000 [00:43, 74.96it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  89%|████████▉ | 2684/3000 [00:43, 69.01it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  90%|████████▉ | 2692/3000 [00:43, 70.69it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  90%|█████████ | 2700/3000 [00:43, 68.09it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  90%|█████████ | 2707/3000 [00:44, 62.70it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  90%|█████████ | 2714/3000 [00:44, 63.91it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  91%|█████████ | 2721/3000 [00:44, 65.41it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  91%|█████████ | 2728/3000 [00:44, 65.43it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  91%|█████████ | 2735/3000 [00:44, 63.16it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  91%|█████████▏| 2742/3000 [00:44, 61.40it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  92%|█████████▏| 2749/3000 [00:44, 63.04it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  92%|█████████▏| 2756/3000 [00:44, 64.92it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  92%|█████████▏| 2763/3000 [00:44, 63.98it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  92%|█████████▏| 2770/3000 [00:45, 62.97it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  93%|█████████▎| 2778/3000 [00:45, 67.17it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  93%|█████████▎| 2785/3000 [00:45, 63.88it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  93%|█████████▎| 2792/3000 [00:45, 64.81it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  93%|█████████▎| 2799/3000 [00:45, 65.26it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  94%|█████████▎| 2806/3000 [00:45, 64.60it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  94%|█████████▍| 2813/3000 [00:45, 64.44it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  94%|█████████▍| 2821/3000 [00:45, 65.34it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  94%|█████████▍| 2829/3000 [00:45, 69.12it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  95%|█████████▍| 2837/3000 [00:46, 67.12it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  95%|█████████▍| 2844/3000 [00:46, 60.70it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  95%|█████████▌| 2851/3000 [00:46, 61.62it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  95%|█████████▌| 2859/3000 [00:46, 62.86it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  96%|█████████▌| 2867/3000 [00:46, 66.35it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  96%|█████████▌| 2875/3000 [00:46, 69.54it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  96%|█████████▌| 2883/3000 [00:46, 68.43it/s, step size=2.22e-01, acc. prob=0.791]\rSample:  96%|█████████▋| 2892/3000 [00:46, 70.05it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  97%|█████████▋| 2900/3000 [00:47, 72.32it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  97%|█████████▋| 2908/3000 [00:47, 71.42it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  97%|█████████▋| 2916/3000 [00:47, 73.23it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  97%|█████████▋| 2924/3000 [00:47, 73.97it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  98%|█████████▊| 2932/3000 [00:47, 73.65it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  98%|█████████▊| 2940/3000 [00:47, 73.39it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  98%|█████████▊| 2948/3000 [00:47, 70.75it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  99%|█████████▊| 2956/3000 [00:47, 72.61it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  99%|█████████▉| 2964/3000 [00:47, 74.24it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  99%|█████████▉| 2972/3000 [00:48, 72.62it/s, step size=2.22e-01, acc. prob=0.792]\rSample:  99%|█████████▉| 2980/3000 [00:48, 74.28it/s, step size=2.22e-01, acc. prob=0.791]\rSample: 100%|█████████▉| 2988/3000 [00:48, 74.18it/s, step size=2.22e-01, acc. prob=0.791]\rSample: 100%|█████████▉| 2996/3000 [00:48, 75.00it/s, step size=2.22e-01, acc. prob=0.791]\rSample: 100%|██████████| 3000/3000 [00:48, 62.01it/s, step size=2.22e-01, acc. prob=0.791]\n```\n:::\n:::\n\n\nAfter running the analysis, we can extract the posterior distributions for our `beta` parameter in each model. Let's see what they tell us.\n\n::: {#5c914d56 .cell execution_count=10}\n``` {.python .cell-code}\n# Get posterior samples and print results\nclip_beta_mean = clip_samples['beta'].mean().item()\nclip_beta_hdi = torch.quantile(clip_samples['beta'], torch.tensor([0.025, 0.975]))\n\nprint(f\"\\nCLIP Similarity - Bayesian Regression:\")\nprint(f\"  Beta (VerbType effect): {clip_beta_mean:.3f}\")\nprint(f\"  95% HDI: [{clip_beta_hdi[0]:.3f}, {clip_beta_hdi[1]:.3f}]\")\nprint(f\"  P(beta < 0): {(clip_samples['beta'] < 0).float().mean():.3f}\")\n\nsubject_beta_mean = subject_samples['beta'].mean().item()\nsubject_beta_hdi = torch.quantile(subject_samples['beta'], torch.tensor([0.025, 0.975]))\n\nprint(f\"\\nSubject Salience - Bayesian Regression:\")\nprint(f\"  Beta (VerbType effect): {subject_beta_mean:.3f}\")\nprint(f\"  95% HDI: [{subject_beta_hdi[0]:.3f}, {subject_beta_hdi[1]:.3f}]\") \nprint(f\"  P(beta < 0): {(subject_samples['beta'] < 0).float().mean():.3f}\")\n\nvlm_beta_mean = vlm_samples['beta'].mean().item()\nvlm_beta_hdi = torch.quantile(vlm_samples['beta'], torch.tensor([0.025, 0.975]))\n\nprint(f\"\\nVLM Score - Ordered Logistic Regression:\")\nprint(f\"  Beta (VerbType effect): {vlm_beta_mean:.3f}\")\nprint(f\"  95% HDI: [{vlm_beta_hdi[0]:.3f}, {vlm_beta_hdi[1]:.3f}]\")\nprint(f\"  P(beta < 0): {(vlm_samples['beta'] < 0).float().mean():.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCLIP Similarity - Bayesian Regression:\n  Beta (VerbType effect): -2.307\n  95% HDI: [-5.404, 0.812]\n  P(beta < 0): 0.925\n\nSubject Salience - Bayesian Regression:\n  Beta (VerbType effect): -1.537\n  95% HDI: [-4.978, 1.823]\n  P(beta < 0): 0.819\n\nVLM Score - Ordered Logistic Regression:\n  Beta (VerbType effect): -2.453\n  95% HDI: [-4.428, -0.612]\n  P(beta < 0): 0.996\n```\n:::\n:::\n\n\nThe plot below visualizes the results of our Bayesian models. For each of our three metrics, it shows the estimated effect of the verb type. Because we coded unaccusatives as +0.5 and unergatives as -0.5, the \"Beta\" (β) value represents the difference between the two.\n\nThe vertical gray line at zero is our baseline for \"no effect\". If the colored whisker line (the posterior distribution) for a metric crosses this line, it means we can't be very confident in the direction of the effect. However, this 'confidence' of course will be quantified by the probability values shown on the right side of the plot.\n\nIf the distribution is shifted to the left of zero (Negative β), it means that unaccusative scenes scored lower than unergative ones for that metric. This is the \"danger zone\" for our stimuli, as it would suggest they are harder for the models to process. As you can see, both the full sentence similarity and the VLM verification scores are shifted heavily to the left, indicating that the unaccusative pictures are indeed less clear or representative.\n\nIf the distribution were on the right side (Positive β), it would mean unaccusatives scored *higher*, suggesting they were actually easier for the models. None of our metrics show this result.\n\n::: {#b2699c18 .cell execution_count=11}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Data dictionary from your MCMC samples\nbeta_data = {\n    'Full Scene (CLIP)': clip_samples['beta'].numpy(),\n    'Subject Salience (CLIP)': subject_samples['beta'].numpy(),\n    'Scene Verification (VLM)': vlm_samples['beta'].numpy()\n}\n\n# Adjust figure size for better vertical separation\nfig, ax = plt.subplots(figsize=(7, 3))\nsns.set_style(\"whitegrid\", {'axes.grid': True, 'grid.color': '.95'})\n\nlabels = list(beta_data.keys())\ncolors = ['#3498db', '#9b59b6', '#e74c3c']\n\nfor i, label in enumerate(labels):\n    samples = beta_data[label]\n    mean_val = samples.mean()\n    \n    # 1. Calculate multiple intervals for the \"stacking\" effect\n    hdi_95 = np.percentile(samples, [2.5, 97.5])\n    hdi_80 = np.percentile(samples, [10, 90])\n    hdi_50 = np.percentile(samples, [25, 75])\n    \n    # 2. Plot the stacked lines (Bottom to Top: thinnest/widest first)\n    # 95% Interval - Thin\n    ax.hlines(i, hdi_95[0], hdi_95[1], color=colors[i], linewidth=1.5, alpha=0.4, zorder=1)\n    # 80% Interval - Medium\n    ax.hlines(i, hdi_80[0], hdi_80[1], color=colors[i], linewidth=5.0, alpha=0.7, zorder=2)\n    # 50% Interval - Thick\n    ax.hlines(i, hdi_50[0], hdi_50[1], color=colors[i], linewidth=10.0, alpha=1.0, zorder=3)\n    \n    # 3. Plot the Mean point\n    ax.plot(mean_val, i, 'o', color='white', markersize=8, zorder=4)\n    \n    # 4. Perfectly Aligned Statistics\n    p_dir = (samples < 0).mean() if mean_val < 0 else (samples > 0).mean()\n    prob_text = f\"$P(\\\\beta {'<' if mean_val < 0 else '>' } 0) = {p_dir:.2f}$\"\n    \n    # Locked to y-coordinate 'i' and x-coordinate 3.0 (outside plot area)\n    ax.text(3.0, i, prob_text, va='center', ha='left', \n            fontsize=13, fontweight='bold', color=colors[i])\n\n# 5. Descriptive Annotations (The \"How to Read\" Guide)\nax.axvline(x=0, color='black', linestyle='-', linewidth=1.5, alpha=0.6, zorder=0)\n\n# Arrow pointing Left (Negative Beta)\nax.annotate('', xy=(-5, -1.0), xytext=(-0.5, -1.0),\n            arrowprops=dict(arrowstyle=\"->\", color='gray', lw=1.5))\nax.text(-2.75, -1.4, \"Lower Scores for\\nUnaccusatives\", ha='center', color='gray', fontweight='bold')\n\n# Arrow pointing Right (Positive Beta)\nax.annotate('', xy=(2.5, -1.0), xytext=(0.5, -1.0),\n            arrowprops=dict(arrowstyle=\"->\", color='gray', lw=1.5))\nax.text(1.5, -1.4, \"Lower Scores for\\nUnergatives\", ha='center', color='gray', fontweight='bold')\n\n# 6. Final Layout Polish\nax.set_yticks(range(len(labels)))\nax.set_yticklabels(labels, fontweight='bold', fontsize=12)\nax.set_xlabel('Posterior Beta Weight (Unaccusative vs. Unergative)', fontsize=13, labelpad=45)\n\n# Lock limits so text and arrows don't shift\nax.set_xlim(-6, 3)\nax.set_ylim(-1.5, len(labels) - 0.5)\n\nsns.despine(left=True, bottom=True)\nplt.subplots_adjust(right=0.75, bottom=0.2) # Make room for text on right and guide on bottom\nplt.savefig('./model_pyro.png', dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=762 height=313}\n:::\n:::\n\n\nSo, what do these models tell us? The results are quite clear: all three of our metrics point in the same direction, suggesting that the unaccusative scenes are in some way more challenging than the unergative ones.\n\nHere's a quick summary of the findings from our Bayesian analysis:\n\n- **Scene Verification (VLM):** A strong negative effect (β ≈ -2.3, P(β<0) = 0.99). The VLM is very confident that the unaccusative sentences are a worse description of their corresponding images.\n- **Full Scene (CLIP):** A similarly strong negative effect (β ≈ -2.3, P(β<0) = 0.92). CLIP also finds a lower visual-textual fit for unaccusative scenes.\n- **Subject Salience (CLIP):** A moderate negative effect (β ≈ -1.5, P(β<0) = 0.83). The evidence is weaker here, with more uncertainty, but it still suggests that the subject is slightly harder to identify in unaccusative scenes.\n\nEven before we get to the human data, the models are sending a clear signal: these two sets of pictures might not be perceived equally. The unaccusative scenes seem to have a lower \"visual-textual fit\", which means we have to be careful not to mistake this perceptual difficulty for a purely linguistic effect. \n\nOne interpretation is that the production effects we see in humans might be at least *partially* due to the visual complexity of the pictures. However, given the broader context of the sentence production literature, this seems unlikely to be the whole story. Other studies have found similar advance planning effects in experiments that did not involve these pictures or pictures at all, such as sentence recall tasks. This suggests that the effect is not just about visual processing, but is tied to the linguistic structure of the sentences themselves. \n\n\n## Conclusion\n\n### The Finding\n\nThe analysis reveals a consistent pattern across all three metrics: unaccusative scenes are rated as more difficult or less representative by the models compared to unergative scenes.\n\n1.  **Scene Verification (VLM):** The Qwen-VL model, which was asked to explicitly rate the match between the sentence and the image, showed a strong negative effect for unaccusatives. It consistently gave lower scores to unaccusative pairs, with a high degree of certainty (P(β<0) = 0.99). This suggests that from a generative, \"common sense\" perspective, the unaccusative sentences are poorer descriptions of their corresponding images.\n\n2.  **Full Scene Similarity (CLIP):** The standard CLIP similarity score also revealed a strong negative effect for unaccusatives (P(β<0) = 0.92). This indicates that the overall visual-textual fit is lower for unaccusative scenes.\n\n3.  **Subject Salience (CLIP):** Even the salience of the subject noun was moderately lower in unaccusative scenes (P(β<0) = 0.83). While the evidence is weaker here, it suggests that the subject may be slightly harder to identify in the context of an unaccusative event.\n\nIn short, the models are telling us that the unaccusative pictures are not as clear-cut as the unergative ones.\n\n### What This Means\n\nThis computational analysis provides a crucial piece of context for the human experimental results. The key takeaway is that the unaccusative stimuli seem to be inherently more complex or ambiguous than the unergative stimuli.\n\nThis doesn't invalidate the syntactic hypothesis about advance planning, but it does add a layer of nuance. The increased processing cost observed in human speakers for unaccusative sentences might not be solely due to a syntactic operation. Instead, it could be a combination of factors:\n\n*   **Perceptual/Conceptual Difficulty:** The visual scenes for unaccusative events might be harder to parse, conceptualize, and map onto a linguistic description. The AI models, particularly the VLM, seem to be picking up on this.\n*   **Syntactic Planning:** The syntactic structure of unaccusatives may still require earlier planning, as originally hypothesized.\n\nThe most likely scenario is that these two factors are intertwined. The very nature of unaccusative events (a change of state happening *to* a patient) makes them visually more complex, and this complexity might be what triggers the earlier, more resource-intensive syntactic planning.\n\nWe can be more confident that the experimental effects are not just due to simple visual confounds like a hidden subject, but we must also acknowledge that the \"difficulty\" is not purely syntactic. It's a property of the entire event, from perception to syntax.\n\n## Final Thoughts\n\nIf you're running experiments with visual stimuli, I highly recommend giving this a try. Vision-language models like CLIP and multimodal LLMs like Qwen are freely available and give us principled ways to ask: \"Are these pictures doing what we think they're doing?\" The ability to triangulate across different model architectures—from similarity-based (CLIP) to generative (Qwen)—provides an unprecedented level of confidence in our experimental materials.\n\nThis analysis didn't end up changing my theoretical interpretation of the experimental findings, I think existing evidence for advance planning require more than random LLM analysis, which probably has nothing to do how humans represent concepts, sentences, or visual areas. \n\nOne thing that I need to do a baseline analysis in which I ran the similar analysis for a randomly assigned sentence-picture pairs to show that, when randomized the clip similarity changes dramatically. I will soon run that and upload the results as a comment. \n\n\nIf you want to run this analysis yourself, you can use the following Colab notebook: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/utkuturk/utkuturk.github.io/blob/main/clip_analysis.ipynb) You can download the cached_scores here: [cached_scores.csv](https://github.com/utkuturk/utkuturk.github.io/blob/main/posts/clip/cached_scores.csv), and the pictures here: [pictures.zip](https://github.com/utkuturk/utkuturk.github.io/blob/main/posts/clip/pictures.zip)\n\n---\n\n## References\n\nMomma, S., & Ferreira, V. (2019). Beyond linear order: The role of argument structure in speaking. *Cognitive Psychology*, 114, 101228.\n\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021). Learning transferable visual models from natural language supervision. *International Conference on Machine Learning* (pp. 8748-8763). PMLR.\n\nBai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., ... & Zhou, J. (2023). Qwen-VL: A frontier large vision-language model with versatile abilities. *arXiv preprint arXiv:2308.12966*.\n\n---\n\n## Session Info\n\nFor reproducibility, here's my setup:\n\n::: {#19f713d9 .cell execution_count=12}\n``` {.python .cell-code}\nimport sys\nprint(f\"Python: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CLIP: (installed from https://github.com/openai/CLIP)\")\nprint(f\"Transformers: (for Qwen-VL-Chat)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPython: 3.13.11 (main, Dec  5 2025, 16:06:33) [Clang 17.0.0 (clang-1700.4.4.1)]\nPyTorch: 2.9.1\nCLIP: (installed from https://github.com/openai/CLIP)\nTransformers: (for Qwen-VL-Chat)\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}