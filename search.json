[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "utku turk",
    "section": "",
    "text": "Arcs in ≈ûirince, Aug 2016, photo by me\n\n\n\nI am mostly interested in morphology and psycholinguistics. I am currently working on (i) morphological planning, i.e., when and how morphemes are planned in sentence production and (ii) how theories of generics/kind NPs can interact with the neurolinguistic findings.\nIn June 2022, I defended my MA thesis titled ‚ÄúAgreement Attraction in Turkish‚Äù in Bogazici University, where I worked with Pavel Logacev and focused on case-ambiguity, response bias, and shallow processing in agreement attraction.\nDuring my time in Bogazici, I also worked on formal semantics of hypothetical comparisons, the phonemic status of Turkish glide [j], treebanking efforts in Turkish and Laz. I spent a semester in Masaryk University, as an invited researcher, where I worked with Pavel Caha and focused on the syntax-phonology interface of Turkish case system and augmentatives in Turkish using nanosyntactic framework.\nMy favorite food is gata with koritz and my favorite icecream flavor is saffron and rose (5pm snack of Anthony Bourdain). In my freetime, I try to do some calligraphy. I co-founded Translate for Justice, an independent platform of voluntary translators from all corners of the world to help raise awareness by providing information for the international public on the issues of restriction of basic human rights and freedom."
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "utku turk",
    "section": "",
    "text": "Highlights\n      üèÅ milestoneSep 2025\n      Visiting Researcher, UMass Amherst (Fall 2025); collaborating with Faruk Akku≈ü and Shota Momma on dissociating morphosyntactic vs. morphophonological timing.\n    \n      üèÅ milestoneApr 2025\n      Advanced to candidacy at UMD after defending my QP on the timing of agreement in sentence production.slides\n    All updates\n      Sep 2025üèÅ milestone\n      Visiting Researcher, UMass Amherst (Fall 2025); collaborating with Faruk Akku≈ü and Shota Momma on dissociating morphosyntactic vs. morphophonological timing.\n    \n      Apr 2025üèÅ milestone\n      Advanced to candidacy at UMD after defending my QP on the timing of agreement in sentence production.slides\n    \n      Apr 2025üé§ talk\n      PLC 2025 talk: syntactic alternatives in Turkish polar questions (with Aron Hirsch).abstract\n    \n      Mar 2025üé§ talk\n      HSP 2025 talk: register effects and response bias in agreement attraction.abstract ¬∑ slides\n    \n      Mar 2025üé§ talk\n      TU+10 (USC): talk on register effects in agreement attraction; poster on scope freezing in Turkish (with Aron Hirsch).talk abstract ¬∑ poster abstract\n    \n      Jan 2025üé§ talk\n      LSA 2025 talk: Superiority effects in Turkish (with Sadira Lewis).slides\n    \n      Oct 2024üé§ talk\n      Puzzles of Agreement: relative timing of agreement vs. verb planning.slides\n    \n      Oct 2024üåê site\n      HSP 2025 site is live‚Äîsubmit your work!website\n    \n      Aug 2024üìÑ pub\n      OUP chapter in press: suppletion and suspended affixation.pdf ¬∑ lingbuzz\n    \n      Jul 2024üé§ talk\n      Designed and taught a workshop on production experiments at the University of Oxford (with Colin Phillips, Allison Dods, and Eun-Kyoung Rosa Lee).handout\n    \n      May 2024üé§ talk\n      HSP 2024 posters: agreement planning and task effects in production.agreement planning ¬∑ task effects\n    \n      Apr 2024üìÑ pub\n      Book chapter on the phonemic status of Turkish glide [j] published.pdf\n    \n      Mar 2024üìÑ pub\n      Paper on case syncretism and agreement attraction (with Pavel Logaƒçev) published in Language, Cognition and Neuroscience.pdf ¬∑ LCN\n    \n      Mar 2023üé§ talk\n      HSP 2023: talk on response bias and agreement attraction.slides"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "utku turk",
    "section": "",
    "text": "[1]Turk, U., Aron, H. (2025, April). Syntactic alternatives in Turkish polar questions. Talk given at PLC 49: The Penn Linguistics Conference. University of Pennsylvania: Philadelphia, PA, USA. [abstract]\n\n\n[2]Turk, U. (2025, March). Turkish register manipulation eliminates grammaticality asymmetry in attraction and challenges retrieval based accounts. Talk given at HSP 2025: Human Sentence Processing. UMD, College Park, MD, USA. [abstract] [slides]\n\n\n[3]Turk, U. (2025, March). Register modulates grammaticality asymmetry in Turkish agreement attraction. Talk given at the 10th Workshop on Turkic and languages in contact with Turkic. USC: Los Angeles, CA, USA. [abstract]\n\n\n[4]Turk, U., Hirsch, A. (2025, March). Turkish allows covert scrambling. Poster presented at the 10th Workshop on Turkic and languages in contact with Turkic. USC: Los Angeles, CA, USA. [abstract]\n\n\n[5]Lewis, S., Turk, U. (2025, January). Superiority Effects with Wh-Adjuncts in Turkish. Talk given at LSA 2025: Linguistic Society of America. Philadelphia, PA, USA. [slides]\n\n\n[6]Turk, U., Lau, E., Phillips, C. (2024, October). When do we plan agreement in our speech: Case from agreement attraction in unaccusatives. Talk given at Puzzles of Agreement:Syntactic, Semantic, and Psycholinguistic Perspectives. [abstract] [slides]\n\n\n[7]Turk, U., Phillips, C. (2024, May). Speech timing evidence on the (in)dependence of root and inflection access in production. Poster presented at Human Sentence Processing 2024. [abstract]\n\n\n[8]Dods, A., Macdonald, A., Turk, U., Mancha, S., Phillips, C. (2024, May). Is the octopus regenerating?: Comparing timing effects in sentence recall and picture description tasks. Poster presented at Human Sentence Processing 2024. [abstract]\n\n\n[9]Turk, U., Logacev, P. (2023, March). Novel analysis of response bias challenges representational accounts in attraction. Talk given at Human Sentence Processing 2023. [abstract]\n\n\n[10]Turk, U. (2023, March). Obviation Problem in Turkish. Poster presented at the 8th Workshop on Turkic and languages in contact with Turkic. Harvard University: Cambridge, MA, USA. [abstract]\n\n\n[11]Sampanis, K., Turk, U. (2021, September). Defining borrowing hierarchies in the light of sociolinguistic and geodemographic causation: contact-induced morphosyntactic change in Asia Minor Greek. Talk given (virtually) at 54th Annual Meeting of the Societas Linguistica Europaea.\n\n\n[12]Turk, U., Logacev, P. (2021, September). Response Bias Manipulation in Turkish Agreement Attraction. Poster presented (virtually) at AMLaP 2021: Architectures and Mechanisms for Language Processing. Paris, France. [abstract] [slides]\n\n\n[13]Turk, U. (2021, September). Effects of Vowel Characteristics in Suspended Affixation with Root Allomorphy. Poster presented (virtually) at AMLaP 2021: Architectures and Mechanisms for Language Processing. Paris, France. [abstract] [slides]\n\n\n[14]Turk, U., Atmaca, F., √ñzate≈ü, ≈û. B., Berk, G., Bedir, S. T., K√∂ksal, A., √ñzTurk Ba≈üaran, B., G√ºng√∂r, T., √ñzg√ºr, A. (2021, August). The more the merrier: a new dependency treebank for Turkish. Talk given (virtually) at the 20th International Conference on Turkish Linguistics. Eski≈üehir, Turkey.\n\n\n[15]Canalis, S., √ñzdemir, S., Turk, U., Tun√ßer, √ú.C. (2021, August). The phonological nature of the Turkish front glide. Talk given (virtually) at the 20th International Conference on Turkish Linguistics. Eski≈üehir, Turkey.\n\n\n[16]Turk, U., Demirok, √ñ. (2021, March). Hypothetical Comparison in Turkish. Talk given (virtually) at the 45th Penn Linguistics Conference. University of Pennsylvania: Philadelphia, PA, USA. [abstract] [handout]\n\n\n[17]Turk, U., Demirok, √ñ. (2021, February). Hypothetical Comparison in Turkish. Talk given (virtually) at the 6th Workshop on Turkic and languages in contact with Turkic. University of Toronto: Toronto, ON, Canada. [abstract] [handout]\n\n\n[18]Turk, U., Caha, P. (2021, February). Nanosyntactic Analysis of Turkish Cases. Poster presented (virtually) at the 6th Workshop on Turkic and languages in contact with Turkic. University of Toronto: Toronto, ON, Canada. [abstract] [handout]\n\n\n[19]Turk, U. (2021, January). When vowel harmony has a say in morpho-syntax. Talk given (virtually) at ConSOLE29: the 29th Conference of the Student Organization of Linguistics in Europe. University of Leiden: Leiden, Netherlands. [abstract] [slides]\n\n\n[20]Canalis, S., √ñzdemir, S., Turk, U., Tun√ßer, √ú.C. (2021, January). When glides are obstruents, or Turkish [j]. Talk given (virtually) at the 18th Old World Phonology Conference. University of the Balearic Islands: Palma, Spain. [abstract] [slides]\n\n\n[21]Turk, U., Bayar, K., √ñzercan, A.D., √ñzate≈ü, ≈û.B. (2020, December). First steps towards Universal Dependencies for Laz. Poster presented (virtually) at the 4th Universal Dependencies Workshop, COLING 2020. Barcelona, Spain.\n\n\n[22]Turk, U., Logacev, P. (2020, February). The role of shallow processing in agreement attraction. Poster presented (virtually) at the 33rd annual CUNY Sentence Processing Conference. University of Massachusetts. [abstract]\n\n\n[23]Turk, U., Logacev, P. (2020, February). The role of shallow processing in agreement attraction. Talk given at Linguistic Evidence: Linguistic Theory Enriched by Experimental Data. Universit√§t T√ºbingen: T√ºbingen, Germany. [abstract]\n\n\n[24]√ñzdemir, S., Turk, U. (2020, February). An investigation into the nature of Turkish glide [j]. Poster presented at the 5th Workshop on Turkic and languages in contact with Turkic. University of Delaware: Newark, DE, USA. [abstract]\n\n\n[25]Turk, U. (2020, February). Tackling the augmentative puzzle in Turkish. Poster presented at the 5th Workshop on Turkic and languages in contact with Turkic. University of Delaware: Newark, DE, USA. [abstract]\n\n\n[26]Turk, U., Atmaca, F., √ñzate≈ü, ≈û.B., √ñzTurk, B., G√ºng√∂r, T., √ñzg√ºr A. (2019, August). Improving the annotations in the Turkish Universal Dependency Treebank. Talk given at the 3rd Universal Dependencies Workshop, SyntaxFest2019. Sorbonne Universit√©: Paris, France.\n\n\n[27]Turk, U., Atmaca, F., √ñzate≈ü, ≈û.B., √ñzTurk, B., G√ºng√∂r, T., √ñzg√ºr A. (2019, August). Turkish Treebanking: Unifying and Constructiong Efforts. Poster presented at the 13th Linguistic Annotation Workshop, ACL 2019. Florence, Italy.\n\n\n[28]Turk, U. (2019, June). Decomposing Turkish Augmentatives. Talk given at the 16th Workshop on Syntax, Semantics, and Phonology. Universidad Complutense de Madrid: Madrid, Spain.\n\n\n[29]Turk, U. (2019, May). Nanosyntax of Augmentative in Turkish: Interfix Analysis. Poster presented at AIMM 4: the 4th American International Morphology Meeting. Stony Brook University: NY, USA.\n\n\n[30]Turk, U., Dikmen, F. (2019, April). Obligatory Adjuncts within Turkish Impersonals. Talk given at the 13th Linguistics Student Conference. Ankara University: Ankara, Turkey."
  },
  {
    "objectID": "talks.html#conference-poster-talks",
    "href": "talks.html#conference-poster-talks",
    "title": "utku turk",
    "section": "",
    "text": "[1]Turk, U., Aron, H. (2025, April). Syntactic alternatives in Turkish polar questions. Talk given at PLC 49: The Penn Linguistics Conference. University of Pennsylvania: Philadelphia, PA, USA. [abstract]\n\n\n[2]Turk, U. (2025, March). Turkish register manipulation eliminates grammaticality asymmetry in attraction and challenges retrieval based accounts. Talk given at HSP 2025: Human Sentence Processing. UMD, College Park, MD, USA. [abstract] [slides]\n\n\n[3]Turk, U. (2025, March). Register modulates grammaticality asymmetry in Turkish agreement attraction. Talk given at the 10th Workshop on Turkic and languages in contact with Turkic. USC: Los Angeles, CA, USA. [abstract]\n\n\n[4]Turk, U., Hirsch, A. (2025, March). Turkish allows covert scrambling. Poster presented at the 10th Workshop on Turkic and languages in contact with Turkic. USC: Los Angeles, CA, USA. [abstract]\n\n\n[5]Lewis, S., Turk, U. (2025, January). Superiority Effects with Wh-Adjuncts in Turkish. Talk given at LSA 2025: Linguistic Society of America. Philadelphia, PA, USA. [slides]\n\n\n[6]Turk, U., Lau, E., Phillips, C. (2024, October). When do we plan agreement in our speech: Case from agreement attraction in unaccusatives. Talk given at Puzzles of Agreement:Syntactic, Semantic, and Psycholinguistic Perspectives. [abstract] [slides]\n\n\n[7]Turk, U., Phillips, C. (2024, May). Speech timing evidence on the (in)dependence of root and inflection access in production. Poster presented at Human Sentence Processing 2024. [abstract]\n\n\n[8]Dods, A., Macdonald, A., Turk, U., Mancha, S., Phillips, C. (2024, May). Is the octopus regenerating?: Comparing timing effects in sentence recall and picture description tasks. Poster presented at Human Sentence Processing 2024. [abstract]\n\n\n[9]Turk, U., Logacev, P. (2023, March). Novel analysis of response bias challenges representational accounts in attraction. Talk given at Human Sentence Processing 2023. [abstract]\n\n\n[10]Turk, U. (2023, March). Obviation Problem in Turkish. Poster presented at the 8th Workshop on Turkic and languages in contact with Turkic. Harvard University: Cambridge, MA, USA. [abstract]\n\n\n[11]Sampanis, K., Turk, U. (2021, September). Defining borrowing hierarchies in the light of sociolinguistic and geodemographic causation: contact-induced morphosyntactic change in Asia Minor Greek. Talk given (virtually) at 54th Annual Meeting of the Societas Linguistica Europaea.\n\n\n[12]Turk, U., Logacev, P. (2021, September). Response Bias Manipulation in Turkish Agreement Attraction. Poster presented (virtually) at AMLaP 2021: Architectures and Mechanisms for Language Processing. Paris, France. [abstract] [slides]\n\n\n[13]Turk, U. (2021, September). Effects of Vowel Characteristics in Suspended Affixation with Root Allomorphy. Poster presented (virtually) at AMLaP 2021: Architectures and Mechanisms for Language Processing. Paris, France. [abstract] [slides]\n\n\n[14]Turk, U., Atmaca, F., √ñzate≈ü, ≈û. B., Berk, G., Bedir, S. T., K√∂ksal, A., √ñzTurk Ba≈üaran, B., G√ºng√∂r, T., √ñzg√ºr, A. (2021, August). The more the merrier: a new dependency treebank for Turkish. Talk given (virtually) at the 20th International Conference on Turkish Linguistics. Eski≈üehir, Turkey.\n\n\n[15]Canalis, S., √ñzdemir, S., Turk, U., Tun√ßer, √ú.C. (2021, August). The phonological nature of the Turkish front glide. Talk given (virtually) at the 20th International Conference on Turkish Linguistics. Eski≈üehir, Turkey.\n\n\n[16]Turk, U., Demirok, √ñ. (2021, March). Hypothetical Comparison in Turkish. Talk given (virtually) at the 45th Penn Linguistics Conference. University of Pennsylvania: Philadelphia, PA, USA. [abstract] [handout]\n\n\n[17]Turk, U., Demirok, √ñ. (2021, February). Hypothetical Comparison in Turkish. Talk given (virtually) at the 6th Workshop on Turkic and languages in contact with Turkic. University of Toronto: Toronto, ON, Canada. [abstract] [handout]\n\n\n[18]Turk, U., Caha, P. (2021, February). Nanosyntactic Analysis of Turkish Cases. Poster presented (virtually) at the 6th Workshop on Turkic and languages in contact with Turkic. University of Toronto: Toronto, ON, Canada. [abstract] [handout]\n\n\n[19]Turk, U. (2021, January). When vowel harmony has a say in morpho-syntax. Talk given (virtually) at ConSOLE29: the 29th Conference of the Student Organization of Linguistics in Europe. University of Leiden: Leiden, Netherlands. [abstract] [slides]\n\n\n[20]Canalis, S., √ñzdemir, S., Turk, U., Tun√ßer, √ú.C. (2021, January). When glides are obstruents, or Turkish [j]. Talk given (virtually) at the 18th Old World Phonology Conference. University of the Balearic Islands: Palma, Spain. [abstract] [slides]\n\n\n[21]Turk, U., Bayar, K., √ñzercan, A.D., √ñzate≈ü, ≈û.B. (2020, December). First steps towards Universal Dependencies for Laz. Poster presented (virtually) at the 4th Universal Dependencies Workshop, COLING 2020. Barcelona, Spain.\n\n\n[22]Turk, U., Logacev, P. (2020, February). The role of shallow processing in agreement attraction. Poster presented (virtually) at the 33rd annual CUNY Sentence Processing Conference. University of Massachusetts. [abstract]\n\n\n[23]Turk, U., Logacev, P. (2020, February). The role of shallow processing in agreement attraction. Talk given at Linguistic Evidence: Linguistic Theory Enriched by Experimental Data. Universit√§t T√ºbingen: T√ºbingen, Germany. [abstract]\n\n\n[24]√ñzdemir, S., Turk, U. (2020, February). An investigation into the nature of Turkish glide [j]. Poster presented at the 5th Workshop on Turkic and languages in contact with Turkic. University of Delaware: Newark, DE, USA. [abstract]\n\n\n[25]Turk, U. (2020, February). Tackling the augmentative puzzle in Turkish. Poster presented at the 5th Workshop on Turkic and languages in contact with Turkic. University of Delaware: Newark, DE, USA. [abstract]\n\n\n[26]Turk, U., Atmaca, F., √ñzate≈ü, ≈û.B., √ñzTurk, B., G√ºng√∂r, T., √ñzg√ºr A. (2019, August). Improving the annotations in the Turkish Universal Dependency Treebank. Talk given at the 3rd Universal Dependencies Workshop, SyntaxFest2019. Sorbonne Universit√©: Paris, France.\n\n\n[27]Turk, U., Atmaca, F., √ñzate≈ü, ≈û.B., √ñzTurk, B., G√ºng√∂r, T., √ñzg√ºr A. (2019, August). Turkish Treebanking: Unifying and Constructiong Efforts. Poster presented at the 13th Linguistic Annotation Workshop, ACL 2019. Florence, Italy.\n\n\n[28]Turk, U. (2019, June). Decomposing Turkish Augmentatives. Talk given at the 16th Workshop on Syntax, Semantics, and Phonology. Universidad Complutense de Madrid: Madrid, Spain.\n\n\n[29]Turk, U. (2019, May). Nanosyntax of Augmentative in Turkish: Interfix Analysis. Poster presented at AIMM 4: the 4th American International Morphology Meeting. Stony Brook University: NY, USA.\n\n\n[30]Turk, U., Dikmen, F. (2019, April). Obligatory Adjuncts within Turkish Impersonals. Talk given at the 13th Linguistics Student Conference. Ankara University: Ankara, Turkey."
  },
  {
    "objectID": "talks.html#invited-internal-talks",
    "href": "talks.html#invited-internal-talks",
    "title": "utku turk",
    "section": "Invited & Internal Talks",
    "text": "Invited & Internal Talks\n\nT√ºrk, U. (February, 2024). Timing of Agreement in Verb Planning. Talk given at UMass/UMD Psycholinguistic Mixer. University of Massachusetts: Amherst, MA, USA.\nT√ºrk, U. (January, 2024). Controlling morphosyntactic competition through phonology. Talk given at S-Lab. University of Maryland: College Park, MD, USA. handout\nT√ºrk, U. (December, 2022). As If they were Turkish As Ifs. Talk given at Meaning Meeting. University of Maryland: College Park, MD, USA.\nT√ºrk, U. (October, 2022). Sources of Bias in Psycholinguistic Data. Talk given at Language Science Lunch Talks. Language Science Center, University of Maryland: College Park, MD, USA.\nT√ºrk, U. (February, 2019). Is Accusative Case Really the Case? Talk given at 1st Bogazici University Student Conference on Theoretical and Experimental Linguistics. Bogazici University: Istanbul, Turkey."
  },
  {
    "objectID": "posts/as-if/index.html",
    "href": "posts/as-if/index.html",
    "title": "can ‚Äòas if‚Äô clauses be degree comparisons?",
    "section": "",
    "text": "After reading Justin Bledin‚Äôs work on English as ifs, we realized that Turkish counterparts of as ifs behave differently. It seemed that instead of a simple comparison of events, Turkish as ifs seems to be susceptible to gradability and degree.\nTo account for Turkish facts, we proposed that a novel analysis of these sentences. While previous accounts were based on event similarity, our hypothesis is based on comparing degrees of gradable elements involved in the event.\nConsider this sentence: Pedro was dancing as if he was possessed by a demon. And, let‚Äôs take a look at the interpretations in (1) and (2). (1) is the simplified version of previous accounts. The key word for those analyses is the resemblance. On the other hand, (2) is our account in a simplified manner. Instead of using resemblance, we proposed an equatative approach. Our analysis compares two degrees of adverbials.\n\n\n\nPedro‚Äôs dancing in the actual world resembles his dancing in other possible worlds where he is possessed by a demon.\n\n\n\n\nThe level of craziness of Pedro‚Äôs dancing in the actual world is ‚â• the level of craziness of his dancing in other possible worlds where he is possessed by a demon.\n\n\n\nHere‚Äôs the facts that support our analysis:\nUnlike English and German, Turkish does not have an overt conditional marker in as ifs constructions.\n\n\n\nPedro was dancing he was possessed by a demon.\n\n\n\n\n\n\n\n¬†\n\n\nPedro i√ßine cin ka√ß-mƒ±≈ücasƒ±na {dans ed-iyor-du}\n\n\nPedro inside jhin escape-HCM dance-IMPF-PST.3SG\n\n\n‚ÄúPedro was dancing as if he was possessed by a demon.‚Äù\n\n\n\n\n\n\nIt is frequently used with an adverbial in a matrix sentence with a verbal predicate.\n\n\n\n\n\n\n¬†\n\n\nPedro i√ßine cin ka√ß-mƒ±≈ücasƒ±na √ßƒ±lgƒ±nca {dans ed-iyor-du}\n\n\nPedro inside jhin escape-HCM crazily dance-IMPF-PST.3SG\n\n\n‚ÄúPedro was dancing as if he was possessed by a demon.‚Äù\n\n\n\n\n\nEven when there is no overt adverbial, the use of -mƒ±≈ücasƒ±na phrase necessitates an adverbial reading to be accessed from the context. When we use -mƒ±≈ücasƒ±na phrase in a sentence where there is no possible adverbial is available in the context, the sentence become infelicitous.\n\n\n\n\n\n\n*\n\n\nYarƒ±n meteor yaƒü-acak-mƒ±≈ücasƒ±na dans ed-iyor-sun.\n\n\ntomorrow meteor rain-FUT-HCM dance-IMPF-2SG\n\n\n‚ÄúYou are dancing [???] as if there will be a meteor shower tomorrow.‚Äù\n\n\n\n\n\nMore importantly, these adverbs are always gradable adverbials. Similarly, when the matrix predicate is non-verbal, it has to be a gradable adjective.\n\n\n\n\n\n\n¬†\n\n\nHelin pandemi bit-mi≈ücesine mutlu.\n\n\nHelin pandemic end-HCM happy.3SG\n\n\n‚ÄúHelin is happy as if the pandemic is over.‚Äù\n\n\n\n\n\n\n\n\n\n*\n\n\nCevab-ƒ± soru-yu duy-ma-mƒ±≈ücasƒ±na yanlƒ±≈ü-tƒ± .\n\n\nanswer-POSS.3SG question-ACC hear-NEG-HCM wrong-PST.3SG\n\n\n‚ÄúIntended: His answer was wrong as if he did not hear the question.‚Äù\n\n\n\n\n\nLastly, when there is an already established degree-related operation, we cannot use -mƒ±≈ücasƒ±na phrases. This is similar to the cases where we cannot use two degree operators most and more at the same time.\n\n\n\n\n\n\n¬†\n\n\nMecidiyek√∂y cehennem-mƒ±≈ücesine kalabalƒ±k bir il√ße-miz-dir.\n\n\nMecidiyek√∂y hell-HCM crowded a district-POSS.1PL-AOR\n\n\n‚ÄúMecidiyek√∂y is a crowded district as if it was hell.‚Äù\n\n\n\n\n\n\n\n\n\n*\n\n\nMecidiyek√∂y cehennem-mƒ±≈ücesine en kalabalƒ±k il√ße-miz-dir.\n\n\nMecidiyek√∂y hell-HCM most crowded district-POSS.1PL-AOR\n\n\n‚ÄúIntended: Mecidiyek√∂y is the most crowded district as if it was hell.‚Äù\n\n\n\n\n\nThe distribution of -mƒ±≈ücasƒ±na clauses shows that Turkish ‚Äúas if‚Äù does not simply signal resemblance between two events, but rather compares the degree of gradable properties across possible worlds. This degree-based perspective explains why such clauses pattern with adverbials and adjectives that admit gradability, and why they fail in contexts that already contain competing degree operators."
  },
  {
    "objectID": "posts/mfa/index.html",
    "href": "posts/mfa/index.html",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "",
    "text": "We have our data from PCIbex and our zip files from the server. Let‚Äôs assume we have unzipped them and converted them from .webm to .wav files. Now, it‚Äôs time to align them using MFA. But before that, we need to prepare our files accordingly. Here are the steps we will follow in this document:\n\nLoad the PCIbex results\nFilter out irrelevant sound files\nMove all of our .wav and .TextGrid files to the same directory\nRename our files according to MFA guidelines\nRun MFA\nCreate a dataframe\n\nBefore we start, let me load my favorite packages. The library() function loads the packages we need, assuming they are already installed. If not, use the install.packages() function to install them. While library() does not require quotes, you should use quotes with install.packages(), e.g., install.packages(\"tidyverse\"). If it asks you to select a mirror from a list, choose a location geographically close to you, such as the UK.\n\nlibrary(tidyverse) # I have to have tidyverse\nlibrary(stringr) # to manipulate string\nlibrary(readtextgrid) # to read TextGrid files\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/mfa/index.html#read-the-results",
    "href": "posts/mfa/index.html#read-the-results",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "Read the results",
    "text": "Read the results\nThe main reason we are loading PCIbex results is because sometimes we use the async() function in our PCIbex code. The async() function allows us to send recordings to our server whenever we want without waiting for the end of the experiment. Even though it is extremely helpful in reducing some of the server-PCIbex connection load at the end of the experiment, it also creates some pesky situations. For example, if a participant decides not to complete their experiment, we will still end up with some of their recordings. We do not want participants who were window-shopping, mainly because we are not sure about the quality of their data. Luckily for us, PCIbex only saves the results of participants who complete the entire experiment.\nTo read the PCIbex results, we are going to use the function provided in the PCIbex documentation. Scroll down on that page, and you will see the words ‚ÄúClick for Base R Version.‚Äù The function is provided there as well. Moreover, please be careful whenever you are copying and pasting functions from this file, or any file, as sometimes PDF or HTML files can include unwanted elements, like a page number.\n\n# User-defined function to read in PCIbex Farm results files\nread.pcibex &lt;- function(\n    filepath, \n    auto.colnames=TRUE, \n    fun.col=\\(col,cols){cols[cols==col]&lt;-paste(col,\"Ibex\",sep=\".\");return(cols)}\n    ) {\n  n.cols &lt;- max(count.fields(filepath,sep=\",\",quote=NULL),na.rm=TRUE)\n  if (auto.colnames){\n    cols &lt;- c()\n    con &lt;- file(filepath, \"r\")\n    while ( TRUE ) {\n      line &lt;- readLines(con, n = 1, warn=FALSE)\n      if ( length(line) == 0) {\n        break\n      }\n      m &lt;- regmatches(line,regexec(\"^# (\\\\d+)\\\\. (.+)\\\\.$\",line))[[1]]\n      if (length(m) == 3) {\n        index &lt;- as.numeric(m[2])\n        value &lt;- m[3]\n        if (is.function(fun.col)){\n         cols &lt;- fun.col(value,cols)\n        }\n        cols[index] &lt;- value\n        if (index == n.cols){\n          break\n        }\n      }\n    }\n    close(con)\n    return(read.csv(filepath, comment.char=\"#\", header=FALSE, col.names=cols))\n  }\n  else{\n    return(read.csv(filepath, comment.char=\"#\", header=FALSE, col.names=seq(1:n.cols)))\n  }\n}\n\nSo, now what we have to do is load our file. I also want to check my file using the str() function. Please run ?str to see what this function does. For any function that you do not understand, you can run the ? operator to see the help pages and some examples.\n\nibex &lt;- read.pcibex(\"~/octo-recall-ibex.csv\")\nstr(ibex)\n\nNow, what I want to do is to get to filenames that are recorded in the PCIbex results. Before doing that, I advise you to go to this documentation and read more about PCIbex under the Basic Concepts header.\n\n\n\n\n\n\n\nColumn\nInformation\n\n\n\n\n1\nTime results were received (seconds since Jan 1 1970)\n\n\n2\nMD5 hash identifying subject. This is based on the subject‚Äôs IP address and various properties of their browser. Together with the value of the first column, this value should uniquely identify each subject.\n\n\n3\nName of the controller for the entity (e.g.¬†‚ÄúDashedSentence‚Äù)\n\n\n4\nItem number\n\n\n5\nElement number\n\n\n6\nLabel. Label of the newTrial()\n\n\n7\nLatin.Square.Group. The group they are assigned to.\n\n\n8\nPennElementType. Name of the specific element, like ‚ÄúHtml‚Äù, ‚ÄúMediaRecorder‚Äù\n\n\n9\nPennElementName. Name we have given to the specific Penn Elements.\n\n\n10\nParameter. This is about what type of element the script is running and saving as a parameter.\n\n\n11\nValue. Value saved for the parameters in column 10.\n\n\n12\nEventTime. Time that specific Element is screened or any action taken with that Element (seconds since Jan 1 1970)"
  },
  {
    "objectID": "posts/mfa/index.html#filter-the-results",
    "href": "posts/mfa/index.html#filter-the-results",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "Filter the results",
    "text": "Filter the results\nSince we are dealing with media recordings, I will first filter the file using the PennElementType column and will only select rows with ‚ÄúMediaRecorder‚Äù values in that column.\n\nibex &lt;- ibex |&gt; filter(PennElementType == \"MediaRecorder\")\nunique(ibex$Value)[1:3]\n\nAfter checking my Value column where the file names for MediaRecorder are stored, I realize that this will not be enough given that we still have other unwanted elements like test-recorder.webm file or some practice files. There are multiple ways to get rid of these files, and you have to think about how to get rid of them for your own specific dataframe. For my own data, I will filter my data utilizing the labels I provided in my PCIbex code. They are stored in the Labels column. What I want is to only get the MediaRecorders that are within a trial whose label starts with the word trial.\nYou may have coded your data differently; you may have used a different word; you may not even have any practice or test-recorders, so maybe you do not even need this second filtering. Check your dataframe using the View() function. I am also using a function called str_detect(), which detects a regular expression pattern, in this case ^trial, meaning starting with the word trial. Now, when I check my dataframe, I will only see experimental trials and recordings related to those trials. Just to make sure, I am also using the unique() function so that I do not have repetitions. And, I am assigning my filenames to a list called ibex_files. You can see that any random sample with the sample() function will give filenames related to experimental trials.\n\nibex &lt;- ibex |&gt; filter(str_detect(Label, \"^trial\"))\nibex_files &lt;- ibex$Value |&gt; unique()\nsample(ibex_files, 3)"
  },
  {
    "objectID": "posts/mfa/index.html#operators-used-in-this-section",
    "href": "posts/mfa/index.html#operators-used-in-this-section",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "Operators used in this section",
    "text": "Operators used in this section\n\n?\nOpens the help page for any function.\nexample use: ?library()\n\n\n\n==\nTest for equality. Don‚Äôt confuse with a single =, which is an assignment operator (and also always returns TRUE).\nexample use: ``\n\n\n\n|&gt;\n(Forward) pipe: Use the expression on the left as a part of the expression on the right.\n\nRead x |&gt; fn() as ‚Äòuse x as the only argument of function fn‚Äô.\nRead x |&gt; fn(1, 2) as ‚Äòuse x as the first argument of function fn‚Äô.\nRead x |&gt; fn(1, ., 2) as ‚Äòuse x as the second argument of function fn‚Äô.\n\nexample use: ``"
  },
  {
    "objectID": "posts/mfa/index.html#our-task-list",
    "href": "posts/mfa/index.html#our-task-list",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "Our Task List",
    "text": "Our Task List\n\nLoad the PCIbex results\nFilter irrelevant sound files\nMove all of our .wav and .TextGrid files to the same directory\nRename our files according to MFA guidelines\nRun MFA\nCreate a dataframe"
  },
  {
    "objectID": "posts/mfa/index.html#operators-used-in-this-section-1",
    "href": "posts/mfa/index.html#operators-used-in-this-section-1",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "Operators used in this section",
    "text": "Operators used in this section\n\n%in%\nTest for membership\nexample use: ``"
  },
  {
    "objectID": "posts/mfa/index.html#our-task-list-1",
    "href": "posts/mfa/index.html#our-task-list-1",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "Our Task List",
    "text": "Our Task List\n\nLoad the PCIbex results\nFilter irrelevant sound files\nMove all of our .wav and .TextGrid files to the same directory\nRename our files according to MFA guidelines\nRun MFA\nCreate a dataframe"
  },
  {
    "objectID": "posts/mfa/index.html#one-file-example",
    "href": "posts/mfa/index.html#one-file-example",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "One-file example",
    "text": "One-file example\nMy files, after unzipping, converting to .wav, and filtering according to the gold list, look like the following. There are some important things to keep in mind here. First of all, now that everything is in our gold directory, we have to use the gold_dir variable to list our files. Secondly, we again need to use the pattern argument to make sure we only select relevant files. The last thing to be aware of in the next code is that I am using indexing with square brackets to refer to the first elements in the list. I will use this element to first ensure that what I am doing is correct.\n\ngold_dir &lt;- \"~/data/gold\"\nfiles &lt;- list.files(gold_dir, pattern = \"\\\\.wav$|\\\\.TextGrid$\")\nexample_file &lt;- files[1]\nexample_file\n\n\nGet the extension and the name\nNow that we have the name of an example file, we can start by extracting its extension. We will use the function file_ext from the package called tools. Sometimes, we do not want to load an entire package, but we want to access a single function. In those cases, we use the operator ::. Additionally, we will use paste0 to prefix the extension with a dot, so that we can use it later when we rename our files.\n\nextension &lt;- tools::file_ext(example_file)\nextension &lt;- paste0(\".\", extension)\nextension\n\nAs for the rest of the name, we will use the file_path_sans_ext() function that we used earlier.\n\nrest &lt;- tools::file_path_sans_ext(example_file)\nrest\n\n\n\nGet the subject id\nNow, the most important part is getting the subject name. If you look at what my rest variable returned, you can see that it consists of the last 4 characters, which are also the last set of characters after the last underscore. There are multiple ways to extract the subject id. I will show you both methods so that you can choose and adapt them for your own data. For the underscore version, we will use the function str_split(), and for the character counting, we will use str_sub().\n\nUnderscore approach\nstr_split() takes a string and splits it according to the separator you provide. In our case, the separator is the underscore. We are also using an additional argument called simplify to make the resulting elements more user-friendly. Our function now returns a small table with 1 row and 5 columns. To select the values in the 5th column, we use square brackets again, this time with a comma. When you apply this approach to your own data, remember that you may end up with fewer or more than 5 columns depending on your naming convention. Be sure to adjust the column number accordingly. It might also be the case that your subject id is not stored last or that your separators are not underscores but simple ‚Äú-‚Äù. Modify the code according to your specific needs.\n\n# Using the underscore information\nsubj &lt;- str_split(rest, \"_\", simplify = TRUE)\nsubj\nsubj &lt;- subj[,5]\nsubj\n\nLastly, we have to modify the rest variable so that we do not include the subject id twice. I will use the same approach again. After obtaining the table, I will use the paste() function to concatenate the columns back together with the underscore separator. Adjust the number of columns used in this function and the separator according to your own data needs.\n\nnosubj &lt;- str_split(rest, \"_\", simplify = TRUE)\nnosubj &lt;- paste(nosubj[,1], nosubj[,2], nosubj[,3], nosubj[,4], sep = \"_\")\nnosubj\n\n\n\nCharacter approach\nstr_sub() allows you to extract a substring using indices. In my case, the subject id is the last four characters. To refer to characters from the end, you can use the minus symbol -. I specify -4 in the start argument, which means I want to extract the string starting from the fourth character counting back from the end.\n\nsubj &lt;- str_sub(rest, start = -4)\nsubj\n\nTo get the rest of the filename, I specify the starting point as 1 and the endpoint as -6. Using -5 would include the underscore as well.\n\nnosubj &lt;- str_sub(rest, start = 1, end = -6)\nnosubj\n\n\n\n\nPut the new name and the path together\nAt this point, we have everything we need: (i) the subject id prefix, (ii) the rest of our file name, and (iii) the extension. Now, we need to combine all of this together. We are going to use the paste0() function. Remember, this function is different from paste(). The main difference is that with paste0(), we cannot specify separators; we have to provide everything. This might seem like a disadvantage at first, but it is beneficial for non-pattern cases like this.\n\nnew_name &lt;- paste0(subj, \"_\", nosubj, extension)\nnew_name\n\nWe also need to create a new path to rename our file.\n\nnew_path &lt;- file.path(gold_dir, new_name)\nnew_path\n\n\n\nRename the file\nWe will once again use the file.rename() function. This time, we are only changing the file name and not the path, so the file will remain in its current location. We also need to obtain the full path of our example_file. We can achieve this easily using the file.path function again.\n\nexample_file_path &lt;- file.path(gold_dir, example_file)\nexample_file_path\n\n\nfile.rename(example_file_path, new_path)\n\nAfter running this, make sure the naming convention is as we want. Check your folder by searching for the trial. It should look something like subj_rest.wav or subj_rest.TextGrid. In my case, it is `, wheredltc` is my subject id or subj."
  },
  {
    "objectID": "posts/mfa/index.html#little-treat-for-you",
    "href": "posts/mfa/index.html#little-treat-for-you",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "Little treat for you",
    "text": "Little treat for you\nI know that some of your files look like the following: squid_S_jtfr.wav. Here, I will provide you with the code to rename this. Please check this code before using it. First, let‚Äôs arbitrarily assign this name to a variable. Remember, in your case, you will obtain this from your files list.\n\nexample_file &lt;- \"squid_S_jtfr.wav\"\n\nNow, I am going to put all the code together in one chunk, except for moving. Also, be aware that I am using my own gold_dir; please specify yours according to your needs. Additionally, be mindful of your operating system (Windows or Mac). If you are using Windows, your gold_dir variable should look like the second line. I have commented out that part with a hashtag/pound symbol. Uncomment it by deleting the first pound symbol.\n\ngold_dir &lt;- \"~/data/gold\"\n# gold_dir &lt;- \"C:/Users/utkuturk/data/gold\" # for windows\nextension &lt;- tools::file_ext(example_file)\nextension &lt;- paste0(\".\", extension)\nrest &lt;- tools::file_path_sans_ext(example_file)\nsubj &lt;- str_sub(rest, start = -4)\nnosubj &lt;- str_sub(rest, start = 1, end = -6)\nnew_name &lt;- paste0(subj, \"_\", nosubj, extension)\nnew_path &lt;- file.path(gold_dir, new_name)\nnew_path\n\n[1] \"~/data/gold/jtfr_squid_S.wav\"\n\n\nThis would be your original example file path.\n\nexample_file_path &lt;- file.path(gold_dir, example_file)\nexample_file_path\n\n[1] \"~/data/gold/squid_S_jtfr.wav\"\n\n\nAnd this line would handle the renaming from the old example_file_path to the new_path, thereby assigning the new name.\n\nfile.rename(example_file_path, new_path)"
  },
  {
    "objectID": "posts/mfa/index.html#for-loop",
    "href": "posts/mfa/index.html#for-loop",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "For loop",
    "text": "For loop\nIf you have ensured that the code above works correctly for you, you are now ready to implement the for loop. Within the loop, define a variable like f and use it instead of example_file. This way, you will iterate over every file in your list. To verify that it is functioning correctly, I also added a line to print a message each time a file is renamed.\n\ngold_dir &lt;- \"~/data/gold\"\n# gold_dir &lt;- \"C:/Users/utkuturk/data/gold\" # for windows\nfiles &lt;- list.files(gold_dir, pattern = \"\\\\.wav$|\\\\.TextGrid$\")\n\nfor (f in files) {\n  extension &lt;- tools::file_ext(f)\n  extension &lt;- paste0(\".\", extension)\n  rest &lt;- tools::file_path_sans_ext(f)\n  subj &lt;- str_sub(rest, start = -4)\n  nosubj &lt;- str_sub(rest, start = 1, end = -6)\n  new_name &lt;- paste0(subj, \"_\", nosubj, extension)\n  new_path &lt;- file.path(gold_dir, new_name)\n  file_path &lt;- file.path(gold_dir, f)\n  file.rename(file_path, new_path)\n  cat(\"Renamed\", f, \"to\", new_name, \"\\n\")\n}"
  },
  {
    "objectID": "posts/mfa/index.html#operators-used-in-this-section-2",
    "href": "posts/mfa/index.html#operators-used-in-this-section-2",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "Operators used in this section",
    "text": "Operators used in this section\n\ndf[selected_rows, indices_columns] or list[selected_element]\n[], Indexing operator: Accesses specific rows and/or columns of a data frame. If it is a list, it only takes a single argument to select an element. Remember in R indices start with 1, unlike python.\n\nselected_rows A vector of indices or names.\nselected_columns A vector of indices or names.\nselected_element A vector of indices or names.\n\nexample use: files[1]\n\n\n\n::\nDouble colon operator: Accesses functions and other objects from packages. Read x::y as ‚Äòfunction y from package x.‚Äô\nexample use: tools::file_ext()"
  },
  {
    "objectID": "posts/mfa/index.html#our-task-list-2",
    "href": "posts/mfa/index.html#our-task-list-2",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "Our Task List",
    "text": "Our Task List\n\nLoad the PCIbex results\nFilter irrelevant sound files\nMove all of our .wav and .TextGrid files to the same directory\nRename our files according to MFA guidelines\nRun MFA\nCreate a dataset"
  },
  {
    "objectID": "posts/mfa/index.html#moving-the-files-to-mfa-directory",
    "href": "posts/mfa/index.html#moving-the-files-to-mfa-directory",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "Moving the Files to MFA Directory",
    "text": "Moving the Files to MFA Directory\n\nWithout Dividing\nAgain, we are going to use the file.rename() and dir.create() functions to create the directory we are moving files to, and of course, to move files.\n\n# gold directory, where all of our files are\ngold_dir &lt;- \"~/data/gold\"\n# MFA directory\nmfa_dir &lt;- \"~/Documents/MFA/mycorpus\"\ndir.create(mfa_dir)\n\n# Files \nfiles &lt;- list.files(gold_dir, pattern = \"\\\\.wav$|\\\\.TextGrid$\")\nfor (f in files) {\n  old_file_path &lt;- file.path(gold_dir, f)\n  mfa_path &lt;- file.path(mfa_dir, f)\n  file.rename(old_file_path, mfa_path)\n  cat(\"Moved\", f, \"to\", mfa_dir, \"\\n\")\n}\n\n\n\nWith Dividing them into subfolders\nI will introduce the following function that I use. Here, I will not go into details, but it basically performs the following steps:\n\nCreates a subfolder called s1 and moves files into it.\nCounts up to 2000.\nWhen it surpasses 2000, it creates another subfolder by incrementing the number from s1 to s2.\nContinues this process until there are no more files.\n\n\ndivide_and_move &lt;- function(source, target, limit=2000) {\n  files &lt;- list.files(source, pattern = \"\\\\.wav$|\\\\.TextGrid$\", full.names = TRUE)\n  base_names &lt;- unique(tools::file_path_sans_ext(basename(files)))\n  s_index &lt;- 1\n  f_index &lt;- 0\n  s_path &lt;- file.path(target, paste0(\"s\", s_index))\n  dir.create(s_path)\n  \n  for (b in base_names) {\n    rel_files &lt;- files[grepl(paste0(\"^\", b, \"\\\\.\"), basename(files))]\n    \n    if (f_index + length(rel_files) &gt; limit) {\n      s_index &lt;- s_index + 1\n      s_path &lt;- file.path(target, paste0(\"s\", s_index))\n      dir.create(s_path)\n      f_index &lt;- 0\n    }\n    \n    for (f in rel_files) {\n      file.rename(f, file.path(s_path, basename(f)))\n    }\n    f_index &lt;- f_index + length(rel_files)\n  }\n}\n\nYou can use this function by simply providing the source and target folders.\n\n# gold directory, where all of our files are\ngold_dir &lt;- \"~/data/gold\"\n# MFA directory\nmfa_main_dir &lt;- \"~/Documents/MFA\"\ndir.create(mfa_dir)\ndivide_and_move(gold_dir, mfa_main_dir)"
  },
  {
    "objectID": "posts/mfa/index.html#terminal-codes",
    "href": "posts/mfa/index.html#terminal-codes",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "Terminal Codes",
    "text": "Terminal Codes\nAfter moving the files either with code or by hand to a specific MFA folder, ~/Documents/MFA, we can start running the terminal commands. At this point, I assume you have gone through the MFA documentation for installation instructions. I am also assuming that you have used a conda environment. If you haven‚Äôt, here are the 3 lines to install MFA.\n\n\n\nConda Installation in Terminal\n\nconda activate base\nconda install -c conda-forge mamba\nmamba create -n aligner -c conda-forge montreal-forced-aligner\n\n\nThere are again two ways to do this. One way is to open your Terminal app or use the Terminal tab in the R console below. The other way, which I prefer more, is to execute commands using the R function system(). I will first go over the easier one, which is using the Terminal app or the Terminal tab in R. But the reason I prefer the system() function is that I can loop over multiple folders more easily that way, and I do not have to run my commands again and again.\n\nUsing Terminal\nThe first command we want to run is the conda environment code. Following the MFA documentation, I renamed my environment to aligner. So, I start by activating that environment.\n\nconda activate aligner\n\nAfter activating the environment, I need to download three models: (i) an acoustic model to recognize phonemes given previous and following acoustic features, (ii) a dictionary to access pretrained phone-word mappings, and (iii) a g2p model to generate sequences of phones based on orthography. For all of these models, we are going to use the english_us_arpa model. You can visit this website to explore various languages and models.\n\nmfa model download acoustic english_us_arpa\nmfa model download dictionary english_us_arpa\nmfa model download g2p english_us_arpa\n\nAfter downloading these models, we are going to validate our corpus. There are many customizable parameters for this step. You can check them here. I am going to use my favorite settings here. You can interpret the following command like this: Dear Montreal Forced Aligner (mfa), can you please analyze my files located in ~/Documents/MFA/mycorpus and validate them using the english_us_arpa acoustic model and english_us_arpa dictionary? Please also consider that I have multiple speakers, indicated by the first 4 characters (-s 4). It would be great to use multiprocessing (--use_mp) for faster execution. Lastly, please clean up previous and new temporary files (--clean --final_clean).\n\nmfa validate -s 4 --use_mp --clean --final_clean ~/Documents/MFA/mycorpus english_us_arpa english_us_arpa\n\nThis process will take some time. Afterward, you will have some out of vocabulary words found in your TextGrids. You can easily create new pronunciations for them and add them to your model.\nThe mfa g2p command can take many arguments; here I am using only three. First, the path to the text file that has out of vocabulary words. This file is automatically created in your folder where your files are located. The path may vary depending on your system and folder naming, but the name of the .txt file will be the same. In my case, it is ~/Documents/MFA/mycorpus/oovs_found_english_us_arpa.txt. The second argument is the name of the g2p model. As you may recall, we downloaded it earlier, and its name is english_us_arpa. Finally, the third argument is the path to a target .txt file to store new pronunciations. I would like to store them in the same place, so I am using the following path: ~/Documents/MFA/mycorpus/g2pped_oovs.txt.\n\nmfa g2p ~/Documents/MFA/mycorpus/oovs_found_english_us_arpa.txt english_us_arpa ~/Documents/MFA/mycorpus/g2pped_oovs.txt\n\nAfter creating the pronunciations, you can add them to your model with mfa model add_words. This command takes the name of the dictionary as an argument (english_us_arpa) and the output of the mfa g2p command, which was a .txt file storing pronunciations: ~/Documents/MFA/mycorpus/g2pped_oovs.txt.\n\nmfa model add_words english_us_arpa ~/Documents/MFA/mycorpus/g2pped_oovs.txt\n\nThe last step is the alignment process. It will align (mfa align) the words and the phones inside our TextGrids stored in ~/Documents/MFA/mycorpus using our previously downloaded dictionary (english_us_arpa) and model (english_us_arpa), and store the newly aligned TextGrids in a new folder called ~/Documents/MFA/output.\n\nmfa align ~/Documents/MFA/mycorpus english_us_arpa english_us_arpa ~/Documents/MFA/output\n\n\n\nUsing R\nWe can also accomplish all of this in R. One advantage of this approach is that it allows us to iterate over multiple subfolders more easily, which can be useful if we have more than 2000 files. We will use four components:\n\nsystem() function to execute terminal commands,\npaste() function to create multiline templates,\n%s string placeholder to create template codes,\nsprintf() function to format our templates.\n\n\nIntroduction to sprintf() and %s\nBefore going further with MFA codes, let me illustrate with an example. Suppose we have a list of folder names, and we want to create a .txt file in each of these folders. We can use the system() function to perform this action. Below, I define my folder_list, then create paths for my .txt files in each folder, such as ~/data1/mydocument.txt. Afterwards, I generate a list of commands to create these files using touch, which is a command-line tool for creating files. Finally, I execute these commands using the system() function.\n\nfolder_list &lt;- c(\"~/data1\", \"~/data2\", \"~/data3\")\n\n\ntxt_list &lt;- paste(folder_list, \"mydocument.txt\", sep=\"/\")\ntxt_list\ncommand_list &lt;- paste(\"touch\", txt_list, sep=\" \")\ncommand_list\n\nfor (command in command_list) {\n  system(command)\n}\n\nTechnically, we didn‚Äôt need to use a for loop; instead, we could have concatenated all these commands with ; and run a single system command. Bash can execute multiple commands in a single line when separated by ;.\n\nconcatenated_commands &lt;- paste(command_list[1], \n                               command_list[2], \n                               command_list[3], \n                               sep=\";\") \n\nsystem(concatenated_commands)\n\nWe could achieve the same without needing a folder list by utilizing the %s placeholder and the sprintf() function.\n\ncommand_template &lt;- \"touch %s/mydocument.txt\"\nconcatenated_commands &lt;- paste(sprintf(command_template, \"~/data1\"), \n                               sprintf(command_template, \"~/data2\"),\n                               sprintf(command_template, \"~/data3\"),\n                               sep=\";\")\n\nsystem(concatenated_commands)\n\nThis approach becomes particularly useful when dealing with multiple placeholders within the same command. For instance, the command template will replace the first %s with the first argument, such as ~/data1, and the second %s with the second argument, like mydoc1, when formatted using sprintf().\n\ncommand_template &lt;- \"touch %s/%s.txt\"\nconcatenated_commands &lt;- paste(sprintf(command_template, \"~/data1\", \"mydoc1\"), \n                               sprintf(command_template, \"~/data2\", \"mydoc2\"),\n                               sprintf(command_template, \"~/data3\", \"mydoc3\"),\n                               sep=\";\")\n\nsystem(concatenated_commands)\n\n\n\nRunning MFA in R\nNext, we‚Äôll consolidate the previous code by concatenating it using paste() and separating commands with ;. If needed, we‚Äôll incorporate placeholders. Each line will be assigned to a new variable, and then they‚Äôll be combined into a single command string using paste(). Finally, we‚Äôll execute the command string using system() with the argument intern = TRUE to capture the output into an R variable, which allows for later inspection.\n\nconda_start &lt;- \"conda activate aligner\"\nget_ac &lt;- \"mfa model download acoustic english_us_arpa\"\nget_dic &lt;- \"mfa model download dictionary english_us_arpa\"\nget_g2p &lt;- \"mfa model download g2p english_us_arpa\"\n\nmfa_init &lt;- paste(conda_start, get_ac, get_dic, get_g2p, sep = \";\")\n\nmfa_init_output &lt;- system(mfa_init, intern = TRUE)\n\nAfter initializing the model, the next step involves validation. Again, I‚Äôll use the same approach and concatenate the commands together. However, sometimes we may have too many files and need to use subfolders. To accommodate this, I‚Äôll use %s placeholders. The validation command has one placeholder for different subfolders. Similarly, our pronunciation creation for g2p has two placeholders, though they‚Äôll be filled with the same value. Lastly, the add_words command will use a single placeholder. Fortunately, all these folders are the same, so we can reuse the same variable repeatedly.\n\nconda_start &lt;- \"conda activate aligner\"\n\nvalidate &lt;- \"mfa validate -s 4 --use_mp --clean --final_clean ~/Documents/MFA/%s english_us_arpa english_us_arpa\"\ng2p_words &lt;- \"mfa g2p ~/Documents/MFA/%s/oovs_found_english_us_arpa.txt english_us_arpa ~/Documents/MFA/%s/g2pped_oovs.txt\"\nadd_words &lt;- \"mfa model add_words english_us_arpa ~/Documents/MFA/%s/g2pped_oovs.txt\"\n\nmfa_val &lt;- paste(conda_start, validate, g2p_words, add_words, sep = \";\")\n\nSince this step takes longer and there‚Äôs more room for errors, I want to save all my outputs in a list. First, I need to identify which folders exist in my MFA directory. Because my divide_and_move function prefixes every subfolder with s, I‚Äôll use ^s to filter for relevant folders.\n\noutput_val &lt;- list()\n\n# Define the base path where folders are located\nbase_path &lt;- \"~/Documents/MFA\"\nfolders &lt;- list.dirs(base_path, recursive = FALSE, full.names = FALSE)\nfolders &lt;- folders[str_detect(folders, \"^s\")]\n\nfolders\n\nNow, we can iterate over this list of folders using a for loop. First, we create a temporary script using sprintf() with four placeholders. Next, we execute the current script and save the output in a temp_output variable. Later, we assign this output to specific output_name variables for each folder using paste0() and assign() functions.\n\nfor (f in folders) {\n  cur_mfa_val &lt;- sprintf(mfa_val, f, f, f, f)\n  \n  temp_output &lt;- system(cur_mfa_val, intern = TRUE)\n  \n  output_name &lt;- paste0(\"output_val_\", f)\n  \n  assign(output_name, temp_output, envir = .GlobalEnv)\n}\n\nNow you can check the outputs by calling specific variables like output_val_s1 or output_val_s2. After this step, the only task remaining is to run the aligner. We will create a template again, iterate over folders, and assign outputs to their respective names for verification. Meanwhile, the bash code will execute in the background. This time, our placeholders will refer to different inputs and an output folder. Fortunately, we can use the same output folder for every subfolder, so instead of using two placeholders, we‚Äôll use a single %s placeholder.\n\nconda_start &lt;- \"conda activate aligner\"\n\nalign &lt;- \"mfa align ~/Documents/MFA/%s english_us_arpa english_us_arpa ~/Documents/MFA/output\"\n\nmfa_align &lt;- paste(conda_start, align, sep = \";\")\n\nfor (f in folders) {\n  cur_mfa_align &lt;- sprintf(mfa_align, f)\n  temp_output &lt;- system(cur_mfa_align, intern=TRUE)\n  output_name &lt;- paste0(\"output_align_\", f)\n  assign(output_name, temp_output, envir = .GlobalEnv)\n}\n\nThis for loop completes the MFA alignment. There is one final task remaining: creating a dataframe for further data analysis."
  },
  {
    "objectID": "posts/mfa/index.html#our-task-list-3",
    "href": "posts/mfa/index.html#our-task-list-3",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "Our Task List",
    "text": "Our Task List\n\nLoad the PCIbex results\nFilter irrelevant sound files\nMove all of our .wav and .TextGrid files to the same directory\nRename our files according to MFA guidelines\nRun MFA\nCreate a dataframe"
  },
  {
    "objectID": "posts/mfa/index.html#one-file-example-1",
    "href": "posts/mfa/index.html#one-file-example-1",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "One-file example",
    "text": "One-file example\nAgain, let‚Äôs work with an example file from our list, starting with the first file [1]. First, we‚Äôll retrieve its full file path. Then, we‚Äôll use the read_textgrid() function to create a dataframe for this single file. I‚Äôll print the structure of the dataframe to give you a clearer view of its contents.\n\nexample_file &lt;- file_list[1]\nfile_path &lt;- file.path(tg_dir, example_file)\nexample_df &lt;- readtextgrid::read_textgrid(file_path)\nstr(example_df)\n\nIn this project, which involves aligning words, we are interested in only a couple of these columns. Specifically, we focus on the file identifier (file) to determine the trial from which the data originates, the tier name (tier_name) to differentiate between word and phone tiers, the start (xmin) and end (xmax) of each interval, and finally, the text. Additionally, I am not interested in retaining the file extension in the file identifier. Therefore, we will first filter to include only annotated words, then select the important columns using select(), remove the .TextGrid extension, and concatenate the words so that we can see the full response for each trial.\n\nexample_df &lt;- example_df |&gt; \n  # Filter annotated \"words\" tier\n  filter(tier_name == \"words\" & text != \"\") |&gt; \n  # Select relevant columns\n  select(file, xmin, xmax, text, annotation_num) |&gt; \n  # Remove .TextGrid and put the response together\n  mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n         response = paste(text, collapse = \" \"))\n\nexample_df\n\nWe also need some information about the trial. Luckily, all of our information is provided in our file name. So, I am going to parse that name to create a dataframe with more information. I am using a set of function that all start with separate_wider_.\n\nThe delim version uses a deliminator to split a row of a dataframe.\nThe regex version uses regular expressions to split the data.\nFinally, the position version uses the number of characters to split the data.\n\nI am doing all of this because of how I initially coded my experiment output in my PCIbex script. You may need to change this code to process your own data.\n\nexample_df &lt;- example_df |&gt;\n  # split the `file` column into 5 different columns.\n  separate_wider_delim(file, \"_\", names = c(\"subj\", \"exp\", \"headVerb\", \"NumNum\", \"sem_type\"), cols_remove = F) |&gt;\n  # split the headVerb column from the \"U\" character\n  separate_wider_regex(headVerb, c(head = \".*\", \"U\", verb_type = \".*\")) |&gt;\n  # Add the \"U\" character back to \"Unacc\" and \"Unerg\"s\n  mutate(verb_type = paste0(\"U\", verb_type)) |&gt;\n  # split the head and distractor numbers.\n  separate_wider_position(NumNum, c(head_num = 2, dist_num = 2))\n\nexample_df"
  },
  {
    "objectID": "posts/mfa/index.html#another-treat-for-you",
    "href": "posts/mfa/index.html#another-treat-for-you",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "Another Treat for you",
    "text": "Another Treat for you\nLet‚Äôs see what you will need to do. You will find your file in the MFA/output folder as well, and your file will look like jtfr_squid_S.TextGrid. Let‚Äôs arbitrarily put them here. Remember, you will have to use the file.list() function as well. You will not need to change anything in the first part where we work on the TextGrid. The necessary changes will need to be done in the parsing procedure. Instead of using the entire regex or position methods, you will just need to use the delim version of the function.\n\n# Define the directory and list files\ntg_dir &lt;- \"~/Documents/MFA/output\"\nyour_file &lt;- \"jtfr_squid_S.TextGrid\"\n\nfile_path &lt;- file.path(tg_dir, your_file)\n\n\n### LETS SAY YOU RAN read_textgrid function.\n### I commented out this part, because I do not have your data.\n### Final dataframe will be slightly different here, because I do not have the textgrid data here.\n# your_df &lt;- readtextgrid::read_textgrid(path = file_path) |&gt;\n#     filter(tier_name == \"words\" & text != \"\") |&gt;\n#     select(file, xmin, xmax, text, annotation_num) |&gt;\n#     mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n#            response = paste(text, collapse = \" \"))\n#   \n\nyour_df &lt;- your_df |&gt;\n  separate_wider_delim(file, \"_\", names = c(\"subj\", \"head\", \"condition\"), cols_remove = F) \n\nyour_df\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubj\nhead\ncondition\nfile\nxmin\nxmax\ntext\nannotation_num\nresponse\n\n\n\n\njtfr\nsquid\nS\njtfr_squid_S\n\n\nsquid\n1\nsquid jumped over the fence\n\n\njtfr\nsquid\nS\njtfr_squid_S\n\n\njumped\n2\nsquid jumped over the fence\n\n\njtfr\nsquid\nS\njtfr_squid_S\n\n\nover\n3\nsquid jumped over the fence\n\n\njtfr\nsquid\nS\njtfr_squid_S\n\n\nthe\n4\nsquid jumped over the fence\n\n\njtfr\nsquid\nS\njtfr_squid_S\n\n\nfence\n5\nsquid jumped over the fence"
  },
  {
    "objectID": "posts/mfa/index.html#for-loop-1",
    "href": "posts/mfa/index.html#for-loop-1",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "For-loop",
    "text": "For-loop\nNow, we need to apply this process to all files in our output directory. To simplify this, I‚Äôll start by creating a function for processing each file individually and then apply it to all files. The function takes a file name and its directory as inputs and returns a dataframe. Before creating each dataframe, it prints ‚ÄúReading the file.‚Äù to indicate progress.\n\nprocess_textgrid &lt;- function(file, directory) {\n  cat(\"Reading\", file, \"\\n\")\n  file_path &lt;- file.path(directory, file)\n  df &lt;- readtextgrid::read_textgrid(path = file_path) |&gt;\n    filter(tier_name == \"words\" & text != \"\") |&gt;\n    select(file, xmin, xmax, text, annotation_num) |&gt;\n    mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n           response = paste(text, collapse = \" \"))\n  \n  df &lt;- df |&gt;\n    separate_wider_delim(file, \"_\", names = c(\"subj\", \"exp\", \"headVerb\", \"NumNum\", \"sem_type\"), cols_remove = F) |&gt;\n    separate_wider_regex(headVerb, c(head = \".*\", \"U\", verb_type = \".*\")) |&gt;\n    mutate(verb_type = paste0(\"U\", verb_type)) |&gt;\n    separate_wider_position(NumNum, c(head_num = 2, dist_num = 2))\n  \n  return(df)\n}\n\nThis is essentially the same process as before, but encapsulated within a function for easier application. Here‚Äôs an example of how I‚Äôm redefining my directory and file list:\n\n# Define the directory and list files\ntg_dir &lt;- \"~/Documents/MFA/output\"\nfile_list &lt;- list.files(path = tg_dir, pattern = \"\\\\.TextGrid$\")\nexample_file &lt;- file_list[1]\nprocess_textgrid(example_file, tg_dir)\n\nYour version will look like this.\n\nprocess_textgrid &lt;- function(file, directory) {\n  cat(\"Reading\", file, \"\\n\")\n  file_path &lt;- file.path(directory, file)\n  df &lt;- readtextgrid::read_textgrid(path = file_path) |&gt;\n    filter(tier_name == \"words\" & text != \"\") |&gt;\n    select(file, xmin, xmax, text, annotation_num) |&gt;\n    mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n           response = paste(text, collapse = \" \"))\n  \n  df &lt;- df |&gt;\n    separate_wider_delim(file, \"_\", names = c(\"subj\", \"head\", \"condition\"), cols_remove = F) \n  \n  return(df)\n}\n\n\n# Define the directory and list files\ntg_dir &lt;- \"~/Documents/MFA/output\"\nfile_list &lt;- list.files(path = tg_dir, pattern = \"\\\\.TextGrid$\")\nexample_file &lt;- file_list[1]\nprocess_textgrid(example_file, tg_dir)\n\nNow, we need to integrate this function into our for-loop. Instead of using a single file like file_list[1], we will apply it to an entire directory. Unlike previous for loops, we will use the map function from the purrr package. It is faster and easier to use in cases like this. map() will return all of our dataframes embedded in a list. After using map(), we need to combine all these smaller dataframes into a larger one using bind_rows.\n\n# Define the directory and list files\ntg_dir &lt;- \"~/Documents/MFA/output\"\nfile_list &lt;- list.files(path = tg_dir, pattern = \"\\\\.TextGrid$\")\n# Run our function, I am using mine, you should use your own.\nprocess_textgrid &lt;- function(file, directory) {\n  cat(\"Reading\", file, \"\\n\")\n  file_path &lt;- file.path(directory, file)\n  df &lt;- readtextgrid::read_textgrid(path = file_path) |&gt;\n    filter(tier_name == \"words\" & text != \"\") |&gt;\n    select(file, xmin, xmax, text, annotation_num) |&gt;\n    mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n           response = paste(text, collapse = \" \"))\n  \n  df &lt;- df |&gt;\n    separate_wider_delim(file, \"_\", names = c(\"subj\", \"exp\", \"headVerb\", \"NumNum\", \"sem_type\"), cols_remove = F) |&gt;\n    separate_wider_regex(headVerb, c(head = \".*\", \"U\", verb_type = \".*\")) |&gt;\n    mutate(verb_type = paste0(\"U\", verb_type)) |&gt;\n    separate_wider_position(NumNum, c(head_num = 2, dist_num = 2))\n  \n  return(df)\n}\n\ndfs &lt;- map(file_list, process_textgrid, directory = tg_dir)\n\nfinal_df &lt;- bind_rows(dfs)\n\nThis completes our MFA Aligning work. We have successfully completed every task on our list, aligned our data, and created a dataframe for analysis. You can check the structure of our final dataframe, the number of rows, and the count of unique trials.\n\nstr(final_df)\n\nnrow(final_df)\n\nlength(unique(final_df$file))"
  },
  {
    "objectID": "posts/mfa/index.html#footnotes",
    "href": "posts/mfa/index.html#footnotes",
    "title": "montreal forced aligner workflow for pcibex production experiments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOn principle, I am against for loops in R, but it is better to use here instead of confusing you more.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/server-recordings/index.html",
    "href": "posts/server-recordings/index.html",
    "title": "automating experimental recording management with bash",
    "section": "",
    "text": "I realized that most of my experimental time was being eaten up not by designing or running studies, but by the tedious task of downloading recordings from the server, unzipping them, converting formats, and filing them into the right participant folders. After a while, the frustration of repeating these steps by hand turned into procrastination and that procrastination produced some bash scripts to automate the whole process."
  },
  {
    "objectID": "posts/server-recordings/index.html#step-1-download-recordings",
    "href": "posts/server-recordings/index.html#step-1-download-recordings",
    "title": "automating experimental recording management with bash",
    "section": "Step 1: Download recordings",
    "text": "Step 1: Download recordings\nFirst I grab all the zipped uploads from the server and drop them into a local directory.\n\nremote_host=\"myserver@myserver.umd.edu\"\nremote_path=\"/Users/myserver/Phillips/Utku/corner_same_verb/uploads/*.zip\"\nlocal_path=\"~/Downloads/rec_feb23\"\nmkdir \"$local_path\"\n\nscp \"$remote_host:$remote_path\" \"$local_path\""
  },
  {
    "objectID": "posts/server-recordings/index.html#step-2-unzip-and-convert-formats",
    "href": "posts/server-recordings/index.html#step-2-unzip-and-convert-formats",
    "title": "automating experimental recording management with bash",
    "section": "Step 2: Unzip and convert formats",
    "text": "Step 2: Unzip and convert formats\nOnce the .zip files are on my machine, I unzip everything and convert the .webm files to .wav with ffmpeg. The originals go into a backup folder.\ncd \"$local_path\"\nunzip \\*.zip\nfor i in *.webm; do ffmpeg -i \"$i\" \"${i%.*}.wav\"; done\nmkdir backup\nmv \\*.zip ,/backup\nrm \\*.webm"
  },
  {
    "objectID": "posts/server-recordings/index.html#step-3-group-files-by-participant",
    "href": "posts/server-recordings/index.html#step-3-group-files-by-participant",
    "title": "automating experimental recording management with bash",
    "section": "Step 3: Group files by participant",
    "text": "Step 3: Group files by participant\nMy participant IDs are randomly generated 8-character strings, thus {file:0:8} and ^.{8}_. I use the first eight characters of the filename as a prefix to create a directory per participant.\nfor file in *; do\n  if [[ -f \"$file\" ]]; then\n    prefix=\"${file:0:8}\"\n    if [[ \"$file\" =~ ^.{8}_ ]]; then\n      if [[ ! -d \"$prefix\" ]]; then\n        mkdir \"$prefix\"\n      fi\n      mv \"$file\" \"$prefix/\"\n    fi\n  fi\ndone"
  },
  {
    "objectID": "posts/server-recordings/index.html#step-4-sort-within-each-participant-folder",
    "href": "posts/server-recordings/index.html#step-4-sort-within-each-participant-folder",
    "title": "automating experimental recording management with bash",
    "section": "Step 4: Sort within each participant folder",
    "text": "Step 4: Sort within each participant folder\nFinally, within each participant‚Äôs folder, I move the recordings into two buckets: - fam/ for habituation (familiarization) files (those with fam), - misc/ for practice, intro, and test files.\nI use nullglob to avoid errors if a folder doesn‚Äôt contain a certain file type.\nfor prefix_dir in */; do\n  prefix_dir=\"${prefix_dir%/}\" \n  if [[ -d \"$prefix_dir\" ]]; then\n\n    if [[ ! -d \"$prefix_dir/fam\" ]]; then\n      mkdir \"$prefix_dir/fam\"\n    fi\n    setopt nullglob\n    for fam_file in \"$prefix_dir/\"*_*fam_*; do\n      if [[ -f \"$fam_file\" ]]; then\n        if [[ \"$fam_file\" =~ .*_fam_.* ]]; then\n          mv \"$fam_file\" \"$prefix_dir/fam/\"\n        fi\n      fi\n    done\n    unsetopt nullglob\n\n    if [[ ! -d \"$prefix_dir/misc\" ]]; then\n      mkdir \"$prefix_dir/misc\"\n    fi\n    setopt nullglob\n    for misc_file in \"$prefix_dir/\"*_*practice_* \"$prefix_dir/\"*_*intro_* \"$prefix_dir/\"*_*test-*; do\n      if [[ -f \"$misc_file\" ]]; then\n        if [[ \"$misc_file\" =~ .*_practice_.* || \"$misc_file\" =~ .*_intro_.* || \"$misc_file\" =~ .*_test-.* ]]; then\n          if [[ ! \"$misc_file\" =~ .*_fam_.* ]]; then \n              mv \"$misc_file\" \"$prefix_dir/misc/\"\n          fi\n        fi\n      fi\n    done\n    unsetopt nullglob\n  fi\ndone\nWhat started as procrastination ended up saving me hours of repetitive work. Probably, it is not a good code, but it turns file management into a background task and leaves more time to me for the actual science. If you have any comments how to make the code better, please reach out!"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "utku turk",
    "section": "",
    "text": "cats of ‚Äô18 summer & bit espresso bar, photo by me"
  },
  {
    "objectID": "research.html#experimental-work",
    "href": "research.html#experimental-work",
    "title": "utku turk",
    "section": "Experimental Work",
    "text": "Experimental Work\n\nSentence PlanningSentence ProcessingTreebanking\n\n\nMy primary focus at the moment is on the flexibility that speakers show when planning sentences. I am currently working on to understand how morphological planning is carried out and whether agents have special status in planning. My goal is to eventually tie this to the work focusing on exploring generative grammars and how they can explain the planning procedure. Broadly, I‚Äôm interested in online sentence building and its interaction with syntactic structure.\nSelected output\nT√ºrk, U., Phillips, C. (2024). Speech timing evidence on the (in)dependence of root and inflection access in production. Poster presented at Human Sentence Processing 2024. [abstract]\nDods, A., MacDonalds, A., T√ºrk, U., Mancha, S., Phillips, C. (2024). Is the octopus regenerating?: Comparing timing effects in sentence recall and picture description tasks. Poster presented at Human Sentence Processing 2024. [abstract]\n\n\nI am interested in people‚Äôs degree of fallibility when it comes to comprehending number agreement. I am currently working on to reconcile conflicting findings on whether attraction effects are modulated by case ambiguity in sentence comprehension. My goal is to eventually tie this to the work focusing on exploring how people access memory and why we need to recontruct them using either abstract or form-related cues. Broadly, I‚Äôm interested in online addressable memory-building, its interaction with syntactic dependencies in typologically different languages, and the nature of the mechanisms and representations this interaction involves.\nWith Pavel Logacev, I focused on whether people‚Äôs failure to notice ungrammatical sentences was due to more cognitive-general issues such as, response bias, shallow processing, or task-effects. Through series of behavioral experiments, we learned that even though that they affect to what degree people fail to assess grammaticality of a sentence, these cognitive-general issues cannot be sole explanation of failure to detect ungrammaticality.\nSelected output\nT√ºrk, U. (2022). Agreement Attraction in Turkish. [local pdf]. [repo]. [summary]\nT√ºrk, U., Logacev, P. (2024). Agreement Attraction in Turkish: The case of genitive attractors. Language, Cognition, and Neuroscience. DOI: 10.1080/23273798.2024.2324766\nT√ºrk, U., Logacev, P. (in prep). Response Bias in Turkish Agreement Attraction. [repo] (available upon request)\n\n\nAs a part of two projects (funded by TUBITAK and European Commission), I partake in creation and re-annotation of multiple Universal Dependencies Treebanks in multiple languages. My current aim is to create a set of treebanks for un(der)represented languages in Asia Minor.\nWith Arzucan Ozgur, Tunga Gungor, and Balkiz Ozturk, we created new guidelines for annotating Turkish data within Universal Dependencies framework. Following these guidelines, we re-annotated two already existing treebanks, and created a new treebank.\nAs a part of my efforts to document minority languages, we also created the first Laz treebank, using data from published linguistic papers and theses. I am currently working on Ladino and Cappadocian Greek (with Konstantinos Sampanis) treebanks. I am extremely open to collaborate and willing to work on any minority language documentation/treebanking effort! Do not hesitate to reach out!\nSelected output\nT√ºrk, U., Atmaca, F., √ñzate≈ü, ≈û.B. et al.¬†(2022). Resources for Turkish dependency parsing: introducing the BOUN Treebank and the BoAT annotation tool. Language Resources & Evaluation 56, 259‚Äì307. DOI: 10.1007/s10579-021-09558-0. [resources]\nT√ºrk, U., Bayar, K., √ñzercan, A. D., √ñzt√ºrk, G. Y., √ñzate≈ü, S. B. (2019). First Steps towards Universal Dependencies for Laz. Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020).\nT√ºrk, U., Atmaca, F., √ñzate≈ü, S. B., Bedir, S. T., K√∂ksal, A., √ñzt√ºrk B., G√ºng√∂r, T., √ñzg√ºr, A. (2019). Turkish Treebanking: Unifying and Constructing Efforts. Proceedings of the 13th Linguistic Annotation Workshop.\nT√ºrk, U., Atmaca, F., √ñzate≈ü, S. B., Bedir, S. T., K√∂ksal, A., √ñzt√ºrk B., G√ºng√∂r, T., √ñzg√ºr, A. (2019). Improving the Annotations in the Turkish Universal Dependency Treebank. Proceedings of the Third Workshop on Universal Dependencies (UDW, SyntaxFest 2019)."
  },
  {
    "objectID": "research.html#theoretical-work",
    "href": "research.html#theoretical-work",
    "title": "utku turk",
    "section": "Theoretical Work",
    "text": "Theoretical Work\n\nMorpho-syntaxSemantics\n\n\nI was lucky to join Pavel Caha at Masaryk University to work on various topics including Turkish case syncretism, augmentatives, and suspended affixation.\nOne of the most exciting work I did was on Turkish adjectivals. I formalized the degree-sensitivity of Turkish Evaluative morphology, specifically ‚Äúdiminutives‚Äù. Adjectival diminutives, in Turkish, only attach to negative-ordered adjectives. I also showed that Turkish -CIK is actually a complex formative unlike previously believed. While -K is an inherent part of the adjective, -CI is the amplifier that attaches to defective adjective ‚Äúroot‚Äù that is stripped from -K.\nI also tried to formalize differential object marking in Turkish. Using nanosyntactic algorithm, I modeled the nominal case paradigm of Turkish.\nLastly, I argued that candidates in morphology can be re-ranked via phonological information. Words with root suppletion generally are not licensed in suspended affixation. However, preceding only a vowel-harmonic conjunction, suppletion was reverted and suspended affixation was licensed.\nSelected output\nT√ºrk, U., Caha, P. (2021). Nanosyntactic Analysis of Turkish Case System. Proceedings of the 6th Workshop on Turkic and languages in contact with Turkic. DOI: 10.3765/ptu.v6i1.5051\nT√ºrk, U. (2020). Tackling the Augmentative Puzzle in Turkish. Proceedings of the 5th Workshop on Turkic and languages in contact with Turkic. DOI: 10.3765/ptu.v5i1.4771\n\n\nWith Omer Demirok, we investigated how hypothetical comparions are formed in Turkish. -mƒ±≈ücasƒ±na is licensed with gradable adjectives but not with non-gradable ones. We took this to suggest that its semantics involves comparison of degrees. We pursued a previously uncharted route: HCM constructions in Turkish compare degrees, not eventualities. Our analysis is sensitive to the semantic difference between OPEN and CLOSE scale adjectives that are independently justified.\nI am currently interested in how pragmatic reality is structured in natural language such as personal experiences, interpretation of indefinite nouns, and justified beliefs. To investigate these topics, I use data from de re/de dicto and acquaintance inference literature.\nSelected output\nT√ºrk, U., Demirok, √ñ. (2021). Hypothetical Comparison in Turkish. Proceedings of the 6th Workshop on Turkic and languages in contact with Turkic. DOI: 10.3765/ptu.v6i1.5054"
  },
  {
    "objectID": "research.html#computing-stack",
    "href": "research.html#computing-stack",
    "title": "utku turk",
    "section": "Computing Stack",
    "text": "Computing Stack\n\nR ‚Äî brms, ggplot2, dplyr, cmdstanr, purrr\nPython ‚Äî PyMC3, pyro, seaborn, pandas\nScientific reporting ‚Äî LaTeX, Rmarkdown, Quarto, knitr\nWeb development ‚Äî HTML/CSS/JS, D3, dc.js, crossfilter\nExperiment deployment ‚Äî IbexFarm, PcIbex, psychopy, jsPsych, opensesame\nLinux ‚Äî Git, bash/zsh, torque\nContainers ‚Äî docker, renv."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "utku turk",
    "section": "",
    "text": "[Summer 24]Workshop on Sentence Production and Employing Production ExperimentsWorkshop instructor, University of OxfordCo-organized with Allison Dods, Rosa Lee, Colin Phillips"
  },
  {
    "objectID": "teaching.html#workshops",
    "href": "teaching.html#workshops",
    "title": "utku turk",
    "section": "",
    "text": "[Summer 24]Workshop on Sentence Production and Employing Production ExperimentsWorkshop instructor, University of OxfordCo-organized with Allison Dods, Rosa Lee, Colin Phillips"
  },
  {
    "objectID": "teaching.html#invited-lectures",
    "href": "teaching.html#invited-lectures",
    "title": "utku turk",
    "section": "Invited Lectures",
    "text": "Invited Lectures\n\n[Fall 23]Agreement Attraction and GrammarLING 240: Language and Mind ‚Äî with Tonia BleamUniversity of Maryland\n\n\n[Fall 23]Production System and GrammarLING 440: Grammar and Cognition ‚Äî with Ellen LauUniversity of Maryland\n\n\n[Spring 19]Dimunitives and Nanosyntax in TurkishLING 202: Morphology ‚Äî with Aslƒ± G√∂kselBogazici University"
  },
  {
    "objectID": "teaching.html#teaching-assistant",
    "href": "teaching.html#teaching-assistant",
    "title": "utku turk",
    "section": "Teaching Assistant",
    "text": "Teaching Assistant\n\nUniversity of Maryland\n\n[Fall 24]LING 640: Psycholinguistics ‚Äî with Ellen Lau10 students, Linguistics Dept, University of MarylandInstructed 2 weeks on experimental methods and statistical analysis\n\n\n[Spring 25]LING 200: Introduction to Linguistics ‚Äî with Tonia Bleam25 students, Linguistics Dept, University of Maryland\n\n\n[Fall 24]LING 440: Cognition and Grammar ‚Äî with Ellen Lau25 students, Linguistics Dept, University of Maryland\n\n\n[Spring 24]ARHU 299: Machine Learning in Language and Art ‚Äî with Omar Agha~15 students, College of Arts and Humanities, University of Maryland[syllabus]\n\n\n[Fall 23]LING 240: Language and Mind ‚Äî with Tonia Bleam~20 students, Linguistics Dept, University of Maryland[syllabus]\n\n\n\nBoƒüazi√ßi University\n\n[Spring 22]Ling 360: Computational Methods in Linguistics ‚Äî with √úmit Atlamaz~40 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Spring 22]Ling 314: Syntax & Semantics of Modern Turkish ‚Äî with √úmit Atlamaz~40 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Fall 21]Ling 101: Introduction to Linguistics I ‚Äî with Mine Nakipoƒülu~80 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Fall 21]Ling 203: Syntax ‚Äî with Balkƒ±z √ñzt√ºrk~80 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Spring 21]Ling 101: Introduction to Linguistics I ‚Äî with Pavel Logacev~125 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Spring 21]Ling 314: Syntax & Semantics of Modern Turkish ‚Äî with Balkƒ±z √ñzt√ºrk~90 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Spring 21]Ling 484: Computational Methods in Linguistics ‚Äî with √úmit Atlamaz~50 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Fall 20]Ling 101: Introduction to Linguistics I ‚Äî with √ñmer Demirok~100 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Fall 20]Ling 203: Syntax ‚Äî with Balkƒ±z √ñzt√ºrk~50 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Fall 20]Ling 313: Phonology & Morphology of Modern Turkish ‚Äî with Metin Baƒürƒ±a√ßƒ±k~160 students, Linguistics Dept, Bogazici University[syllabus]shared courseload with Furkan Dikmen & Muhammed ƒ∞leri\n\n\n[Fall 20]Ling 411: Linguistic Methodology ‚Äî with Pavel Logacev~40 students, Linguistics Dept, Bogazici University[syllabus]contributed to Class Notes (github) as well as holding PS sessions for data wrangling and visualization in R\n\n\n[Spring 20]Ling 202: Morphology ‚Äî with Aslƒ± G√∂ksel~100 students, Linguistics Dept, Bogazici University[syllabus]shared the workload with Furkan Dikmen\n\n\n[Spring 20]Ling 206: Language Typology ‚Äî with Metin Baƒürƒ±a√ßƒ±k~40 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Fall 19]Ling 101: Introduction to Linguistics I ‚Äî with Meltem Kelepir~100 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Fall 19]Ling 313: Phonology & Morphology of Modern Turkish ‚Äî with Aslƒ± G√∂ksel~100 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Spring 19]Ling 101: Introduction to Linguistics I ‚Äî with Elena Guerzoni~70 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Spring 19]Ling 314: Syntax & Semantics of Modern Turkish ‚Äî with Balkƒ±z √ñzt√ºrk~100 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Fall 18]Ling 313: Phonology & Morphology of Modern Turkish ‚Äî with Kadir G√∂kg√∂z~100 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Spring 18]Ling 101: Introduction to Linguistics I ‚Äî with Pavel Logacev~100 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Spring 18]Ling 102: Introduction to Linguistics II ‚Äî with Mine Nakipoƒülu~60 students, Linguistics Dept, Bogazici University[syllabus]\n\n\n[Summer 18]S30: Advanced Turkish10 students, Turkish Language and Culture Program, Bogazici University\n\n\n[Summer 19]S25: Upper Indermediate Turkish16 students, Turkish Language and Culture Program, Bogazici University"
  },
  {
    "objectID": "posts/j-glide/index.html",
    "href": "posts/j-glide/index.html",
    "title": "is /j/ really a sonorant in turkish? i guess not.",
    "section": "",
    "text": "This work started as a term paper for our master‚Äôs phonology class with Semra Ozdemir. She was interested in the properties of sonorants and I was interested in the phonetics of them. We saw that in many environments Turkish glide /j/ acts differently than other sonorants. The fact that /j/ was the only phonemic glide in Turkish was already interesting by itself. We asked the following question: Is it really a glide?\nWe looked at certain phonological behaviors of the /j/: (i) /h/ deletion prior to sonorants, (ii) consonant-clusters in coda, and (iii) [e]-lowering prior to sonorants. We also conducted a phonetics experiment where we asked people to read sentences that contains sonorants in word-final, word-initial, and intervocalic positions following the sound [e]. Our results from this experiment can be seen in the figure below.\n\n\n\nFigure 1. F1 and F2 values of individual utterances of the mid-front vowel e.We grouped these instances into two: following /j/ or following a non-/j/ sound.\n\n\nWe saw that /j/ behaves nothing like a sonorant. When the sound /j/ follows the mid-front vowel [e], it did not lower the vowel. However, every other sonorant (non-/j/) substantially lowered the vowel. Even though, it is clear that /j/ does not act like a sonorant, we were indecisive whether it were unspecified with respect to its sonoracy or it was just a fricative.\nLater on, Stefano Canalis and Umit Tuncer also joined us in this project. With their help, we decided that /j/ is a phonemically fricative in Turkish, but may show sonorant-like behavior in certain environments. With them, we looked at the phonemic behavior of /j/ in more environments. We also investigated the feature symmetry in Turkish.\nWe saw that other phonetic sonorants in Turkish were also phonemically fricative and /j/ and other sonorants become sonorant-like in the very same environments."
  },
  {
    "objectID": "posts/misunderstanding-kate/index.html",
    "href": "posts/misunderstanding-kate/index.html",
    "title": "misunderstanding Kate‚Äôs talk on nonuniformity of phonology and phonetics",
    "section": "",
    "text": "This post is part of a series I‚Äôm calling Misunderstandings. The idea is simple: I pick up on talks, papers, or arguments that I admire but also don‚Äôt quite buy in the way they‚Äôre framed. Sometimes I pose counterexamples, sometimes I just get tangled in the details. The label isn‚Äôt meant as dismissive‚Äîusually the ‚Äúmisunderstanding‚Äù is mine‚Äîbut I find that pushing on these points is a good way to clarify what‚Äôs at stake.\nKate Mooney‚Äôs recent talk at UMass, Segmental phonology, gestural phonetics: Explaining asymmetries between phonological and phonetic operations, was about something deceptively simple: are phonological processes uniform?\nThe traditional view has been ‚Äúyes.‚Äù If an alternation applies freely, that‚Äôs phonology. If it‚Äôs tied to a particular morpheme, that‚Äôs phonology too, just gated by a diacritic, indexed constraint, or cyclic ranking. The assumption is that the underlying machinery is the same; what differs is timing and scope.\nKate pushed back. She focused on vowel harmony versus vowel co-articulation. Zsiga had already argued these belong to different representational domains‚Äîrules vs gestures‚Äîbut Kate sharpened the contrast. Vowel harmony: categorical, typologically robust, governed by rules. Vowel coarticulation: gradient, gestural blending, and crucially never morphologically restricted. That asymmetry, she argued, is real. If you try to collapse the two into one ‚Äúuniform phonology,‚Äù you erase those typological gaps.\nFigure¬†1: A schematic 2√ó2 for Kate‚Äôs asymmetry. The gradient+restricted quadrant is predicted empty.\nIt‚Äôs a nice move because it comes with clear predictions. You shouldn‚Äôt get consonant copy or epenthesis as general phonotactic repairs, only as morphologically restricted alternations. You shouldn‚Äôt get morphologically restricted coarticulation. The grid is crisp: categorical + restricted = phonology, gradient + free = phonetics.\nI should also say I‚Äôm not hostile to this move. In fact, I like the idea that phonology and phonetics are not a uniform system. Substance-free phonology has long made a similar claim: phonology is its own representational domain, not reducible to phonetic substance. I‚Äôve always found that perspective appealing, and Kate‚Äôs framework fits that general spirit even though she keeps an arm‚Äôs-length stance toward committing to any particular theory.\nBut I‚Äôm suspicious of two-level partitions in general. I feel like in cognitive sciences, whenever there is a hard to explain data, first instinct of many people is to propose a two-level model. Automatic vs controlled processing, System 1 vs System 2 reasoning, parsing vs grammar. Sometimes, this two-level model is needed, especially when it is used to isolate a problem. For example, in the case of I-language idea, it was a powerful tool to carve a theoretical space just to discuss grammar. However, as theorization accumulated, this distinction sometimes used as a pile of ‚ÄúI-actually-dont-have-any-idea-so-it-might-be-processing‚Äù.\nThis pattern is everywhere in cognitive science, and all of these frameworks‚Äîincluding what Kate‚Äôs suggesting‚Äîshare the same basic architecture: one process is fast, automatic, and associative; the other is slow, deliberate, and rule-based. The hope is that human behavior can be explained by sorting phenomena into the right bin. And of course, if you set out to show that sometimes people behave automatically and make mistakes; sometimes they behave deliberately and do not make mistakes, you‚Äôll find confirmation all over the place.\nBut as Gawronski, Sherman & Trope (2014) point out, dual-process theories only make progress if they meet a demanding set of conditions: they must specify what the two systems actually are (their operating principles), not just when they operate; they must define the boundaries of each system clearly; and they must generate empirical predictions that could, in principle, be falsified. Otherwise, they collapse into post-hoc labeling‚ÄîSystem 1 when it‚Äôs effortless, System 2 when it‚Äôs not. While dual-process frameworks can be falsifiable in principle, they are often practically unfalsifiable because the internal ‚Äòsystems‚Äô aren‚Äôt anchored to observable input‚Äìoutput relations. Almost any finding can be retrofitted: if an effect looks automatic, it‚Äôs assigned to System 1; if it‚Äôs resource-dependent, to System 2.\nThat‚Äôs my main worry here. Dual-process models are a tempting last resort. They look tidy, but they often function as theories that can‚Äôt really lose‚Äîprecisely the kind that Popper warned us about. John von Neumann put it bluntly: ‚ÄúWith four parameters I can fit an elephant, and with five I can make him wiggle his trunk.‚Äù And if you add one more, you can even make him wink. There‚Äôs even an R implementation‚Äîthe winking pink elephant‚Äîthat literally draws and animates an elephant with just a handful of parameters. It‚Äôs cute, but it‚Äôs also the point: if your model is that flexible, fitting isn‚Äôt the same thing as explaining. (The original code is base R/MASS heavy, but you can rewrite it cleanly with ggplot2 and dplyr.)\nFigure¬†2: ‚ÄúWith four parameters I can fit an elephant.‚Äù The outline is generated by a short Fourier series using complex coefficients.\nThat‚Äôs where Turkish comes in."
  },
  {
    "objectID": "posts/misunderstanding-kate/index.html#the-turkish-case",
    "href": "posts/misunderstanding-kate/index.html#the-turkish-case",
    "title": "misunderstanding Kate‚Äôs talk on nonuniformity of phonology and phonetics",
    "section": "The Turkish case",
    "text": "The Turkish case\nAs it is almost a common knowledge, Turkish exhibit multiple vowel harmonies. Harmonization spread from left to right. The easier one is the backness/frontness harmony. For example, the plural morpheme in Turkish can surface as either -lar or -ler, depending on the last vowel in the word that they are attaching. It is argued that plural morpheme is underspecified in its backness characteristics. And, whatever the value of the previous vowel, it spread into the plural morpheme.\n\n[yzym] + /-lAr/ -&gt; [yzymler]\n[adam] + /-lAr/ -&gt; [adamlar]\n\nAnother present harmony is Backness and Roundness Harmony. For example, the accusative case can surface as -…®, -i, -u, -y in Turkish. Again, the way that it is going to surface is completely dependent on the previous vowel. In this case, the accusative case is underspecified for its roundness and backness, but not underspecified for its height, -I.\n\n[adam] + /-I/ -&gt; [adam…®]\n[herif] + /-I/ -&gt; [herifi]\n[odun] + /-I/ -&gt; [odunu]\n[yzym] + /-I/ -&gt; [yzymy]\n\nTurkish does not have any other widely accepted vowel harmony apart from these two. This means that Turkish normally doesn‚Äôt have free-standing rounding harmony. Moreover, Turkish vowel harmony is strictly word-internal and left-to-right. Backwards harmony is virtually absent. Between-word harmony is nonexistent.\nHowever, there is an interesting case in which all of these properties are overhauled. Turkish exhibit optional vowel characteristic change in a very specific environment. In fast production of complex noun phrases (an NP and a modifier), speakers often round the initial vowel of the second NP (NP2) if:\n\nThe last vowel of NP1 is [+round].\n\nThe first vowel of NP2 is [+high, ‚Äìround].\n\nThe second vowel of NP2 is [+round].\n\nNP1 ends in a single consonant (clusters tend to block it).\n\n\nExamples (NP1 modifiers in italics):\n\nbir ikon (one icon) ‚Üí [bir ikon] but on ikon (ten icons) ‚Üí [on ykon]\niki milyon (two millions) ‚Üí [iki miljon] but dokuz milyon (nine millions) ‚Üí [dokuz mylyon]\n\ng√ºzel vizyon (beautiful vision) ‚Üí [gyzel vizyon] but ho≈ü vizyon (nice vision) ‚Üí [ho É vyzyon]\nbazƒ± sigorta (some insurance) ‚Üí [baz…® sigorta] but t√ºm sigorta (whole insurance) ‚Üí [tym sygorta]\ntek biyoloji (single biology) ‚Üí [tek bijolo íi] but v√ºcut biyoloji (body biology) ‚Üí [vy íut byjolo íi]\n\nNot everything goes through:\n\n√º√ß nil√ºfer (three lilies) ‚Üí [yt É nil√ºfer] (no rounding)\n\nt√ºm limon (whole lemon) ‚Üí [tym limon] (no rounding)\n\nk√º√ß√ºk i≈üg√ºc√º (small workforce) ‚Üí [k√º√ß√ºk i≈üg√ºc√º] (blocked by CC cluster)\n\n\nLoanwords are especially revealing. Epenthetic vowels often participate: hipodrom, sigorta, diskotek. Compounds resist, especially when the boundary is vowel‚Äìvowel. Some high-frequency words seem entrenched with disharmony, like dinazor (originally dinozor). There are also asymmetries with labials: sometimes a labial consonant plus a preceding rounded vowel creates the right environment, but not always (nil√ºfer resists)."
  },
  {
    "objectID": "posts/misunderstanding-kate/index.html#why-it-bugs-me",
    "href": "posts/misunderstanding-kate/index.html#why-it-bugs-me",
    "title": "misunderstanding Kate‚Äôs talk on nonuniformity of phonology and phonetics",
    "section": "Why it bugs me",
    "text": "Why it bugs me\nAt first glance, this seems like a good evidence for Kate‚Äôs argument.\n\nIt‚Äôs optional: It does not necessarily go through, actually you will hear people do not make the rounding bunch of the times.\nIt‚Äôs gradient: I do not feel like the vowel quality of the rounded one is similar to other round vowels.\nIt seems to apply semi-automatically in fast speech.\n\nHowever, Kate‚Äôs argument relies on the non-collapsibility of the quadrant. An event that looks like this should be co-articulation, and should happen irregardless of morphological or syntactic environment. However, that is not the case for this phenomenon. This specific bidirectional vowel harmony, or stuck vowel, only occurs within the syntactically complex DPs. It does not occur within the same word when the same configuration exist. On the contrary, within the same root, there is an additional push for creating disharmony [see Clements and Sezer].\nIn phrases that is not dominated by the same syntactic node, this rounding harmony does not occur. For example, in a sentence like (1), one would expect ‚Äúikon‚Äù to be pronounced as [ykon] due to the presence of the adverb d√ºn [dyn] prior to it, however, that is not the case. The same resistance is preserved even if we do not have this string in a place close to the topic area, the left-most phrases in Turkish sentences. The same resistance is still preserved if we place these two elements post-verbally, where they are produced in the same breadth, without a possible prosodic break.\n\nD√ºn ikon aldƒ±m `I bought an icon yesterday‚Äô\nBen Ahmetlerin d√ºkkanƒ±ndan d√ºn ikon aldƒ±m. `I bought an icon yesterday from Ahmets‚Äô store‚Äô\nBen Ahmetlerin d√ºkkanƒ±ndan aldƒ±m d√ºn ikon. `I bought an icon yesterday from Ahmets‚Äô store‚Äô\n\nThis is exactly the quadrant that Kate‚Äôs asymmetry says should be empty: co-articulation is supposed to be gradient and unrestricted, phonology is supposed to be categorical and morphological. Turkish gives you a phenomenon that‚Äôs both gradient but restricted.\nMy worry is that the two-level carve‚Äîcategorical vs gradient, phonology vs phonetics‚Äîis just too strong. If phonology is supposed to be categorical and morphologically restricted, and phonetics gradient and unrestricted, what do we do with Turkish? Calling it ‚Äújust coarticulation‚Äù ignores the morphological conditioning. Calling it ‚Äúphonology‚Äù ignores the gradient, variable character.\nMaybe I‚Äôm misunderstanding Kate‚Äôs point‚Äîher claim is typological, and she might be perfectly happy calling Turkish an outlier. But it makes me uneasy. Two-level models are good at drawing clean lines, and that‚Äôs why they‚Äôre attractive. But reality doesn‚Äôt always honor the bins we set up for it. One possibility is that coarticulation might be systematically governed prosodical structure. As of now, I categorize this in terms of nouns and their modifiers. It is possible that this is generalized to other heads and their modifiers. Then the question would be the following: is this rounding-harmony conditioned by prosodical structure that is a by-product of ‚Äúhead-modifier‚Äù relation or is it conditioned by syntactic locality?"
  },
  {
    "objectID": "posts/ma-thesis/index.html",
    "href": "posts/ma-thesis/index.html",
    "title": "agreement attraction in turkish",
    "section": "",
    "text": "The term agreement attraction is used to describe either increased acceptability of ungrammatical sentences or reduced difficulty in reading ungrammatical sentences. This was made possible by an illusionary element which matches with the features of the verb, thus attracting the probe of the agreement from the head noun to itself.   Consider the following sentence: ‚Äú*The key to the cabinets are rusty.‚Äù Even though it is ungrammatical, people systematically accepted these sentences more often than the ones with the word ‚Äúattractor.‚Äù They also spend less time when there is a plural nominal like ‚Äúcabinets‚Äù rather than the word ‚Äúcabinet.‚Äù   This thesis tries to explore how the attraction phenomenon interacts with issues like case syncretism, form heuristics, response bias, and register, all of which are rarely investigated in the literature. This thesis also provides an extensive, but not exhaustive, picture of attraction effects in Turkish, an understudied language in psycholinguistics.\n\n\n\n\n\n\nNoteAbstract\n\n\n\n\n\nIn this thesis, I investigate the existing agreement attraction effects in Turkish and how these effects interact with various phenomenon such as (i) case syncretism and local ambiguity, (ii) form heuristics, (iii) response bias, and (iv) honorific readings. Previous studies have shown that speakers occasionally find ungrammatical sentences violating number agreement acceptable when there is another noun sharing same number with the verb, in other words exhibited agreement attraction. Lago et al.¬†(2019) found that genitive-possessive structures were able to induce agreement attraction effects within native Turkish speakers in a speeded acceptability experiment. However, due to the nature of the Turkish and acceptability studies, there are multiple alternative explanations for the existing effects. This thesis aims to weed out possible confounds and clarify the effects by conducting four speeded acceptability judgment experiments. We showed (i) that case-ambiguity on the head noun does not play a role in Turkish agreement attraction (Experiment 1, N = 118), (ii) that participants do not use form-driven-processing-strategies to answer judgment questions (Experiments 2A, N = 80, and 2B, N = 95), (iii) that response bias induced ungrammaticality illusion and only decreased the magnitude of grammaticality illusion (Experiment 3, N = 114), and (iv) that a possible honorific reading does not license superfluous plural marking at the verb (Experiment 4, N = 174). Together, our results challenge cue-based retrieval accounts of agreement attraction and can be accommodated by accounts that assume attraction occurs due to erroneous encodings.\n\n\n\n\n\n\n\n\n\nTipDefense Slides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantThesis\n\n\n\n\n\nBogazici Page | Local PDF | Overleaf\n\n\n\n\n\n\n\n\n\nNotePlots from defense & some more\n\n\n\n\n\n\n\n\n\n\n\nNoneExp1 Averages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneExp1 Models\n\n\n\n\n\nBayesian Model fitted to all experimental items\n\nBayesian Model fitted to only ungrammatical experimental items\n\n\n\n\n\n\n\n\n\n\nNoneExp2A Averages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneExp2A Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneExp2B Averages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneExp2B Models\n\n\n\n\n\nBayesian Model fitted to all experimental items\n\nBayesian Model fitted to only experimental items with RC attractors\n\n\n\n\n\n\n\n\n\n\nNoneExp3 Averages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneHammerly et al.¬†(2019) Averages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneHammerly et al.¬†(2019) Bias Informed Averages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneHammerly et al.¬†(2019) Bias Informed Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneExp3 Bias-Related\n\n\n\n\n\nParticipants Bias in our Experiment 3 using fillers and Bayes Factor for parametric t-test\n\nParticipants Bias in Hammerly et al.¬†(2019) using fillers and experimental items as well as Bayes Factor for parametric t-tests\n\n\n\n\n\n\n\n\n\n\nNoneExp3 Models\n\n\n\n\n\nBayesian Model fitted to only ungrammatical experimental items\n\nBayesian Model fitted to only grammatical experimental items\n\n\n\n\n\n\n\n\n\n\nNoneExp4 Averages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneExp4 Models\n\n\n\n\n\nBayesian Model fitted to all experimental items\n\nBayesian Model fitted to only informal experimental items\n\n\n\n\n\n\n\n\nSupplementary links\n\nDownload Full Thesis\nGithub Repo\nAll Code (Grouped according to the chapters)\nExperiment Raw Data\nLago et al.‚Äôs (2019) work\nHammerly et al.‚Äôs (2019) work\nCode for nicer plots from the defense\nExperiment 1\nExperiment 2A\nExperiment 2B\nExperiment 3\nExperiment 4"
  },
  {
    "objectID": "posts/yaml-rnotebook/index.html",
    "href": "posts/yaml-rnotebook/index.html",
    "title": "how to include r expressions in (quarto) yaml",
    "section": "",
    "text": "When writing R Markdown or Quarto files, I‚Äôve always disliked cluttering them with long file paths, fixed parameters, or small snippets of code. Many of these values are constants or things I don‚Äôt plan to change often, sometimes not even across projects. So I started looking for a way to move such information into a separate YAML file and refer to it from within Quarto. You could of course do this with a regular R script and source(), but I found that YAML makes these kinds of project-wide parameters easier to organize and read."
  },
  {
    "objectID": "posts/yaml-rnotebook/index.html#embedding-r-in-yaml",
    "href": "posts/yaml-rnotebook/index.html#embedding-r-in-yaml",
    "title": "how to include r expressions in (quarto) yaml",
    "section": "Embedding R in YAML",
    "text": "Embedding R in YAML\nWhile digging around, I came across this helpful StackOverflow post, which pointed out that YAML can evaluate R expressions directly if marked with !expr. This means your config file can contain values that are not just static strings, but dynamically computed.\nFor example, my _paper.yaml file looks like this:\nexperiments:\n  first: \n    path: !expr 'paste0(\"C:/Users/\", Sys.info()[6], \"/OneDrive/Science/experiments/first\")'\n    nsubject: !expr 'nrow(read.csv(file.path(paste0(\"C:/Users/\", Sys.info()[6], \"/OneDrive/Science/experiments/first\"), \"results.csv\")))'\n  second: !expr 'paste0(\"C:/Users/\", Sys.info()[6], \"/OneDrive/Science/experiments/second\")'\n  third: ..."
  },
  {
    "objectID": "posts/yaml-rnotebook/index.html#reading-it-in-r",
    "href": "posts/yaml-rnotebook/index.html#reading-it-in-r",
    "title": "how to include r expressions in (quarto) yaml",
    "section": "Reading it in R",
    "text": "Reading it in R\nTo load the config file, you just need to enable expression evaluation when calling yaml::read_yaml():\n\nconfig &lt;- yaml::read_yaml('_paper.yaml', eval.expr = TRUE)\n\nNow, config behaves like a nested list, and you can access parameters directly. For example:\n\nconfig$experiments$first\nconfig$experiments$first$nsubject\n\nThis makes it easy to centralize all project-level constants and keep your Quarto documents cleaner."
  },
  {
    "objectID": "posts/yaml-rnotebook/index.html#why-this-helps",
    "href": "posts/yaml-rnotebook/index.html#why-this-helps",
    "title": "how to include r expressions in (quarto) yaml",
    "section": "Why This Helps",
    "text": "Why This Helps\n\nCleaner documents: no repeated paths or magic numbers in your .qmd files.\nReusability: the same YAML file can be shared across multiple projects.\nFlexibility: values can be static (like participant counts) or computed on the fly (like paths that depend on the username).\n\nIt might feel a little fragile, since I am evaluating R code hidden in a config file, but it‚Äôs been reliable and I am finding it quite useful. For instance, I sometimes even compute the number of participants automatically from an Ibex CSV and store it in the YAML‚Äîthen refer to it in my Quarto text."
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "utku turk",
    "section": "",
    "text": "[1]T√ºrk, U. (2022). Agreement Attraction in Turkish. MA thesis. Boƒüazi√ßi University. [link] [PDF] [repo]"
  },
  {
    "objectID": "papers.html#dissertation-thesis",
    "href": "papers.html#dissertation-thesis",
    "title": "utku turk",
    "section": "",
    "text": "[1]T√ºrk, U. (2022). Agreement Attraction in Turkish. MA thesis. Boƒüazi√ßi University. [link] [PDF] [repo]"
  },
  {
    "objectID": "papers.html#journal-articles",
    "href": "papers.html#journal-articles",
    "title": "utku turk",
    "section": "Journal Articles",
    "text": "Journal Articles\n\n[1]T√ºrk, U.; Logacev, P. (2024). Agreement Attraction in Turkish: The case of genitive attractors. Language, Cognition and Neuroscience, 39(4), 448‚Äì454. [PDF]\n\n\n[2]T√ºrk, U.; Atmaca, F.; √ñzate≈ü, ≈û. B.; et al.¬†(2022). Resources for Turkish dependency parsing: introducing the BOUN Treebank and the BoAT annotation tool.. Language Resources & Evaluation, 56, 259‚Äì307. [PDF]\n\n\n[3]Yu, X.; T√ºrk, U.; Dods, A.; Tian, X.; Lau, E. (2025). Non-visually-derived mental objects tax ‚Äòvisual‚Äô pointers. (submitted). [preprint]"
  },
  {
    "objectID": "papers.html#book-chapters",
    "href": "papers.html#book-chapters",
    "title": "utku turk",
    "section": "Book Chapters",
    "text": "Book Chapters\n\n[1]T√ºrk, U. (2024). Controlling morphosyntactic competition through phonology. In Exploring Nanosyntax II: The Lexicalisation algorithm (eds.¬†Karen de Clerq; Pavel Caha; Guido Vanden Wyngaerd), Oxford Academic Press. (revision) [link] [PDF]\n\n\n[2]Canalis, S.; √ñzdemir, S.; T√ºrk, U.; Tun√ßer, √ú. C. (2024). The phonological nature of the Turkish front glide. In Selected Essays on Turkish Linguistics: The Anadolu Meeting, Otto Harrassowitz Verlag. [PDF]"
  },
  {
    "objectID": "papers.html#in-progress",
    "href": "papers.html#in-progress",
    "title": "utku turk",
    "section": "In Progress",
    "text": "In Progress\n\n[1]T√ºrk, U. (2025). Response Bias in Turkish Agreement Attraction. (in prep.) (available upon request) [repo]\n\n\n[2]T√ºrk, U. (2025). The Role of Register in Agreement Attraction. (in prep.) \n\n\n[3]T√ºrk, U.; Lau, E. (2025). Kind NPs. (in prep.) (available upon request) [repo]"
  },
  {
    "objectID": "papers.html#proceedings",
    "href": "papers.html#proceedings",
    "title": "utku turk",
    "section": "Proceedings",
    "text": "Proceedings\n\n[1]Lewis, S.; T√ºrk, U. (2025). Superiority Effects With Wh-Adjuncts in Turkish. Proceedings of the Linguistic Society of America, 10(1): 5931. Doi: 10.3765/plsa.v10i1.5931. [DOI] [link] [PDF]\n\n\n[2]T√ºrk, U.; Demirok, √ñ. (2021). Hypothetical Comparison in Turkish.. Proceedings of the 6th Workshop on Turkic and languages in contact with Turkic (LSA). Doi: 10.3765/ptu.v6i1.5054. [DOI] [link] [PDF]\n\n\n[3]T√ºrk, U.; Caha, P. (2021). Nanosyntactic Analysis of Turkish Case System.. Proceedings of the 6th Workshop on Turkic and languages in contact with Turkic (LSA). Doi: 10.3765/ptu.v6i1.5051. [DOI] [link] [PDF]\n\n\n[4]T√ºrk, U. (2020). Tackling the Augmentative Puzzle in Turkish.. Proceedings of the 5th Workshop on Turkic and languages in contact with Turkic (LSA). Doi: 10.3765/ptu.v5i1.4771. [DOI] [link] [PDF]\n\n\n[5]T√ºrk, U.; Bayar, K.; √ñzercan, A. D.; √ñzt√ºrk, G. Y.; √ñzate≈ü, S. B. (2020). First Steps towards Universal Dependencies for Laz.. Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020)., 189‚Äì194 [link] [PDF]\n\n\n[6]T√ºrk, U.; Atmaca, F.; √ñzate≈ü, S. B.; Bedir, S. T.; K√∂ksal, A.; √ñzt√ºrk, B.; G√ºng√∂r, T.; √ñzg√ºr, A. (2019). Turkish Treebanking: Unifying and Constructing Efforts.. Proceedings of the 13th Linguistic Annotation Workshop., 166‚Äì177 [link] [PDF]\n\n\n[7]T√ºrk, U.; Atmaca, F.; √ñzate≈ü, S. B.; Bedir, S. T.; K√∂ksal, A.; √ñzt√ºrk, B.; G√ºng√∂r, T.; √ñzg√ºr, A. (2019). Improving the Annotations in the Turkish Universal Dependency Treebank.. Proceedings of the Third Workshop on Universal Dependencies (UDW, SyntaxFest 2019)., 108‚Äì117 [link] [PDF]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "recent blog posts",
    "section": "",
    "text": "utku turk\nPhD Candidate, Linguistics\nUniversity of Maryland\nworking with Ellen Lau and Colin Phillips\n interested in the timing of morphological and syntactic planning in speech, and how it shapes online comprehension and production \n Previously MA at Boƒüazi√ßi (with Pavel Logaƒçev);\nvisiting researcher at Masaryk (with Pavel Caha). \nutkuturk@umd.edu\n\n\nNews\n\nApr 2025 Advanced to candidacy (QP: timing of agreement in production) ¬∑ slides\n\nMore ‚Üí\n\n\n\n\n\n\n\n  \n    \n      \n      \n\n      \n      \n        \n          \n            \n          \n        \n      \n\n      \n        \n          \n            misunderstanding Kate's talk on nonuniformity of phonology and phonetics\n          \n\n          \n            Kate‚Äôs asymmetry meets Turkish rounding: evidence that‚Äôs gradient but syntactically bounded (also a rant about dual-process framings)\n          \n          \n          \n            \n              Utku Turk\n            \n            \n              \n              Oct 29, 2025\n            \n          \n\n          \n            \n              \n              linguistics misunderstanding sound turkish\n\n              \n                \n                  \n                    linguistics\n                  \n                \n                  \n                    misunderstanding\n                  \n                \n                  \n                    sound\n                  \n                \n                  \n                    turkish\n                  \n                \n              \n            \n            Read\n          \n        \n      \n    \n  \n    \n      \n      \n\n      \n      \n        \n          \n            \n          \n        \n      \n\n      \n        \n          \n            montreal forced aligner workflow for pcibex production experiments\n          \n\n          \n            step-by-step instructions for filtering, renaming, and aligning pcibex recordings with mfa\n          \n          \n          \n            \n              Utku Turk\n            \n            \n              \n              Jul 12, 2024\n            \n          \n\n          \n            \n              \n              linguistics papers sound experiment\n\n              \n                \n                  \n                    linguistics\n                  \n                \n                  \n                    papers\n                  \n                \n                  \n                    sound\n                  \n                \n                  \n                    experiment\n                  \n                \n              \n            \n            Read\n          \n        \n      \n    \n  \n    \n      \n      \n\n      \n      \n        \n          \n            \n          \n        \n      \n\n      \n        \n          \n            automating experimental recording management with bash\n          \n\n          \n            it got really annoying after a while, so i wrote this code\n          \n          \n          \n            \n              Utku Turk\n            \n            \n              \n              Feb 23, 2023\n            \n          \n\n          \n            \n              \n              technical code\n\n              \n                \n                  \n                    technical\n                  \n                \n                  \n                    code\n                  \n                \n              \n            \n            Read\n          \n        \n      \n    \n  \n    \n      \n      \n\n      \n      \n        \n          \n            \n          \n        \n      \n\n      \n        \n          \n            agreement attraction in turkish\n          \n\n          \n            attraction in turkish seems to be modulated by bias, register, task details, but not morpology\n          \n          \n          \n            \n              Utku Turk\n            \n            \n              \n              Aug 20, 2022\n            \n          \n\n          \n            \n              \n              linguistics dissertation agreement attraction\n\n              \n                \n                  \n                    linguistics\n                  \n                \n                  \n                    dissertation\n                  \n                \n                  \n                    agreement attraction\n                  \n                \n              \n            \n            Read\n          \n        \n      \n    \n  \n    \n      \n      \n\n      \n      \n        \n          \n            \n          \n        \n      \n\n      \n        \n          \n            is /j/ really a sonorant in turkish? i guess not.\n          \n\n          \n            findings point to a fricative status for /j/, despite its occasional sonorant disguise\n          \n          \n          \n            \n              Utku Turk\n            \n            \n              \n              May 10, 2022\n            \n          \n\n          \n            \n              \n              linguistics papers sound\n\n              \n                \n                  \n                    linguistics\n                  \n                \n                  \n                    papers\n                  \n                \n                  \n                    sound\n                  \n                \n              \n            \n            Read\n          \n        \n      \n    \n  \n    \n      \n      \n\n      \n      \n        \n          \n            \n          \n        \n      \n\n      \n        \n          \n            can ‚Äòas if‚Äô clauses be degree comparisons?\n          \n\n          \n            turkish as if clauses specifically target gradable adverbials and adjectives, pointing to a degree-based analysis\n          \n          \n          \n            \n              Utku Turk\n            \n            \n              \n              May 10, 2022\n            \n          \n\n          \n            \n              \n              linguistics papers meaning\n\n              \n                \n                  \n                    linguistics\n                  \n                \n                  \n                    papers\n                  \n                \n                  \n                    meaning\n                  \n                \n              \n            \n            Read\n          \n        \n      \n    \n  \n    \n      \n      \n\n      \n      \n        \n          \n            \n          \n        \n      \n\n      \n        \n          \n            how to include r expressions in (quarto) yaml\n          \n\n          \n            using yaml with embedded r expressions for cleaner, reusable quarto configs\n          \n          \n          \n            \n              Utku Turk\n            \n            \n              \n              Jan 23, 2020\n            \n          \n\n          \n            \n              \n              technical code\n\n              \n                \n                  \n                    technical\n                  \n                \n                  \n                    code\n                  \n                \n              \n            \n            Read\n          \n        \n      \n    \n  \n\n\n\n\nNo matching items"
  }
]