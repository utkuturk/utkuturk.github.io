{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unaccusativity Syntax or Picture Difficulty? \n",
        "**Full Computational Analysis Pipeline**\n",
        "\n",
        "This notebook replicates the analysis of visual stimuli using CLIP and Qwen-VL, including Bayesian regressions for similarity, salience, and ordinal verification scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Environment Setup\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y libvips-dev\n",
        "!pip install pyvips git+https://github.com/openai/CLIP.git\n",
        "!pip install transformers torch torchvision accelerate sentencepiece pillow pandas numpy matplotlib seaborn scikit-learn scipy pyro-ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Set up plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "# plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    device = \"cpu\" \n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "model_clip, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"CLIP model loaded successfully!\")\n",
        "\n",
        "rom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
        "import transformers\n",
        "import torch\n",
        "from transformers.generation.beam_search import BeamSearchScorer\n",
        "transformers.BeamSearchScorer = BeamSearchScorer\n",
        "\n",
        "# Load Qwen-VL-Chat model\n",
        "model_id = \"Qwen/Qwen-VL-Chat\"\n",
        "\n",
        "model_vlm = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    dtype=torch.float32\n",
        ").to('cpu')\n",
        "tokenizer_vlm = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "# Create the streamer\n",
        "streamer = TextStreamer(tokenizer_vlm, skip_prompt=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data & Model Loading\n",
        "If you do not have the image files locally, this notebook will attempt to load `cached_scores.csv` if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unergative scenes\n",
        "df_unerg = pd.DataFrame({\n",
        "    \"Filename\": [\n",
        "        \"./pictures/octopus_swim.jpg\",\n",
        "        \"./pictures/ballerina_run.jpg\",\n",
        "        \"./pictures/boy_float.jpg\",\n",
        "        \"./pictures/chef_yell.jpg\",\n",
        "        \"./pictures/clown_walk.jpg\",\n",
        "        \"./pictures/cowboy_wink.jpg\",\n",
        "        \"./pictures/dog_bark.jpg\",\n",
        "        \"./pictures/monkey_sleep.jpg\",\n",
        "        \"./pictures/penguin_sneeze.jpg\",\n",
        "        \"./pictures/pirate_cough.jpg\",\n",
        "        \"./pictures/rabbit_smile.jpg\",\n",
        "        \"./pictures/snail_crawl.jpg\",\n",
        "    ],\n",
        "    \"Sentence\": [\n",
        "        \"The octopus is swimming.\",\n",
        "        \"The ballerina is running.\",\n",
        "        \"The boy is floating.\",\n",
        "        \"The chef is yelling.\",\n",
        "        \"The clown is walking.\",\n",
        "        \"The cowboy is winking.\",\n",
        "        \"The dog is barking.\",\n",
        "        \"The monkey is sleeping.\",\n",
        "        \"The penguin is sneezing.\",\n",
        "        \"The pirate is coughing.\",\n",
        "        \"The rabbit is smiling.\",\n",
        "        \"The snail is crawling.\",\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Unaccusative scenes\n",
        "df_unacc = pd.DataFrame({\n",
        "    \"Filename\": [\n",
        "        \"./pictures/octopus_boil.jpg\",\n",
        "        \"./pictures/ballerina_shrink.jpg\",\n",
        "        \"./pictures/boy_yawn.jpg\",\n",
        "        \"./pictures/chef_drown.jpg\",\n",
        "        \"./pictures/clown_grow.jpg\",\n",
        "        \"./pictures/cowboy_fall.jpg\",\n",
        "        \"./pictures/dog_spin.jpg\",\n",
        "        \"./pictures/monkey_trip.jpg\",\n",
        "        \"./pictures/penguin_bounce.jpg\",\n",
        "        \"./pictures/pirate_sink.jpg\",\n",
        "        \"./pictures/rabbit_shake.jpg\",\n",
        "        \"./pictures/snail_melt.jpg\",\n",
        "    ],\n",
        "    \"Sentence\": [\n",
        "        \"The octopus is boiling.\",\n",
        "        \"The ballerina is shrinking.\",\n",
        "        \"The boy is yawning.\",\n",
        "        \"The chef is drowning.\",\n",
        "        \"The clown is growing.\",\n",
        "        \"The cowboy is falling.\",\n",
        "        \"The dog is spinning.\",\n",
        "        \"The monkey is tripping.\",\n",
        "        \"The penguin is bouncing.\",\n",
        "        \"The pirate is sinking.\",\n",
        "        \"The rabbit is shaking.\",\n",
        "        \"The snail is melting.\",\n",
        "    ]\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian Regressions (Normal & Ordinal Logistic)\n",
        "We model the effect of Verb Type across three metrics [cite: 270-272]:\n",
        "1. **Full Scene (CLIP)**: Linear Regression\n",
        "2. **Subject Salience (CLIP)**: Linear Regression\n",
        "3. **Scene Verification (VLM)**: Ordered Logistic Regression (Ordinal Match 1-10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_clip_similarity(df, model, preprocess, device):\n",
        "    \"\"\"\n",
        "    Compute CLIP similarity scores for image-text pairs.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame with 'Filename' and 'Sentence' columns\n",
        "    model : CLIP model\n",
        "        Loaded CLIP model\n",
        "    preprocess : function\n",
        "        CLIP preprocessing function\n",
        "    device : str\n",
        "        'cuda' or 'cpu'\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        Original dataframe with added 'CLIP_Similarity' column\n",
        "    \"\"\"\n",
        "    similarity_scores = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        img_path = row['Filename']\n",
        "        text = row['Sentence']\n",
        "\n",
        "        # Preprocess image and tokenize text\n",
        "        img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "        text_tokenized = clip.tokenize([text]).to(device)\n",
        "\n",
        "        # Compute similarity\n",
        "        with torch.no_grad():\n",
        "            logits_per_image, _ = model(img, text_tokenized)\n",
        "            similarity_score = logits_per_image.item()\n",
        "\n",
        "        similarity_scores.append(similarity_score)\n",
        "\n",
        "    # Add scores to dataframe\n",
        "    df_copy = df.copy()\n",
        "    df_copy['CLIP_Similarity'] = similarity_scores\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "def compute_subject_salience(df, model, preprocess, device):\n",
        "    \"\"\"\n",
        "    Compute CLIP similarity scores for subject noun alone.\n",
        "    This measures how visually salient/easy to identify the subject is.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame with 'Filename' and 'Sentence' columns\n",
        "    model : CLIP model\n",
        "        Loaded CLIP model\n",
        "    preprocess : function\n",
        "        CLIP preprocessing function\n",
        "    device : str\n",
        "        'cuda' or 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        Original dataframe with added 'Subject_Salience' column\n",
        "    \"\"\"\n",
        "    subject_scores = []\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        img_path = row['Filename']\n",
        "        sentence = row['Sentence']\n",
        "        \n",
        "        # Extract subject noun (assumes format \"The X is ...\")\n",
        "        # Extract word after \"The \" and before \" is\"\n",
        "        subject = sentence.split(\"The \")[1].split(\" is\")[0]\n",
        "        \n",
        "        # Preprocess image and tokenize subject\n",
        "        img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "        text_tokenized = clip.tokenize([subject]).to(device)\n",
        "        \n",
        "        # Compute similarity\n",
        "        with torch.no_grad():\n",
        "            logits_per_image, _ = model(img, text_tokenized)\n",
        "            similarity_score = logits_per_image.item()\n",
        "        \n",
        "        subject_scores.append(similarity_score)\n",
        "    \n",
        "    df_copy = df.copy()\n",
        "    df_copy['Subject_Salience'] = subject_scores\n",
        "    \n",
        "    return df_copy\n",
        "\n",
        "def compute_qwen_scores(df, model, tokenizer, streamer=None):\n",
        "    \"\"\"\n",
        "    Compute verification scores using Qwen-VL-Chat multimodal LLM.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame with 'Filename' and 'Sentence' columns\n",
        "    model : Qwen-VL-Chat model\n",
        "        Loaded Qwen model\n",
        "    tokenizer : AutoTokenizer\n",
        "        Qwen tokenizer\n",
        "    streamer : TextStreamer, optional\n",
        "        Streamer for real-time output\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        Original dataframe with added 'VLM_Score' and 'VLM_Response' columns\n",
        "    \"\"\"\n",
        "    import re\n",
        "    scores = []\n",
        "    responses = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        img_path = row['Filename']\n",
        "        sentence = row['Sentence']\n",
        "\n",
        "        # Create query for Qwen-VL-Chat\n",
        "        query = tokenizer.from_list_format([\n",
        "            {'image': img_path},\n",
        "            {'text': f'Rate how well this sentence describes the image: \"{sentence}\"\\nScore from 1-10 (1=mismatch, 10=perfect match). Reply with just the number.'},\n",
        "        ])\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            response, _ = model.chat(tokenizer, query=query, history=None, streamer=streamer)\n",
        "\n",
        "        # Extract numeric score\n",
        "        try:\n",
        "            match = re.search(r'(\\d+(?:\\.\\d+)?)', response)\n",
        "            score = float(match.group(1)) if match else 5.0\n",
        "            score = min(10.0, max(1.0, score))  # Clamp to 1-10\n",
        "        except:\n",
        "            score = 5.0\n",
        "\n",
        "        scores.append(score)\n",
        "        responses.append(response)\n",
        "\n",
        "    df_copy = df.copy()\n",
        "    df_copy['VLM_Score'] = scores\n",
        "    df_copy['VLM_Response'] = responses\n",
        "\n",
        "    return df_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "CACHE_FILE = \"./cached_scores.csv\"\n",
        "\n",
        "\n",
        "if os.path.exists(CACHE_FILE):\n",
        "    df_all = pd.read_csv(CACHE_FILE)\n",
        "else:\n",
        "    # Compute CLIP similarities\n",
        "    df_unerg_clip = compute_clip_similarity(df_unerg, model_clip, preprocess, device)\n",
        "    df_unacc_clip = compute_clip_similarity(df_unacc, model_clip, preprocess, device)\n",
        "    \n",
        "    # Compute subject salience scores\n",
        "    df_unerg_subj = compute_subject_salience(df_unerg, model_clip, preprocess, device)\n",
        "    df_unacc_subj = compute_subject_salience(df_unacc, model_clip, preprocess, device)\n",
        "\n",
        "    # Compute Qwen-VL scores\n",
        "    df_unerg_vlm = compute_qwen_scores(df_unerg, model_vlm, tokenizer_vlm, streamer=streamer)\n",
        "    df_unacc_vlm = compute_qwen_scores(df_unacc, model_vlm, tokenizer_vlm, streamer=streamer)\n",
        "\n",
        "    # Combine CLIP scores with VLM scores and subject salience\n",
        "    df_unerg_scored = df_unerg_clip.copy()\n",
        "    df_unerg_scored['Subject_Salience'] = df_unerg_subj['Subject_Salience']\n",
        "    df_unerg_scored['VLM_Score'] = df_unerg_vlm['VLM_Score']\n",
        "    df_unerg_scored['VLM_Response'] = df_unerg_vlm['VLM_Response']\n",
        "    df_unerg_scored['VerbType'] = 'Unergative'\n",
        "\n",
        "    df_unacc_scored = df_unacc_clip.copy()\n",
        "    df_unacc_scored['Subject_Salience'] = df_unacc_subj['Subject_Salience']\n",
        "    df_unacc_scored['VLM_Score'] = df_unacc_vlm['VLM_Score']\n",
        "    df_unacc_scored['VLM_Response'] = df_unacc_vlm['VLM_Response']\n",
        "    df_unacc_scored['VerbType'] = 'Unaccusative'\n",
        "\n",
        "    # Combine for analysis\n",
        "    df_all = pd.concat([df_unerg_scored, df_unacc_scored], ignore_index=True)\n",
        "\n",
        "    # Save to cache\n",
        "    df_all.to_csv(CACHE_FILE, index=False)\n",
        "\n",
        "print(df_all.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison plot with all three metrics\n",
        "fig, axes = plt.subplots(1, 3, figsize=(8, 3))\n",
        "\n",
        "# CLIP full sentence results\n",
        "sns.pointplot(data=df_all, x='VerbType', y='CLIP_Similarity',\n",
        "              hue='VerbType', palette=['#3498db', '#e74c3c'], \n",
        "              ax=axes[0], errorbar='ci', capsize=0.1, \n",
        "              linestyle='none', markers='o', legend=False)\n",
        "sns.stripplot(data=df_all, x='VerbType', y='CLIP_Similarity',\n",
        "              color='black', alpha=0.5, size=8, ax=axes[0], jitter=0.2)\n",
        "\n",
        "axes[0].set_xlabel('Verb Type', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('CLIP Similarity Score', fontsize=14, fontweight='bold')\n",
        "axes[0].set_title('Full Sentence Similarity',\n",
        "                  fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "for verb_type in ['Unergative', 'Unaccusative']:\n",
        "    mean_val = df_all[df_all['VerbType'] == verb_type]['CLIP_Similarity'].mean()\n",
        "    axes[0].text(0 if verb_type == 'Unergative' else 1, mean_val + 1,\n",
        "                 f'M = {mean_val:.2f}', ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Subject salience results\n",
        "sns.pointplot(data=df_all, x='VerbType', y='Subject_Salience',\n",
        "              hue='VerbType', palette=['#3498db', '#e74c3c'], \n",
        "              ax=axes[1], errorbar='ci', capsize=0.1, \n",
        "              linestyle='none', markers='o', legend=False)\n",
        "sns.stripplot(data=df_all, x='VerbType', y='Subject_Salience',\n",
        "              color='black', alpha=0.5, size=8, ax=axes[1], jitter=0.2)\n",
        "\n",
        "axes[1].set_xlabel('Verb Type', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Subject Salience Score', fontsize=14, fontweight='bold')\n",
        "axes[1].set_title('Subject Noun Identifiability',\n",
        "                  fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "for verb_type in ['Unergative', 'Unaccusative']:\n",
        "    mean_val = df_all[df_all['VerbType'] == verb_type]['Subject_Salience'].mean()\n",
        "    axes[1].text(0 if verb_type == 'Unergative' else 1, mean_val + 0.5,\n",
        "                 f'M = {mean_val:.2f}', ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# VLM results\n",
        "sns.pointplot(data=df_all, x='VerbType', y='VLM_Score',\n",
        "              hue='VerbType', palette=['#3498db', '#e74c3c'], \n",
        "              ax=axes[2], errorbar='ci', capsize=0.1, \n",
        "              linestyle='none', markers='o', legend=False)\n",
        "sns.stripplot(data=df_all, x='VerbType', y='VLM_Score',\n",
        "              color='black', alpha=0.5, size=8, ax=axes[2], jitter=0.2)\n",
        "\n",
        "axes[2].set_xlabel('Verb Type', fontsize=14, fontweight='bold')\n",
        "axes[2].set_ylabel('Qwen-VL Match Score (1-10)', fontsize=14, fontweight='bold')\n",
        "axes[2].set_title('Scene Verification (Qwen-VL)',\n",
        "                  fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "for verb_type in ['Unergative', 'Unaccusative']:\n",
        "    mean_val = df_all[df_all['VerbType'] == verb_type]['VLM_Score'].mean()\n",
        "    axes[2].text(0 if verb_type == 'Unergative' else 1, mean_val + 0.3,\n",
        "                 f'M = {mean_val:.2f}', ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./model_comparison_plot.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from pyro.infer import MCMC, NUTS\n",
        "\n",
        "# Prepare data for Pyro\n",
        "# We'll center the scores and code VerbType numerically\n",
        "df_pyro = df_all.copy()\n",
        "df_pyro['VerbType_num'] = df_pyro['VerbType'].map({'Unergative': -0.5, 'Unaccusative': 0.5})\n",
        "df_pyro['CLIP_centered'] = df_pyro['CLIP_Similarity'] - df_pyro['CLIP_Similarity'].mean()\n",
        "df_pyro['Subject_centered'] = df_pyro['Subject_Salience'] - df_pyro['Subject_Salience'].mean()\n",
        "vlm_score_tensor = torch.tensor(df_pyro['VLM_Score'].values, dtype=torch.long)\n",
        "\n",
        "# Convert to tensors\n",
        "verb_type_tensor = torch.tensor(df_pyro['VerbType_num'].values, dtype=torch.float32)\n",
        "clip_tensor = torch.tensor(df_pyro['CLIP_centered'].values, dtype=torch.float32)\n",
        "subject_tensor = torch.tensor(df_pyro['Subject_centered'].values, dtype=torch.float32)\n",
        "\n",
        "# --- Model for CLIP Similarity ---\n",
        "def clip_model(verb_type, obs=None):\n",
        "    intercept = pyro.sample('intercept', dist.Normal(0., 10.))\n",
        "    beta = pyro.sample('beta', dist.Normal(0., 10.))\n",
        "    sigma = pyro.sample('sigma', dist.HalfNormal(10.))\n",
        "    mu = intercept + beta * verb_type\n",
        "    with pyro.plate('data', len(verb_type)):\n",
        "        pyro.sample('obs', dist.Normal(mu, sigma), obs=obs)\n",
        "\n",
        "# --- Model for Subject Salience ---\n",
        "def subject_model(verb_type, obs=None):\n",
        "    intercept = pyro.sample('intercept', dist.Normal(0., 10.))\n",
        "    beta = pyro.sample('beta', dist.Normal(0., 10.))\n",
        "    sigma = pyro.sample('sigma', dist.HalfNormal(10.))\n",
        "    mu = intercept + beta * verb_type\n",
        "    with pyro.plate('data', len(verb_type)):\n",
        "        pyro.sample('obs', dist.Normal(mu, sigma), obs=obs)\n",
        "        \n",
        "# --- Model for VLM Score (Ordered Logistic) ---\n",
        "k_categories = vlm_score_tensor.max().item() + 1\n",
        "k_cutpoints = k_categories - 1\n",
        "def vlm_model(verb_type, obs=None):\n",
        "    alpha = pyro.sample('alpha', dist.Normal(0., 10.))\n",
        "    beta = pyro.sample('beta', dist.Normal(0., 10.))\n",
        "    with pyro.plate(\"cutpoints_plate\", k_cutpoints):\n",
        "        raw_cutpoints = pyro.sample('raw_cutpoints', dist.Normal(torch.arange(k_cutpoints).float(), 1.))\n",
        "    cutpoints = torch.sort(raw_cutpoints)[0]\n",
        "    latent_propensity = alpha + beta * verb_type\n",
        "    with pyro.plate('data', len(verb_type)):\n",
        "        pyro.sample('obs', dist.OrderedLogistic(latent_propensity, cutpoints), obs=obs)\n",
        "\n",
        "# Run the MCMC samplers\n",
        "mcmc_clip = MCMC(NUTS(clip_model), num_samples=2000, warmup_steps=1000)\n",
        "mcmc_clip.run(verb_type_tensor, clip_tensor)\n",
        "clip_samples = mcmc_clip.get_samples()\n",
        "\n",
        "mcmc_subject = MCMC(NUTS(subject_model), num_samples=2000, warmup_steps=1000)\n",
        "mcmc_subject.run(verb_type_tensor, subject_tensor)\n",
        "subject_samples = mcmc_subject.get_samples()\n",
        "\n",
        "mcmc_vlm = MCMC(NUTS(vlm_model), num_samples=2000, warmup_steps=1000, num_chains=1)\n",
        "mcmc_vlm.run(verb_type_tensor, vlm_score_tensor)\n",
        "vlm_samples = mcmc_vlm.get_samples()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get posterior samples and print results\n",
        "clip_beta_mean = clip_samples['beta'].mean().item()\n",
        "clip_beta_hdi = torch.quantile(clip_samples['beta'], torch.tensor([0.025, 0.975]))\n",
        "\n",
        "print(f\"\\nCLIP Similarity - Bayesian Regression:\")\n",
        "print(f\"  Beta (VerbType effect): {clip_beta_mean:.3f}\")\n",
        "print(f\"  95% HDI: [{clip_beta_hdi[0]:.3f}, {clip_beta_hdi[1]:.3f}]\")\n",
        "print(f\"  P(beta < 0): {(clip_samples['beta'] < 0).float().mean():.3f}\")\n",
        "\n",
        "subject_beta_mean = subject_samples['beta'].mean().item()\n",
        "subject_beta_hdi = torch.quantile(subject_samples['beta'], torch.tensor([0.025, 0.975]))\n",
        "\n",
        "print(f\"\\nSubject Salience - Bayesian Regression:\")\n",
        "print(f\"  Beta (VerbType effect): {subject_beta_mean:.3f}\")\n",
        "print(f\"  95% HDI: [{subject_beta_hdi[0]:.3f}, {subject_beta_hdi[1]:.3f}]\") \n",
        "print(f\"  P(beta < 0): {(subject_samples['beta'] < 0).float().mean():.3f}\")\n",
        "\n",
        "vlm_beta_mean = vlm_samples['beta'].mean().item()\n",
        "vlm_beta_hdi = torch.quantile(vlm_samples['beta'], torch.tensor([0.025, 0.975]))\n",
        "\n",
        "print(f\"\\nVLM Score - Ordered Logistic Regression:\")\n",
        "print(f\"  Beta (VerbType effect): {vlm_beta_mean:.3f}\")\n",
        "print(f\"  95% HDI: [{vlm_beta_hdi[0]:.3f}, {vlm_beta_hdi[1]:.3f}]\")\n",
        "print(f\"  P(beta < 0): {(vlm_samples['beta'] < 0).float().mean():.3f}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Data dictionary from your MCMC samples\n",
        "beta_data = {\n",
        "    'Full Scene (CLIP)': clip_samples['beta'].numpy(),\n",
        "    'Subject Salience (CLIP)': subject_samples['beta'].numpy(),\n",
        "    'Scene Verification (VLM)': vlm_samples['beta'].numpy()\n",
        "}\n",
        "\n",
        "# Adjust figure size for better vertical separation\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "sns.set_style(\"whitegrid\", {'axes.grid': True, 'grid.color': '.95'})\n",
        "\n",
        "labels = list(beta_data.keys())\n",
        "colors = ['#3498db', '#9b59b6', '#e74c3c']\n",
        "\n",
        "for i, label in enumerate(labels):\n",
        "    samples = beta_data[label]\n",
        "    mean_val = samples.mean()\n",
        "    \n",
        "    # 1. Calculate multiple intervals for the \"stacking\" effect\n",
        "    hdi_95 = np.percentile(samples, [2.5, 97.5])\n",
        "    hdi_80 = np.percentile(samples, [10, 90])\n",
        "    hdi_50 = np.percentile(samples, [25, 75])\n",
        "    \n",
        "    # 2. Plot the stacked lines (Bottom to Top: thinnest/widest first)\n",
        "    # 95% Interval - Thin\n",
        "    ax.hlines(i, hdi_95[0], hdi_95[1], color=colors[i], linewidth=1.5, alpha=0.4, zorder=1)\n",
        "    # 80% Interval - Medium\n",
        "    ax.hlines(i, hdi_80[0], hdi_80[1], color=colors[i], linewidth=5.0, alpha=0.7, zorder=2)\n",
        "    # 50% Interval - Thick\n",
        "    ax.hlines(i, hdi_50[0], hdi_50[1], color=colors[i], linewidth=10.0, alpha=1.0, zorder=3)\n",
        "    \n",
        "    # 3. Plot the Mean point\n",
        "    ax.plot(mean_val, i, 'o', color='white', markersize=8, zorder=4)\n",
        "    \n",
        "    # 4. Perfectly Aligned Statistics\n",
        "    p_dir = (samples < 0).mean() if mean_val < 0 else (samples > 0).mean()\n",
        "    prob_text = f\"$P(\\\\beta {'<' if mean_val < 0 else '>' } 0) = {p_dir:.2f}$\"\n",
        "    \n",
        "    # Locked to y-coordinate 'i' and x-coordinate 3.0 (outside plot area)\n",
        "    ax.text(3.0, i, prob_text, va='center', ha='left', \n",
        "            fontsize=13, fontweight='bold', color=colors[i])\n",
        "\n",
        "# 5. Descriptive Annotations (The \"How to Read\" Guide)\n",
        "ax.axvline(x=0, color='black', linestyle='-', linewidth=1.5, alpha=0.6, zorder=0)\n",
        "\n",
        "# Arrow pointing Left (Negative Beta)\n",
        "ax.annotate('', xy=(-5, -1.0), xytext=(-0.5, -1.0),\n",
        "            arrowprops=dict(arrowstyle=\"->\", color='gray', lw=1.5))\n",
        "ax.text(-2.75, -1.4, \"Lower Scores for\\nUnaccusatives\", ha='center', color='gray', fontweight='bold')\n",
        "\n",
        "# Arrow pointing Right (Positive Beta)\n",
        "ax.annotate('', xy=(2.5, -1.0), xytext=(0.5, -1.0),\n",
        "            arrowprops=dict(arrowstyle=\"->\", color='gray', lw=1.5))\n",
        "ax.text(1.5, -1.4, \"Lower Scores for\\nUnergatives\", ha='center', color='gray', fontweight='bold')\n",
        "\n",
        "# 6. Final Layout Polish\n",
        "ax.set_yticks(range(len(labels)))\n",
        "ax.set_yticklabels(labels, fontweight='bold', fontsize=12)\n",
        "ax.set_xlabel('Posterior Beta Weight (Unaccusative vs. Unergative)', fontsize=13, labelpad=45)\n",
        "\n",
        "# Lock limits so text and arrows don't shift\n",
        "ax.set_xlim(-6, 3)\n",
        "ax.set_ylim(-1.5, len(labels) - 0.5)\n",
        "\n",
        "sns.despine(left=True, bottom=True)\n",
        "plt.subplots_adjust(right=0.75, bottom=0.2) # Make room for text on right and guide on bottom\n",
        "plt.savefig('./model_pyro.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
